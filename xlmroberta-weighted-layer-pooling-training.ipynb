{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024399,
     "end_time": "2021-11-14T18:27:31.925043",
     "exception": false,
     "start_time": "2021-11-14T18:27:31.900644",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-11-14T18:27:31.990368Z",
     "iopub.status.busy": "2021-11-14T18:27:31.989636Z",
     "iopub.status.idle": "2021-11-14T18:27:39.002051Z",
     "shell.execute_reply": "2021-11-14T18:27:39.001421Z",
     "shell.execute_reply.started": "2021-11-14T13:15:57.171419Z"
    },
    "papermill": {
     "duration": 7.053768,
     "end_time": "2021-11-14T18:27:39.002211",
     "exception": false,
     "start_time": "2021-11-14T18:27:31.948443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "gc.enable()\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import multiprocessing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn import model_selection\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import (\n",
    "    Dataset, DataLoader,\n",
    "    SequentialSampler, RandomSampler\n",
    ")\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "try:\n",
    "    from apex import amp\n",
    "    APEX_INSTALLED = True\n",
    "except ImportError:\n",
    "    APEX_INSTALLED = False\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    logging,\n",
    "    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n",
    ")\n",
    "logging.set_verbosity_warning()\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "def fix_all_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def optimal_num_of_loader_workers():\n",
    "    num_cpus = multiprocessing.cpu_count()\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    optimal_value = min(num_cpus, num_gpus*4) if num_gpus else num_cpus - 1\n",
    "    return optimal_value\n",
    "\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T18:27:39.054659Z",
     "iopub.status.busy": "2021-11-14T18:27:39.054020Z",
     "iopub.status.idle": "2021-11-14T18:27:46.105733Z",
     "shell.execute_reply": "2021-11-14T18:27:46.104930Z",
     "shell.execute_reply.started": "2021-11-14T13:16:04.383240Z"
    },
    "papermill": {
     "duration": 7.07926,
     "end_time": "2021-11-14T18:27:46.105863",
     "exception": false,
     "start_time": "2021-11-14T18:27:39.026603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>answers</th>\n",
       "      <th>c_id</th>\n",
       "      <th>is_in</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56be85543aeaaa14008c9063</td>\n",
       "      <td>बियॉन्से कब लोकप्रिय होने लगी?</td>\n",
       "      <td>बेयोंसे गिजेल नोल्स-कार्टर (/biːˈjɒnseɪ/ Bee-Y...</td>\n",
       "      <td>[{'text': '1990 के दशक के अंत में', 'answer_st...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56be85543aeaaa14008c9065</td>\n",
       "      <td>जब वह बड़ी हो रही थी तो बियॉन्से ने किन क्षेत्...</td>\n",
       "      <td>बेयोंसे गिजेल नोल्स-कार्टर (/biːˈjɒnseɪ/ Bee-Y...</td>\n",
       "      <td>[{'text': 'गायन और नृत्य', 'answer_start': 213}]</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56be85543aeaaa14008c9066</td>\n",
       "      <td>बियॉन्से ने डेस्टिनीज़ चाइल्ड को कब छोड़ा और ए...</td>\n",
       "      <td>बेयोंसे गिजेल नोल्स-कार्टर (/biːˈjɒnseɪ/ Bee-Y...</td>\n",
       "      <td>[{'text': '2003', 'answer_start': 535}]</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9601</td>\n",
       "      <td>बेयोंसे किस शहर और राज्य में पली-बढ़ी?</td>\n",
       "      <td>बेयोंसे गिजेल नोल्स-कार्टर (/biːˈjɒnseɪ/ Bee-Y...</td>\n",
       "      <td>[{'text': 'ह्यूस्टन, टेक्सास', 'answer_start':...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9602</td>\n",
       "      <td>बियॉन्से किस दशक में प्रसिद्ध हुई?</td>\n",
       "      <td>बेयोंसे गिजेल नोल्स-कार्टर (/biːˈjɒnseɪ/ Bee-Y...</td>\n",
       "      <td>[{'text': '1990 के दशक के अंत में', 'answer_st...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  \\\n",
       "0  56be85543aeaaa14008c9063   \n",
       "1  56be85543aeaaa14008c9065   \n",
       "2  56be85543aeaaa14008c9066   \n",
       "3  56bf6b0f3aeaaa14008c9601   \n",
       "4  56bf6b0f3aeaaa14008c9602   \n",
       "\n",
       "                                            question  \\\n",
       "0                     बियॉन्से कब लोकप्रिय होने लगी?   \n",
       "1  जब वह बड़ी हो रही थी तो बियॉन्से ने किन क्षेत्...   \n",
       "2  बियॉन्से ने डेस्टिनीज़ चाइल्ड को कब छोड़ा और ए...   \n",
       "3             बेयोंसे किस शहर और राज्य में पली-बढ़ी?   \n",
       "4                 बियॉन्से किस दशक में प्रसिद्ध हुई?   \n",
       "\n",
       "                                             context  \\\n",
       "0  बेयोंसे गिजेल नोल्स-कार्टर (/biːˈjɒnseɪ/ Bee-Y...   \n",
       "1  बेयोंसे गिजेल नोल्स-कार्टर (/biːˈjɒnseɪ/ Bee-Y...   \n",
       "2  बेयोंसे गिजेल नोल्स-कार्टर (/biːˈjɒnseɪ/ Bee-Y...   \n",
       "3  बेयोंसे गिजेल नोल्स-कार्टर (/biːˈjɒnseɪ/ Bee-Y...   \n",
       "4  बेयोंसे गिजेल नोल्स-कार्टर (/biːˈjɒnseɪ/ Bee-Y...   \n",
       "\n",
       "                                             answers  c_id  is_in  \n",
       "0  [{'text': '1990 के दशक के अंत में', 'answer_st...     0   True  \n",
       "1   [{'text': 'गायन और नृत्य', 'answer_start': 213}]     0   True  \n",
       "2            [{'text': '2003', 'answer_start': 535}]     0   True  \n",
       "3  [{'text': 'ह्यूस्टन, टेक्सास', 'answer_start':...     0   True  \n",
       "4  [{'text': '1990 के दशक के अंत में', 'answer_st...     0   True  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tamil_xquad_tr = pd.read_csv('../input/google-translated-squad20-to-hindi-and-tamil/squad_ta.csv')\n",
    "hindi_xquad_tr = pd.read_csv('../input/google-translated-squad20-to-hindi-and-tamil/squad_hi.csv')\n",
    "hindi_xquad_tr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T18:27:46.185202Z",
     "iopub.status.busy": "2021-11-14T18:27:46.184369Z",
     "iopub.status.idle": "2021-11-14T18:27:50.111911Z",
     "shell.execute_reply": "2021-11-14T18:27:50.111501Z",
     "shell.execute_reply.started": "2021-11-14T13:16:11.571478Z"
    },
    "papermill": {
     "duration": 3.981442,
     "end_time": "2021-11-14T18:27:50.112049",
     "exception": false,
     "start_time": "2021-11-14T18:27:46.130607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2269, 5), (1437, 5))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "hindi_xquad_tr['answers'] = hindi_xquad_tr['answers'].apply(ast.literal_eval)\n",
    "tamil_xquad_tr['answers'] = tamil_xquad_tr['answers'].apply(ast.literal_eval)\n",
    "\n",
    "def get_text(d):\n",
    "    return d[0]['text']\n",
    "def get_start(d):\n",
    "    return d[0]['answer_start']\n",
    "\n",
    "hindi_xquad_tr['answer_text'] = hindi_xquad_tr['answers'].apply(get_text)\n",
    "hindi_xquad_tr['answer_start'] = hindi_xquad_tr['answers'].apply(get_start)\n",
    "tamil_xquad_tr['answer_text'] = tamil_xquad_tr['answers'].apply(get_text)\n",
    "tamil_xquad_tr['answer_start'] = tamil_xquad_tr['answers'].apply(get_start)\n",
    "\n",
    "hindi_xquad_tr['language'] = 'hindi'\n",
    "tamil_xquad_tr['language'] = 'tamil'\n",
    "\n",
    "hindi_xquad_tr.drop(['id','answers','c_id','is_in'], axis=1, inplace=True)\n",
    "tamil_xquad_tr.drop(['id','answers','c_id','is_in'], axis=1, inplace=True)\n",
    "\n",
    "hindi_xquad_tr = hindi_xquad_tr[hindi_xquad_tr['answer_start']!=-1]\n",
    "tamil_xquad_tr = tamil_xquad_tr[tamil_xquad_tr['answer_start']!=-1]\n",
    "\n",
    "hindi_xquad_tr = hindi_xquad_tr.sample(frac=0.03)\n",
    "tamil_xquad_tr = tamil_xquad_tr.sample(frac=0.05)\n",
    "\n",
    "tamil_xquad_tr.shape,hindi_xquad_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T18:27:50.177325Z",
     "iopub.status.busy": "2021-11-14T18:27:50.176401Z",
     "iopub.status.idle": "2021-11-14T18:27:50.180446Z",
     "shell.execute_reply": "2021-11-14T18:27:50.179986Z",
     "shell.execute_reply.started": "2021-11-14T13:16:15.607664Z"
    },
    "papermill": {
     "duration": 0.043552,
     "end_time": "2021-11-14T18:27:50.180554",
     "exception": false,
     "start_time": "2021-11-14T18:27:50.137002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62524</th>\n",
       "      <td>முழு மார்பு கவசம் என்ன அழைக்கப்பட்டது?</td>\n",
       "      <td>ஏழாம் நூற்றாண்டு வரை ஜப்பானிய வீரர்கள் ஒரு வகை...</td>\n",
       "      <td>டவு</td>\n",
       "      <td>554</td>\n",
       "      <td>tamil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56613</th>\n",
       "      <td>ஓக்லஹோமா பல்கலைக்கழகத்தின் நடன நிகழ்ச்சி எப்போ...</td>\n",
       "      <td>இந்த மாநிலத்தில் பாலேவில் ஒரு பணக்கார வரலாறு உ...</td>\n",
       "      <td>1962</td>\n",
       "      <td>551</td>\n",
       "      <td>tamil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>சோபின் தனது வேலையின் எந்தப் பகுதிகள் பாடும் மா...</td>\n",
       "      <td>அவரது பெற்றோரின் குடியிருப்பில் நான்கு போர்டர்...</td>\n",
       "      <td>டைட்டஸ் வோய்சிகோவ்ஸ்கி</td>\n",
       "      <td>85</td>\n",
       "      <td>tamil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90634</th>\n",
       "      <td>குப்லாய் நிர்வாகத்தின் பணம் எப்போது தீர்ந்துவி...</td>\n",
       "      <td>1279 க்குப் பிறகு குப்லாய் அரசாங்கம் நிதி சிக்...</td>\n",
       "      <td>1279 க்குப் பிறகு</td>\n",
       "      <td>0</td>\n",
       "      <td>tamil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56585</th>\n",
       "      <td>ஓக்லஹோமாவின் நான்காவது மிகவும் பிரபலமான மொழி எது?</td>\n",
       "      <td>ஜெர்மன் பொதுவாகப் பயன்படுத்தப்படும் நான்காவது ...</td>\n",
       "      <td>ஜெர்மன்</td>\n",
       "      <td>0</td>\n",
       "      <td>tamil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12130</th>\n",
       "      <td>எந்த உலாவி புலம் புதியது?</td>\n",
       "      <td>உலாவி சந்தையில் மிக சமீபத்திய முக்கிய நுழைவு க...</td>\n",
       "      <td>குரோம்</td>\n",
       "      <td>45</td>\n",
       "      <td>tamil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81001</th>\n",
       "      <td>ரெக்ஸ் சாக்ரோரம் எந்த வகையான அதிகாரத்தைக் கொண்...</td>\n",
       "      <td>ரீகல் சகாப்தத்தில், ஒரு ரெக்ஸ் சாக்ரோரம் (புனி...</td>\n",
       "      <td>சிவில்</td>\n",
       "      <td>194</td>\n",
       "      <td>tamil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40569</th>\n",
       "      <td>ஜோஹன் ஹெர்ட்டர் யார்?</td>\n",
       "      <td>புராட்டஸ்டன்ட் சீர்திருத்தத்தின் நிகழ்வும், அத...</td>\n",
       "      <td>ஜெர்மன் தத்துவஞானி</td>\n",
       "      <td>387</td>\n",
       "      <td>tamil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40288</th>\n",
       "      <td>அமைதிக்கான புதிய நட்பு கோரிக்கைகள் பிரான்ஸ் எந...</td>\n",
       "      <td>நெப்போலியன், போரில் வெற்றி பெறுவார் என்று எதிர...</td>\n",
       "      <td>1791</td>\n",
       "      <td>385</td>\n",
       "      <td>tamil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21440</th>\n",
       "      <td>நிறைய திட்டங்கள் எதற்காகக் காத்திருக்கின்றன?</td>\n",
       "      <td>பல்பணி என்பது பல நிரல்களுக்கு இடையில் மாறிக்கொ...</td>\n",
       "      <td>உள்ளீடு/வெளியீட்டு சாதனங்கள்</td>\n",
       "      <td>201</td>\n",
       "      <td>tamil</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2269 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                question  \\\n",
       "62524             முழு மார்பு கவசம் என்ன அழைக்கப்பட்டது?   \n",
       "56613  ஓக்லஹோமா பல்கலைக்கழகத்தின் நடன நிகழ்ச்சி எப்போ...   \n",
       "932    சோபின் தனது வேலையின் எந்தப் பகுதிகள் பாடும் மா...   \n",
       "90634  குப்லாய் நிர்வாகத்தின் பணம் எப்போது தீர்ந்துவி...   \n",
       "56585  ஓக்லஹோமாவின் நான்காவது மிகவும் பிரபலமான மொழி எது?   \n",
       "...                                                  ...   \n",
       "12130                          எந்த உலாவி புலம் புதியது?   \n",
       "81001  ரெக்ஸ் சாக்ரோரம் எந்த வகையான அதிகாரத்தைக் கொண்...   \n",
       "40569                              ஜோஹன் ஹெர்ட்டர் யார்?   \n",
       "40288  அமைதிக்கான புதிய நட்பு கோரிக்கைகள் பிரான்ஸ் எந...   \n",
       "21440       நிறைய திட்டங்கள் எதற்காகக் காத்திருக்கின்றன?   \n",
       "\n",
       "                                                 context  \\\n",
       "62524  ஏழாம் நூற்றாண்டு வரை ஜப்பானிய வீரர்கள் ஒரு வகை...   \n",
       "56613  இந்த மாநிலத்தில் பாலேவில் ஒரு பணக்கார வரலாறு உ...   \n",
       "932    அவரது பெற்றோரின் குடியிருப்பில் நான்கு போர்டர்...   \n",
       "90634  1279 க்குப் பிறகு குப்லாய் அரசாங்கம் நிதி சிக்...   \n",
       "56585  ஜெர்மன் பொதுவாகப் பயன்படுத்தப்படும் நான்காவது ...   \n",
       "...                                                  ...   \n",
       "12130  உலாவி சந்தையில் மிக சமீபத்திய முக்கிய நுழைவு க...   \n",
       "81001  ரீகல் சகாப்தத்தில், ஒரு ரெக்ஸ் சாக்ரோரம் (புனி...   \n",
       "40569  புராட்டஸ்டன்ட் சீர்திருத்தத்தின் நிகழ்வும், அத...   \n",
       "40288  நெப்போலியன், போரில் வெற்றி பெறுவார் என்று எதிர...   \n",
       "21440  பல்பணி என்பது பல நிரல்களுக்கு இடையில் மாறிக்கொ...   \n",
       "\n",
       "                        answer_text  answer_start language  \n",
       "62524                           டவு           554    tamil  \n",
       "56613                          1962           551    tamil  \n",
       "932          டைட்டஸ் வோய்சிகோவ்ஸ்கி            85    tamil  \n",
       "90634             1279 க்குப் பிறகு             0    tamil  \n",
       "56585                       ஜெர்மன்             0    tamil  \n",
       "...                             ...           ...      ...  \n",
       "12130                        குரோம்            45    tamil  \n",
       "81001                        சிவில்           194    tamil  \n",
       "40569            ஜெர்மன் தத்துவஞானி           387    tamil  \n",
       "40288                          1791           385    tamil  \n",
       "21440  உள்ளீடு/வெளியீட்டு சாதனங்கள்           201    tamil  \n",
       "\n",
       "[2269 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tamil_xquad_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T18:27:50.234550Z",
     "iopub.status.busy": "2021-11-14T18:27:50.234059Z",
     "iopub.status.idle": "2021-11-14T18:27:51.687305Z",
     "shell.execute_reply": "2021-11-14T18:27:51.686521Z",
     "shell.execute_reply.started": "2021-11-14T13:16:15.633949Z"
    },
    "papermill": {
     "duration": 1.482099,
     "end_time": "2021-11-14T18:27:51.687454",
     "exception": false,
     "start_time": "2021-11-14T18:27:50.205355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>நிகழ்வுகள்.\\n- ஜனவரி 20 - பிரெஞ்சுப் படைகள் ஆம...</td>\n",
       "      <td>விடுதலை, சமத்துவம், சகோதரத்துவம் ஆகிய கொள்கைக...</td>\n",
       "      <td>பிரெஞ்சுப் புரட்சி</td>\n",
       "      <td>513</td>\n",
       "      <td>tamil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>பிறரது பாடல்கள் 20 இற்குக் குறைவாகவே உள்ளன.\\n\\...</td>\n",
       "      <td>எனப்படுவது தமிழில் கிறித்துவுக்கு முற்பட்ட கா...</td>\n",
       "      <td>சங்க இலக்கியம்</td>\n",
       "      <td>58</td>\n",
       "      <td>tamil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4306</th>\n",
       "      <td>இந்து மதத்தில், திக்பாலர்களின உருவங்களை கோவில்...</td>\n",
       "      <td>இந்து மதத்தில் கூறப்படும்  குபேரன், யமன், இந்த...</td>\n",
       "      <td>திக்பாலர்</td>\n",
       "      <td>16</td>\n",
       "      <td>tamil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3367</th>\n",
       "      <td>பாம்பனில் உள்ள இரு பாலங்களும் ஊாின் நுழைவாயிலி...</td>\n",
       "      <td>தமிழ்நாடு மாநிலத்தின் பெருநிலப்பரப்பையும் இராம...</td>\n",
       "      <td>பாம்பன் பாலம்</td>\n",
       "      <td>67</td>\n",
       "      <td>tamil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5851</th>\n",
       "      <td>பாளி மொழியில் அமைந்த தம்மபதம் 26 அத்தியாயங்களு...</td>\n",
       "      <td>பாளி மொழியில் எழுதப்பட்ட பௌத்த சமயப் புனித இல...</td>\n",
       "      <td>தம்மபதம்</td>\n",
       "      <td>21</td>\n",
       "      <td>tamil</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                context  \\\n",
       "535   நிகழ்வுகள்.\\n- ஜனவரி 20 - பிரெஞ்சுப் படைகள் ஆம...   \n",
       "482   பிறரது பாடல்கள் 20 இற்குக் குறைவாகவே உள்ளன.\\n\\...   \n",
       "4306  இந்து மதத்தில், திக்பாலர்களின உருவங்களை கோவில்...   \n",
       "3367  பாம்பனில் உள்ள இரு பாலங்களும் ஊாின் நுழைவாயிலி...   \n",
       "5851  பாளி மொழியில் அமைந்த தம்மபதம் 26 அத்தியாயங்களு...   \n",
       "\n",
       "                                               question         answer_text  \\\n",
       "535    விடுதலை, சமத்துவம், சகோதரத்துவம் ஆகிய கொள்கைக...  பிரெஞ்சுப் புரட்சி   \n",
       "482    எனப்படுவது தமிழில் கிறித்துவுக்கு முற்பட்ட கா...      சங்க இலக்கியம்   \n",
       "4306  இந்து மதத்தில் கூறப்படும்  குபேரன், யமன், இந்த...           திக்பாலர்   \n",
       "3367  தமிழ்நாடு மாநிலத்தின் பெருநிலப்பரப்பையும் இராம...       பாம்பன் பாலம்   \n",
       "5851   பாளி மொழியில் எழுதப்பட்ட பௌத்த சமயப் புனித இல...            தம்மபதம்   \n",
       "\n",
       "      answer_start language  \n",
       "535            513    tamil  \n",
       "482             58    tamil  \n",
       "4306            16    tamil  \n",
       "3367            67    tamil  \n",
       "5851            21    tamil  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XQA_tamil_dev = pd.read_csv('../input/preprocessed-xqa-tamil/XQA_tamil_dev.csv')\n",
    "XQA_tamil_test = pd.read_csv('../input/preprocessed-xqa-tamil/XQA_tamil_test.csv')\n",
    "XQA_tamil_dev = XQA_tamil_dev[XQA_tamil_dev['answer_start']!=-1]\n",
    "XQA_tamil_test = XQA_tamil_test[XQA_tamil_test['answer_start']!=-1]\n",
    "XQA_tamil_dev = XQA_tamil_dev.sample(frac=0.5)\n",
    "XQA_tamil_test = XQA_tamil_test.sample(frac=0.5)\n",
    "XQA_tamil_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T18:27:51.745118Z",
     "iopub.status.busy": "2021-11-14T18:27:51.744494Z",
     "iopub.status.idle": "2021-11-14T18:27:51.747573Z",
     "shell.execute_reply": "2021-11-14T18:27:51.747992Z",
     "shell.execute_reply.started": "2021-11-14T13:16:16.874313Z"
    },
    "papermill": {
     "duration": 0.034053,
     "end_time": "2021-11-14T18:27:51.748125",
     "exception": false,
     "start_time": "2021-11-14T18:27:51.714072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((258, 5), (240, 5))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XQA_tamil_test.shape,XQA_tamil_dev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026363,
     "end_time": "2021-11-14T18:27:51.800241",
     "exception": false,
     "start_time": "2021-11-14T18:27:51.773878",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T18:27:51.857725Z",
     "iopub.status.busy": "2021-11-14T18:27:51.857022Z",
     "iopub.status.idle": "2021-11-14T18:27:51.859817Z",
     "shell.execute_reply": "2021-11-14T18:27:51.859394Z",
     "shell.execute_reply.started": "2021-11-14T13:16:16.882089Z"
    },
    "papermill": {
     "duration": 0.033764,
     "end_time": "2021-11-14T18:27:51.859926",
     "exception": false,
     "start_time": "2021-11-14T18:27:51.826162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # model\n",
    "    model_type = 'xlm_roberta'\n",
    "    model_name_or_path = '../input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2'\n",
    "    config_name = '../input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2'\n",
    "    fp16 = True if APEX_INSTALLED else False\n",
    "    fp16_opt_level = \"O1\"\n",
    "    gradient_accumulation_steps = 2\n",
    "\n",
    "    # tokenizer\n",
    "    tokenizer_name = '../input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2'\n",
    "    max_seq_length = 400\n",
    "    doc_stride = 135\n",
    "\n",
    "    # train\n",
    "    epochs = 1\n",
    "    train_batch_size = 4\n",
    "    eval_batch_size = 8\n",
    "\n",
    "    # optimizer\n",
    "    optimizer_type = 'AdamW'\n",
    "    learning_rate = 1e-5\n",
    "    weight_decay = 1e-2\n",
    "    epsilon = 1e-8\n",
    "    max_grad_norm = 1.0\n",
    "    \n",
    "    # scheduler\n",
    "    decay_name = 'cosine-warmup'\n",
    "    warmup_ratio = 0.1\n",
    "\n",
    "    # logging\n",
    "    logging_steps = 10\n",
    "\n",
    "    # evaluate\n",
    "    output_dir = 'output'\n",
    "    seed = 43"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026714,
     "end_time": "2021-11-14T18:27:51.912138",
     "exception": false,
     "start_time": "2021-11-14T18:27:51.885424",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Data Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T18:27:51.974067Z",
     "iopub.status.busy": "2021-11-14T18:27:51.973505Z",
     "iopub.status.idle": "2021-11-14T18:27:53.300059Z",
     "shell.execute_reply": "2021-11-14T18:27:53.299500Z",
     "shell.execute_reply.started": "2021-11-14T13:16:16.893632Z"
    },
    "papermill": {
     "duration": 1.361947,
     "end_time": "2021-11-14T18:27:53.300197",
     "exception": false,
     "start_time": "2021-11-14T18:27:51.938250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/train.csv')\n",
    "test = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/test.csv')\n",
    "external_mlqa = pd.read_csv('../input/mlqa-hindi-processed/mlqa_hindi.csv')\n",
    "external_xquad = pd.read_csv('../input/mlqa-hindi-processed/xquad.csv')\n",
    "external_train = pd.concat([external_mlqa, external_xquad,XQA_tamil_dev,XQA_tamil_test,hindi_xquad_tr, tamil_xquad_tr])#\n",
    "\n",
    "def create_folds(data, num_splits):\n",
    "    data[\"kfold\"] = -1\n",
    "    kf = model_selection.StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=43)\n",
    "    for f, (t_, v_) in enumerate(kf.split(X=data, y=data['language'])):\n",
    "        data.loc[v_, 'kfold'] = f\n",
    "    return data\n",
    "\n",
    "train = create_folds(train, num_splits=5)\n",
    "external_train[\"kfold\"] = -1\n",
    "external_train['id'] = list(np.arange(1, len(external_train)+1))\n",
    "train = pd.concat([train, external_train]).reset_index(drop=True)\n",
    "\n",
    "def convert_answers(row):\n",
    "    return {'answer_start': [row[0]], 'text': [row[1]]}\n",
    "\n",
    "train['answers'] = train[['answer_start', 'answer_text']].apply(convert_answers, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T18:27:53.357819Z",
     "iopub.status.busy": "2021-11-14T18:27:53.356874Z",
     "iopub.status.idle": "2021-11-14T18:27:53.360783Z",
     "shell.execute_reply": "2021-11-14T18:27:53.360372Z",
     "shell.execute_reply.started": "2021-11-14T13:16:18.156892Z"
    },
    "papermill": {
     "duration": 0.034061,
     "end_time": "2021-11-14T18:27:53.360894",
     "exception": false,
     "start_time": "2021-11-14T18:27:53.326833",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11933"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T18:27:53.511725Z",
     "iopub.status.busy": "2021-11-14T18:27:53.510973Z",
     "iopub.status.idle": "2021-11-14T18:27:53.645620Z",
     "shell.execute_reply": "2021-11-14T18:27:53.645175Z",
     "shell.execute_reply.started": "2021-11-14T13:16:18.168516Z"
    },
    "papermill": {
     "duration": 0.258636,
     "end_time": "2021-11-14T18:27:53.645736",
     "exception": false,
     "start_time": "2021-11-14T18:27:53.387100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11928"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.drop_duplicates(subset=['context','question','answer_text','answer_start','language'])\n",
    "len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026547,
     "end_time": "2021-11-14T18:27:53.698780",
     "exception": false,
     "start_time": "2021-11-14T18:27:53.672233",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Convert Examples to Features (Preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T18:27:53.764853Z",
     "iopub.status.busy": "2021-11-14T18:27:53.764128Z",
     "iopub.status.idle": "2021-11-14T18:27:53.767184Z",
     "shell.execute_reply": "2021-11-14T18:27:53.766430Z",
     "shell.execute_reply.started": "2021-11-14T13:16:18.359899Z"
    },
    "papermill": {
     "duration": 0.042157,
     "end_time": "2021-11-14T18:27:53.767296",
     "exception": false,
     "start_time": "2021-11-14T18:27:53.725139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_train_features(args, example, tokenizer):\n",
    "    example[\"question\"] = example[\"question\"].lstrip()\n",
    "    tokenized_example = tokenizer(\n",
    "        example[\"question\"],\n",
    "        example[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=args.max_seq_length,\n",
    "        stride=args.doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_mapping = tokenized_example.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = tokenized_example.pop(\"offset_mapping\")\n",
    "\n",
    "    features = []\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        feature = {}\n",
    "\n",
    "        input_ids = tokenized_example[\"input_ids\"][i]\n",
    "        attention_mask = tokenized_example[\"attention_mask\"][i]\n",
    "\n",
    "        feature['input_ids'] = input_ids\n",
    "        feature['attention_mask'] = attention_mask\n",
    "        feature['offset_mapping'] = offsets\n",
    "\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "        sequence_ids = tokenized_example.sequence_ids(i)\n",
    "        \n",
    "        ## for validation\n",
    "        feature[\"example_id\"] = example['id'] \n",
    "        feature['sequence_ids'] = [0 if i is None else i for i in tokenized_example.sequence_ids(i)] \n",
    "        feature['context'] = example[\"context\"]\n",
    "        feature['question'] = example[\"question\"]\n",
    "        feature['hindi_tamil'] = 0 if example[\"language\"]=='hindi' else 1 \n",
    "        ## \n",
    "\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = example[\"answers\"]\n",
    "\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            feature[\"start_position\"] = cls_index\n",
    "            feature[\"end_position\"] = cls_index\n",
    "        else:\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != 1:\n",
    "                token_start_index += 1\n",
    "\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != 1:\n",
    "                token_end_index -= 1\n",
    "\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                feature[\"start_position\"] = cls_index\n",
    "                feature[\"end_position\"] = cls_index\n",
    "            else:\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                feature[\"start_position\"] = token_start_index - 1\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                feature[\"end_position\"] = token_end_index + 1\n",
    "\n",
    "        features.append(feature)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026311,
     "end_time": "2021-11-14T18:27:53.819926",
     "exception": false,
     "start_time": "2021-11-14T18:27:53.793615",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Dataset Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T18:27:53.885135Z",
     "iopub.status.busy": "2021-11-14T18:27:53.883658Z",
     "iopub.status.idle": "2021-11-14T18:27:53.885980Z",
     "shell.execute_reply": "2021-11-14T18:27:53.886423Z",
     "shell.execute_reply.started": "2021-11-14T13:16:18.375330Z"
    },
    "papermill": {
     "duration": 0.040399,
     "end_time": "2021-11-14T18:27:53.886562",
     "exception": false,
     "start_time": "2021-11-14T18:27:53.846163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DatasetRetriever(Dataset):\n",
    "    def __init__(self, features, mode='train'):\n",
    "        super(DatasetRetriever, self).__init__()\n",
    "        self.features = features\n",
    "        self.mode = mode\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, item):   \n",
    "        feature = self.features[item]\n",
    "        if self.mode == 'train':\n",
    "            return {\n",
    "                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n",
    "                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n",
    "                'offset_mapping':torch.tensor(feature['offset_mapping'], dtype=torch.long),\n",
    "                'start_position':torch.tensor(feature['start_position'], dtype=torch.long),\n",
    "                'end_position':torch.tensor(feature['end_position'], dtype=torch.long)\n",
    "            }\n",
    "        else:\n",
    "            if self.mode == 'valid': \n",
    "                return {\n",
    "                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n",
    "                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n",
    "                'offset_mapping':torch.tensor(feature['offset_mapping'], dtype=torch.long),\n",
    "                'sequence_ids':feature['sequence_ids'],\n",
    "                'start_position':torch.tensor(feature['start_position'], dtype=torch.long),\n",
    "                'end_position':torch.tensor(feature['end_position'], dtype=torch.long),\n",
    "                'example_id':feature['example_id'],\n",
    "                'context': feature['context'],\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n",
    "                    'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n",
    "                    'offset_mapping':feature['offset_mapping'],\n",
    "                    'sequence_ids':feature['sequence_ids'],\n",
    "                    'id':feature['example_id'],\n",
    "                    'context': feature['context'],\n",
    "                    'question': feature['question']\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026589,
     "end_time": "2021-11-14T18:27:53.939194",
     "exception": false,
     "start_time": "2021-11-14T18:27:53.912605",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T18:27:54.005235Z",
     "iopub.status.busy": "2021-11-14T18:27:54.004524Z",
     "iopub.status.idle": "2021-11-14T18:27:54.007338Z",
     "shell.execute_reply": "2021-11-14T18:27:54.006899Z",
     "shell.execute_reply.started": "2021-11-14T13:16:18.390784Z"
    },
    "papermill": {
     "duration": 0.04199,
     "end_time": "2021-11-14T18:27:54.007456",
     "exception": false,
     "start_time": "2021-11-14T18:27:53.965466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WeightedLayerPooling(nn.Module):\n",
    "    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights=None):\n",
    "        super(WeightedLayerPooling, self).__init__()\n",
    "        self.layer_start = layer_start\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.layer_weights = layer_weights if layer_weights is not None \\\n",
    "            else nn.Parameter(\n",
    "            torch.tensor([1] * (num_hidden_layers + 1 - layer_start), dtype=torch.float)\n",
    "        )\n",
    "\n",
    "    def forward(self, all_hidden_states):\n",
    "        all_layer_embedding = all_hidden_states[self.layer_start:, :, :, :]\n",
    "        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n",
    "        weighted_average = (weight_factor * all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n",
    "        return weighted_average\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, modelname_or_path, config, layer_start, layer_weights=None):\n",
    "        super(Model, self).__init__()\n",
    "        self.config = config\n",
    "        config.update({\n",
    "            \"hidden_dropout_prob\": 0.0,\n",
    "            \"layer_norm_eps\": 1e-7,\n",
    "            \"output_hidden_states\": True\n",
    "        })\n",
    "        self.xlm_roberta = AutoModel.from_pretrained(modelname_or_path, config=config)\n",
    "        self.layer_start = layer_start\n",
    "        self.pooling = WeightedLayerPooling(config.num_hidden_layers,\n",
    "                                            layer_start=layer_start,\n",
    "                                            layer_weights=None)\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.qa_output = torch.nn.Linear(config.hidden_size, 2)\n",
    "        torch.nn.init.normal_(self.qa_output.weight, std=0.02)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.xlm_roberta(input_ids, attention_mask=attention_mask)\n",
    "        all_hidden_states = torch.stack(outputs.hidden_states)\n",
    "        weighted_pooling_embeddings = self.layer_norm(self.pooling(all_hidden_states))\n",
    "        #weighted_pooling_embeddings = weighted_pooling_embeddings[:, 0]\n",
    "\n",
    "        norm_embeddings = self.dropout(weighted_pooling_embeddings)\n",
    "        logits = self.qa_output(norm_embeddings)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "\n",
    "        return start_logits, end_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026022,
     "end_time": "2021-11-14T18:27:54.059607",
     "exception": false,
     "start_time": "2021-11-14T18:27:54.033585",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T18:27:54.116958Z",
     "iopub.status.busy": "2021-11-14T18:27:54.116190Z",
     "iopub.status.idle": "2021-11-14T18:27:54.119005Z",
     "shell.execute_reply": "2021-11-14T18:27:54.118592Z",
     "shell.execute_reply.started": "2021-11-14T13:16:18.406920Z"
    },
    "papermill": {
     "duration": 0.033244,
     "end_time": "2021-11-14T18:27:54.119111",
     "exception": false,
     "start_time": "2021-11-14T18:27:54.085867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_fn(preds, labels):\n",
    "    start_preds, end_preds = preds\n",
    "    start_labels, end_labels = labels\n",
    "    start_loss = nn.CrossEntropyLoss(ignore_index=-1)(start_preds, start_labels)\n",
    "    end_loss = nn.CrossEntropyLoss(ignore_index=-1)(end_preds, end_labels)\n",
    "    total_loss = (start_loss + end_loss) / 2\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026102,
     "end_time": "2021-11-14T18:27:54.171294",
     "exception": false,
     "start_time": "2021-11-14T18:27:54.145192",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Grouped Layerwise Learning Rate Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T18:27:54.238861Z",
     "iopub.status.busy": "2021-11-14T18:27:54.238197Z",
     "iopub.status.idle": "2021-11-14T18:27:54.241353Z",
     "shell.execute_reply": "2021-11-14T18:27:54.240922Z",
     "shell.execute_reply.started": "2021-11-14T13:16:18.418464Z"
    },
    "papermill": {
     "duration": 0.043786,
     "end_time": "2021-11-14T18:27:54.241470",
     "exception": false,
     "start_time": "2021-11-14T18:27:54.197684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_optimizer_grouped_parameters(args, model):\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    group1=['layer.0.','layer.1.','layer.2.','layer.3.']\n",
    "    group2=['layer.4.','layer.5.','layer.6.','layer.7.']    \n",
    "    group3=['layer.8.','layer.9.','layer.10.','layer.11.']\n",
    "    group_all=['layer.0.','layer.1.','layer.2.','layer.3.','layer.4.','layer.5.','layer.6.','layer.7.','layer.8.','layer.9.','layer.10.','layer.11.']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': args.weight_decay},\n",
    "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': args.weight_decay, 'lr': args.learning_rate/10},\n",
    "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': args.weight_decay, 'lr': args.learning_rate},\n",
    "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': args.weight_decay, 'lr': args.learning_rate*10},\n",
    "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': 0.0, 'lr': args.learning_rate/10},\n",
    "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': 0.0, 'lr': args.learning_rate},\n",
    "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': 0.0, 'lr': args.learning_rate*10},\n",
    "        {'params': [p for n, p in model.named_parameters() if args.model_type not in n], 'lr':args.learning_rate*40, \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    return optimizer_grouped_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026014,
     "end_time": "2021-11-14T18:27:54.293678",
     "exception": false,
     "start_time": "2021-11-14T18:27:54.267664",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Metric Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T18:27:54.352657Z",
     "iopub.status.busy": "2021-11-14T18:27:54.351929Z",
     "iopub.status.idle": "2021-11-14T18:27:54.354807Z",
     "shell.execute_reply": "2021-11-14T18:27:54.354399Z",
     "shell.execute_reply.started": "2021-11-14T13:16:18.438233Z"
    },
    "papermill": {
     "duration": 0.034554,
     "end_time": "2021-11-14T18:27:54.354905",
     "exception": false,
     "start_time": "2021-11-14T18:27:54.320351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.max = 0\n",
    "        self.min = 1e5\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        if val > self.max:\n",
    "            self.max = val\n",
    "        if val < self.min:\n",
    "            self.min = val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025941,
     "end_time": "2021-11-14T18:27:54.406858",
     "exception": false,
     "start_time": "2021-11-14T18:27:54.380917",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T18:27:54.478419Z",
     "iopub.status.busy": "2021-11-14T18:27:54.477552Z",
     "iopub.status.idle": "2021-11-14T18:27:54.479622Z",
     "shell.execute_reply": "2021-11-14T18:27:54.480040Z",
     "shell.execute_reply.started": "2021-11-14T13:16:18.447417Z"
    },
    "papermill": {
     "duration": 0.046532,
     "end_time": "2021-11-14T18:27:54.480221",
     "exception": false,
     "start_time": "2021-11-14T18:27:54.433689",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_model(args):\n",
    "    config = AutoConfig.from_pretrained(args.config_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)\n",
    "    model = Model(args.model_name_or_path, config=config,layer_start=12,layer_weights=None)\n",
    "    #model = Model(args.model_name_or_path, config=config)\n",
    "    return config, tokenizer, model\n",
    "\n",
    "def make_optimizer(args, model):\n",
    "    named_parameters = list(model.named_parameters())   \n",
    "\n",
    "    roberta_parameters = named_parameters[:389]   \n",
    "    pooler_parameters = named_parameters[389:391] \n",
    "    qa_parameters = named_parameters[391:]\n",
    "    \n",
    "    parameters = []\n",
    "    \n",
    "    # increase lr every k layer\n",
    "    increase_lr_every_k_layer = 1\n",
    "    lrs = np.linspace(1, 5, 24 // increase_lr_every_k_layer)\n",
    "    for layer_num, (name, params) in enumerate(roberta_parameters):\n",
    "        weight_decay = 0.0 if \"bias\" in name else 0.01\n",
    "        splitted_name = name.split('.')\n",
    "        lr = args.learning_rate #Config.lr\n",
    "        if len(splitted_name) >= 4 and str.isdigit(splitted_name[3]):\n",
    "            layer_num = int(splitted_name[3])\n",
    "            lr = lrs[layer_num // increase_lr_every_k_layer] * lr\n",
    "\n",
    "        parameters.append({\"params\": params,\n",
    "                           \"weight_decay\": weight_decay,\n",
    "                           \"lr\": lr})\n",
    "    \n",
    "    default_lr = 1e-3 #default LR for AdamW\n",
    "    for layer_num, (name,params) in enumerate(qa_parameters):\n",
    "        weight_decay = 0.0 if \"bias\" in name else 0.01\n",
    "        parameters.append({\"params\": params,\n",
    "                           \"weight_decay\": weight_decay,\n",
    "                           \"lr\": default_lr})\n",
    "    \n",
    "    for layer_num, (name,params) in enumerate(pooler_parameters):\n",
    "        weight_decay = 0.0 if \"bias\" in name else 0.01\n",
    "        parameters.append({\"params\": params,\n",
    "                           \"weight_decay\": weight_decay,\n",
    "                           \"lr\": default_lr})\n",
    "\n",
    "    return AdamW(parameters)\n",
    "\n",
    "\n",
    "def make_scheduler(\n",
    "    args, optimizer, \n",
    "    num_warmup_steps, \n",
    "    num_training_steps\n",
    "):\n",
    "    if args.decay_name == \"cosine-warmup\":\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "    else:\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "    return scheduler    \n",
    "\n",
    "\n",
    "def make_loader(\n",
    "    args, data, \n",
    "    tokenizer, fold\n",
    "):\n",
    "    train_set, valid_set = data[data['kfold']!=fold], data[data['kfold']==fold].reset_index(drop=True)\n",
    "    \n",
    "    train_features, valid_features = [[] for _ in range(2)]\n",
    "    for i, row in train_set.iterrows():\n",
    "        train_features += prepare_train_features(args, row, tokenizer)\n",
    "    for i, row in valid_set.iterrows():\n",
    "        valid_features += prepare_train_features(args, row, tokenizer)\n",
    "\n",
    "    ## Weighted sampler\n",
    "    hindi_tamil_count = [] \n",
    "    for i, f in enumerate(train_features):\n",
    "        hindi_tamil_count.append(train_features[i]['hindi_tamil'])      \n",
    "    class_sample_count = pd.Series(hindi_tamil_count).value_counts().values\n",
    "    weight = 1. / class_sample_count\n",
    "    samples_weight = np.array([weight[t] for t in hindi_tamil_count]) \n",
    "    samples_weight = torch.from_numpy(samples_weight)\n",
    "    wsampler = torch.utils.data.sampler.WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight))\n",
    "\n",
    "    train_dataset = DatasetRetriever(train_features, mode=\"train\")\n",
    "    valid_dataset = DatasetRetriever(valid_features, mode=\"valid\")\n",
    "    print(f\"Num examples Train= {len(train_dataset)}, Num examples Valid={len(valid_dataset)}\")\n",
    "    \n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    valid_sampler = SequentialSampler(valid_dataset)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.train_batch_size,\n",
    "        sampler=train_sampler, #wsampler\n",
    "        num_workers=optimal_num_of_loader_workers(),\n",
    "        pin_memory=True,\n",
    "        drop_last=False \n",
    "    )\n",
    "\n",
    "    valid_dataloader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=args.eval_batch_size, \n",
    "        sampler=valid_sampler,\n",
    "        num_workers=optimal_num_of_loader_workers(),\n",
    "        pin_memory=True, \n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    return train_dataloader, valid_dataloader, valid_features, valid_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026065,
     "end_time": "2021-11-14T18:27:54.532515",
     "exception": false,
     "start_time": "2021-11-14T18:27:54.506450",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T18:27:54.597727Z",
     "iopub.status.busy": "2021-11-14T18:27:54.596478Z",
     "iopub.status.idle": "2021-11-14T18:27:54.599370Z",
     "shell.execute_reply": "2021-11-14T18:27:54.598921Z",
     "shell.execute_reply.started": "2021-11-14T13:16:18.469744Z"
    },
    "papermill": {
     "duration": 0.040628,
     "end_time": "2021-11-14T18:27:54.599476",
     "exception": false,
     "start_time": "2021-11-14T18:27:54.558848",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self, model, tokenizer, \n",
    "        optimizer, scheduler\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "    def train(\n",
    "        self, args, \n",
    "        train_dataloader, \n",
    "        epoch, result_dict\n",
    "    ):\n",
    "        count = 0\n",
    "        losses = AverageMeter()\n",
    "        \n",
    "        self.model.zero_grad()\n",
    "        self.model.train()\n",
    "        \n",
    "        fix_all_seeds(args.seed)\n",
    "        \n",
    "        for batch_idx, batch_data in enumerate(train_dataloader):\n",
    "            input_ids, attention_mask, targets_start, targets_end = \\\n",
    "                batch_data['input_ids'], batch_data['attention_mask'], \\\n",
    "                    batch_data['start_position'], batch_data['end_position']\n",
    "            \n",
    "            input_ids, attention_mask, targets_start, targets_end = \\\n",
    "                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n",
    "\n",
    "            outputs_start, outputs_end = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            \n",
    "            loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n",
    "            loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            if args.fp16:\n",
    "                with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            count += input_ids.size(0)\n",
    "            losses.update(loss.item(), input_ids.size(0))\n",
    "\n",
    "            # if args.fp16:\n",
    "            #     torch.nn.utils.clip_grad_norm_(amp.master_params(self.optimizer), args.max_grad_norm)\n",
    "            # else:\n",
    "            #     torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.max_grad_norm)\n",
    "\n",
    "            if batch_idx % args.gradient_accumulation_steps == 0 or batch_idx == len(train_dataloader) - 1:\n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "            if (batch_idx % args.logging_steps == 0) or (batch_idx+1)==len(train_dataloader):\n",
    "                _s = str(len(str(len(train_dataloader.sampler))))\n",
    "                ret = [\n",
    "                    ('Epoch: {:0>2} [{: >' + _s + '}/{} ({: >3.0f}%)]').format(epoch, count, len(train_dataloader.sampler), 100 * count / len(train_dataloader.sampler)),\n",
    "                    'Train Loss: {: >4.5f}'.format(losses.avg),\n",
    "                ]\n",
    "                print(', '.join(ret))\n",
    "\n",
    "        result_dict['train_loss'].append(losses.avg)\n",
    "        return result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026213,
     "end_time": "2021-11-14T18:27:54.651985",
     "exception": false,
     "start_time": "2021-11-14T18:27:54.625772",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T18:27:54.714808Z",
     "iopub.status.busy": "2021-11-14T18:27:54.714108Z",
     "iopub.status.idle": "2021-11-14T18:27:54.716802Z",
     "shell.execute_reply": "2021-11-14T18:27:54.716401Z",
     "shell.execute_reply.started": "2021-11-14T13:16:18.485935Z"
    },
    "papermill": {
     "duration": 0.0387,
     "end_time": "2021-11-14T18:27:54.716905",
     "exception": false,
     "start_time": "2021-11-14T18:27:54.678205",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def save(self, result, output_dir):\n",
    "        with open(f'{output_dir}/result_dict.json', 'w') as f:\n",
    "            f.write(json.dumps(result, sort_keys=True, indent=4, ensure_ascii=False))\n",
    "\n",
    "    def evaluate(self, valid_dataloader, epoch, result_dict):\n",
    "        losses = AverageMeter()\n",
    "        all_outputs_start, all_outputs_end = [], []\n",
    "        for batch_idx, batch_data in enumerate(valid_dataloader):\n",
    "            self.model = self.model.eval()\n",
    "            input_ids, attention_mask, targets_start, targets_end = \\\n",
    "                batch_data['input_ids'], batch_data['attention_mask'], \\\n",
    "                    batch_data['start_position'], batch_data['end_position']\n",
    "            \n",
    "            input_ids, attention_mask, targets_start, targets_end = \\\n",
    "                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n",
    "            \n",
    "            with torch.no_grad():            \n",
    "                outputs_start, outputs_end = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                )\n",
    "                all_outputs_start.append(outputs_start.cpu().numpy().tolist())\n",
    "                all_outputs_end.append(outputs_end.cpu().numpy().tolist())\n",
    "        \n",
    "                loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n",
    "                losses.update(loss.item(), input_ids.size(0))\n",
    "                \n",
    "        all_outputs_start = np.vstack(all_outputs_start)\n",
    "        all_outputs_end = np.vstack(all_outputs_end)\n",
    "\n",
    "        print('----Validation Results Summary----')\n",
    "        print('Epoch: [{}] Valid Loss: {: >4.5f}'.format(epoch, losses.avg))\n",
    "        result_dict['val_loss'].append(losses.avg)        \n",
    "        return result_dict, all_outputs_start, all_outputs_end\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026608,
     "end_time": "2021-11-14T18:27:54.770003",
     "exception": false,
     "start_time": "2021-11-14T18:27:54.743395",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Initialize Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T18:27:54.832689Z",
     "iopub.status.busy": "2021-11-14T18:27:54.831890Z",
     "iopub.status.idle": "2021-11-14T18:27:54.834693Z",
     "shell.execute_reply": "2021-11-14T18:27:54.834271Z",
     "shell.execute_reply.started": "2021-11-14T13:16:18.500145Z"
    },
    "papermill": {
     "duration": 0.038464,
     "end_time": "2021-11-14T18:27:54.834793",
     "exception": false,
     "start_time": "2021-11-14T18:27:54.796329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_training(args, data, fold):\n",
    "    fix_all_seeds(args.seed)\n",
    "    \n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.makedirs(args.output_dir)\n",
    "    \n",
    "    # model\n",
    "    model_config, tokenizer, model = make_model(args)\n",
    "    if torch.cuda.device_count() >= 1:\n",
    "        print('Model pushed to {} GPU(s), type {}.'.format(\n",
    "            torch.cuda.device_count(), \n",
    "            torch.cuda.get_device_name(0))\n",
    "        )\n",
    "        model = model.cuda() \n",
    "    else:\n",
    "        raise ValueError('CPU training is not supported')\n",
    "    \n",
    "    # data loaders\n",
    "    train_dataloader, valid_dataloader, valid_features, valid_set = make_loader(args, data, tokenizer, fold)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = make_optimizer(args, model)\n",
    "\n",
    "    # scheduler\n",
    "    num_training_steps = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps) * args.epochs\n",
    "    if args.warmup_ratio > 0:\n",
    "        num_warmup_steps = int(args.warmup_ratio * num_training_steps)\n",
    "    else:\n",
    "        num_warmup_steps = 0\n",
    "    print(f\"Total Training Steps: {num_training_steps}, Total Warmup Steps: {num_warmup_steps}\")\n",
    "    scheduler = make_scheduler(args, optimizer, num_warmup_steps, num_training_steps)\n",
    "\n",
    "    # mixed precision training with NVIDIA Apex\n",
    "    if args.fp16:\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
    "    \n",
    "    result_dict = {\n",
    "        'epoch':[], \n",
    "        'train_loss': [], \n",
    "        'val_loss' : [], \n",
    "        'best_val_loss': np.inf\n",
    "    }\n",
    "\n",
    "    return (\n",
    "        model, model_config, tokenizer, optimizer, scheduler, \n",
    "        train_dataloader, valid_dataloader, result_dict, valid_features, valid_set\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026273,
     "end_time": "2021-11-14T18:27:54.887493",
     "exception": false,
     "start_time": "2021-11-14T18:27:54.861220",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Validation Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T18:27:54.954004Z",
     "iopub.status.busy": "2021-11-14T18:27:54.953237Z",
     "iopub.status.idle": "2021-11-14T18:27:54.956197Z",
     "shell.execute_reply": "2021-11-14T18:27:54.955736Z",
     "shell.execute_reply.started": "2021-11-14T13:16:18.514179Z"
    },
    "papermill": {
     "duration": 0.042538,
     "end_time": "2021-11-14T18:27:54.956304",
     "exception": false,
     "start_time": "2021-11-14T18:27:54.913766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ref: https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-infer\n",
    "import collections\n",
    "\n",
    "def postprocess_qa_predictions(examples, features1, raw_predictions, tokenizer, n_best_size = 20, max_answer_length = 30):\n",
    "    features = features1\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "    \n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "    predictions = collections.OrderedDict()\n",
    "\n",
    "    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n",
    "\n",
    "    for example_index, example in examples.iterrows():\n",
    "        feature_indices = features_per_example[example_index]\n",
    "        #print(example['id'],example_index,feature_indices)\n",
    "        min_null_score = None\n",
    "        valid_answers = []\n",
    "        \n",
    "        context = example[\"context\"]\n",
    "        for feature_index in feature_indices:\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "\n",
    "            sequence_ids = features[feature_index][\"sequence_ids\"]\n",
    "            context_index = 1\n",
    "\n",
    "            offset_mapping = [\n",
    "                (o if sequence_ids[k] == context_index else None)\n",
    "                for k, o in enumerate(features[feature_index][\"offset_mapping\"])\n",
    "            ]    \n",
    "        \n",
    "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
    "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "            if min_null_score is None or min_null_score < feature_null_score:\n",
    "                min_null_score = feature_null_score\n",
    "\n",
    "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or offset_mapping[end_index] is None\n",
    "                    ):\n",
    "                        continue\n",
    "                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "\n",
    "                    start_char = offset_mapping[start_index][0]\n",
    "                    end_char = offset_mapping[end_index][1]\n",
    "                    valid_answers.append(\n",
    "                        {\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                            \"text\": context[start_char: end_char]\n",
    "                        }\n",
    "                    )\n",
    "        \n",
    "        if len(valid_answers) > 0:\n",
    "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
    "        else:\n",
    "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
    "        \n",
    "        predictions[example[\"id\"]] = best_answer[\"text\"]\n",
    "        \n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T18:27:55.013839Z",
     "iopub.status.busy": "2021-11-14T18:27:55.013147Z",
     "iopub.status.idle": "2021-11-14T18:27:55.015889Z",
     "shell.execute_reply": "2021-11-14T18:27:55.015477Z",
     "shell.execute_reply.started": "2021-11-14T13:16:18.531390Z"
    },
    "papermill": {
     "duration": 0.033641,
     "end_time": "2021-11-14T18:27:55.016024",
     "exception": false,
     "start_time": "2021-11-14T18:27:54.982383",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025995,
     "end_time": "2021-11-14T18:27:55.068477",
     "exception": false,
     "start_time": "2021-11-14T18:27:55.042482",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T18:27:55.133200Z",
     "iopub.status.busy": "2021-11-14T18:27:55.132490Z",
     "iopub.status.idle": "2021-11-14T18:27:55.135264Z",
     "shell.execute_reply": "2021-11-14T18:27:55.134834Z",
     "shell.execute_reply.started": "2021-11-14T13:16:18.545613Z"
    },
    "papermill": {
     "duration": 0.04088,
     "end_time": "2021-11-14T18:27:55.135374",
     "exception": false,
     "start_time": "2021-11-14T18:27:55.094494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_jacard_scores = []\n",
    "\n",
    "def run(data, fold):\n",
    "    args = Config()\n",
    "    model, model_config, tokenizer, optimizer, scheduler, train_dataloader, \\\n",
    "        valid_dataloader, result_dict, valid_features, valid_set = init_training(args, data, fold)\n",
    "    \n",
    "    trainer = Trainer(model, tokenizer, optimizer, scheduler)\n",
    "    evaluator = Evaluator(model)\n",
    "\n",
    "    train_time_list = []\n",
    "    valid_time_list = []\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        result_dict['epoch'].append(epoch)\n",
    "\n",
    "        # Train\n",
    "        torch.cuda.synchronize()\n",
    "        tic1 = time.time()\n",
    "        result_dict = trainer.train(\n",
    "            args, train_dataloader, \n",
    "            epoch, result_dict\n",
    "        )\n",
    "        torch.cuda.synchronize()\n",
    "        tic2 = time.time() \n",
    "        train_time_list.append(tic2 - tic1)\n",
    "        # Evaluate\n",
    "        torch.cuda.synchronize()\n",
    "        tic3 = time.time()\n",
    "        result_dict, all_outputs_start, all_outputs_end = evaluator.evaluate(\n",
    "            valid_dataloader, epoch, result_dict\n",
    "        )\n",
    "        torch.cuda.synchronize()\n",
    "#         # Get valid jaccard score\n",
    "        valid_features1 = valid_features.copy()\n",
    "        valid_preds = postprocess_qa_predictions(valid_set, valid_features1, (all_outputs_start, all_outputs_end), tokenizer)\n",
    "        valid_set['PredictionString'] = valid_set['id'].map(valid_preds)\n",
    "        valid_set['jaccard'] = valid_set[['answer_text','PredictionString']].apply(lambda x: jaccard(x[0],x[1]), axis=1)\n",
    "        print(\"valid jaccard: \",np.mean(valid_set.jaccard))\n",
    "        all_jacard_scores.append(np.mean(valid_set.jaccard))\n",
    "        \n",
    "        tic4 = time.time() \n",
    "        valid_time_list.append(tic4 - tic3)\n",
    "            \n",
    "        output_dir = os.path.join(args.output_dir, f\"checkpoint-fold-{fold}-epoch-{epoch}\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        if result_dict['val_loss'][-1] < result_dict['best_val_loss']:\n",
    "            print(\"{} Epoch, Best epoch was updated! Valid Loss: {: >4.5f}\".format(epoch, result_dict['val_loss'][-1]))\n",
    "            result_dict[\"best_val_loss\"] = result_dict['val_loss'][-1]        \n",
    "            \n",
    "#             os.makedirs(output_dir, exist_ok=True)\n",
    "            torch.save(model.state_dict(), f\"{output_dir}/pytorch_model.bin\")\n",
    "            model_config.save_pretrained(output_dir)\n",
    "            tokenizer.save_pretrained(output_dir)\n",
    "            print(f\"Saving model checkpoint to {output_dir}.\")\n",
    "            \n",
    "        print()\n",
    "\n",
    "    evaluator.save(result_dict, output_dir)\n",
    "    \n",
    "    print(f\"Total Training Time: {np.sum(train_time_list)}secs, Average Training Time per Epoch: {np.mean(train_time_list)}secs.\")\n",
    "    print(f\"Total Validation Time: {np.sum(valid_time_list)}secs, Average Validation Time per Epoch: {np.mean(valid_time_list)}secs.\")\n",
    "    \n",
    "    #del trainer, evaluator\n",
    "    #del model, model_config, tokenizer\n",
    "    #del optimizer, scheduler\n",
    "    #del train_dataloader, valid_dataloader, result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-14T18:27:55.193181Z",
     "iopub.status.busy": "2021-11-14T18:27:55.192543Z",
     "iopub.status.idle": "2021-11-15T00:02:07.606180Z",
     "shell.execute_reply": "2021-11-15T00:02:07.605665Z",
     "shell.execute_reply.started": "2021-11-14T13:16:18.562875Z"
    },
    "papermill": {
     "duration": 20052.444837,
     "end_time": "2021-11-15T00:02:07.606318",
     "exception": false,
     "start_time": "2021-11-14T18:27:55.161481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "FOLD: 0\n",
      "--------------------------------------------------\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Num examples Train= 26073, Num examples Valid=2427\n",
      "Total Training Steps: 3260, Total Warmup Steps: 326\n",
      "Epoch: 00 [    4/26073 (  0%)], Train Loss: 3.35908\n",
      "Epoch: 00 [   44/26073 (  0%)], Train Loss: 3.30513\n",
      "Epoch: 00 [   84/26073 (  0%)], Train Loss: 3.25556\n",
      "Epoch: 00 [  124/26073 (  0%)], Train Loss: 3.23333\n",
      "Epoch: 00 [  164/26073 (  1%)], Train Loss: 3.19503\n",
      "Epoch: 00 [  204/26073 (  1%)], Train Loss: 3.14787\n",
      "Epoch: 00 [  244/26073 (  1%)], Train Loss: 3.08589\n",
      "Epoch: 00 [  284/26073 (  1%)], Train Loss: 3.01475\n",
      "Epoch: 00 [  324/26073 (  1%)], Train Loss: 2.91228\n",
      "Epoch: 00 [  364/26073 (  1%)], Train Loss: 2.80831\n",
      "Epoch: 00 [  404/26073 (  2%)], Train Loss: 2.68667\n",
      "Epoch: 00 [  444/26073 (  2%)], Train Loss: 2.55810\n",
      "Epoch: 00 [  484/26073 (  2%)], Train Loss: 2.41252\n",
      "Epoch: 00 [  524/26073 (  2%)], Train Loss: 2.30834\n",
      "Epoch: 00 [  564/26073 (  2%)], Train Loss: 2.21533\n",
      "Epoch: 00 [  604/26073 (  2%)], Train Loss: 2.12583\n",
      "Epoch: 00 [  644/26073 (  2%)], Train Loss: 2.05236\n",
      "Epoch: 00 [  684/26073 (  3%)], Train Loss: 1.97124\n",
      "Epoch: 00 [  724/26073 (  3%)], Train Loss: 1.88809\n",
      "Epoch: 00 [  764/26073 (  3%)], Train Loss: 1.82719\n",
      "Epoch: 00 [  804/26073 (  3%)], Train Loss: 1.76538\n",
      "Epoch: 00 [  844/26073 (  3%)], Train Loss: 1.71356\n",
      "Epoch: 00 [  884/26073 (  3%)], Train Loss: 1.65420\n",
      "Epoch: 00 [  924/26073 (  4%)], Train Loss: 1.60511\n",
      "Epoch: 00 [  964/26073 (  4%)], Train Loss: 1.56635\n",
      "Epoch: 00 [ 1004/26073 (  4%)], Train Loss: 1.52905\n",
      "Epoch: 00 [ 1044/26073 (  4%)], Train Loss: 1.48521\n",
      "Epoch: 00 [ 1084/26073 (  4%)], Train Loss: 1.44727\n",
      "Epoch: 00 [ 1124/26073 (  4%)], Train Loss: 1.41592\n",
      "Epoch: 00 [ 1164/26073 (  4%)], Train Loss: 1.37692\n",
      "Epoch: 00 [ 1204/26073 (  5%)], Train Loss: 1.35768\n",
      "Epoch: 00 [ 1244/26073 (  5%)], Train Loss: 1.33047\n",
      "Epoch: 00 [ 1284/26073 (  5%)], Train Loss: 1.30552\n",
      "Epoch: 00 [ 1324/26073 (  5%)], Train Loss: 1.27877\n",
      "Epoch: 00 [ 1364/26073 (  5%)], Train Loss: 1.25854\n",
      "Epoch: 00 [ 1404/26073 (  5%)], Train Loss: 1.23678\n",
      "Epoch: 00 [ 1444/26073 (  6%)], Train Loss: 1.21446\n",
      "Epoch: 00 [ 1484/26073 (  6%)], Train Loss: 1.19747\n",
      "Epoch: 00 [ 1524/26073 (  6%)], Train Loss: 1.18180\n",
      "Epoch: 00 [ 1564/26073 (  6%)], Train Loss: 1.16468\n",
      "Epoch: 00 [ 1604/26073 (  6%)], Train Loss: 1.14619\n",
      "Epoch: 00 [ 1644/26073 (  6%)], Train Loss: 1.12884\n",
      "Epoch: 00 [ 1684/26073 (  6%)], Train Loss: 1.11286\n",
      "Epoch: 00 [ 1724/26073 (  7%)], Train Loss: 1.10022\n",
      "Epoch: 00 [ 1764/26073 (  7%)], Train Loss: 1.08863\n",
      "Epoch: 00 [ 1804/26073 (  7%)], Train Loss: 1.07449\n",
      "Epoch: 00 [ 1844/26073 (  7%)], Train Loss: 1.06419\n",
      "Epoch: 00 [ 1884/26073 (  7%)], Train Loss: 1.05186\n",
      "Epoch: 00 [ 1924/26073 (  7%)], Train Loss: 1.04309\n",
      "Epoch: 00 [ 1964/26073 (  8%)], Train Loss: 1.03201\n",
      "Epoch: 00 [ 2004/26073 (  8%)], Train Loss: 1.02239\n",
      "Epoch: 00 [ 2044/26073 (  8%)], Train Loss: 1.01347\n",
      "Epoch: 00 [ 2084/26073 (  8%)], Train Loss: 1.00284\n",
      "Epoch: 00 [ 2124/26073 (  8%)], Train Loss: 0.99719\n",
      "Epoch: 00 [ 2164/26073 (  8%)], Train Loss: 0.99057\n",
      "Epoch: 00 [ 2204/26073 (  8%)], Train Loss: 0.98488\n",
      "Epoch: 00 [ 2244/26073 (  9%)], Train Loss: 0.97834\n",
      "Epoch: 00 [ 2284/26073 (  9%)], Train Loss: 0.97076\n",
      "Epoch: 00 [ 2324/26073 (  9%)], Train Loss: 0.96524\n",
      "Epoch: 00 [ 2364/26073 (  9%)], Train Loss: 0.95944\n",
      "Epoch: 00 [ 2404/26073 (  9%)], Train Loss: 0.95129\n",
      "Epoch: 00 [ 2444/26073 (  9%)], Train Loss: 0.94508\n",
      "Epoch: 00 [ 2484/26073 ( 10%)], Train Loss: 0.94091\n",
      "Epoch: 00 [ 2524/26073 ( 10%)], Train Loss: 0.93558\n",
      "Epoch: 00 [ 2564/26073 ( 10%)], Train Loss: 0.92908\n",
      "Epoch: 00 [ 2604/26073 ( 10%)], Train Loss: 0.92448\n",
      "Epoch: 00 [ 2644/26073 ( 10%)], Train Loss: 0.92308\n",
      "Epoch: 00 [ 2684/26073 ( 10%)], Train Loss: 0.91729\n",
      "Epoch: 00 [ 2724/26073 ( 10%)], Train Loss: 0.91265\n",
      "Epoch: 00 [ 2764/26073 ( 11%)], Train Loss: 0.91270\n",
      "Epoch: 00 [ 2804/26073 ( 11%)], Train Loss: 0.90930\n",
      "Epoch: 00 [ 2844/26073 ( 11%)], Train Loss: 0.90498\n",
      "Epoch: 00 [ 2884/26073 ( 11%)], Train Loss: 0.89898\n",
      "Epoch: 00 [ 2924/26073 ( 11%)], Train Loss: 0.89575\n",
      "Epoch: 00 [ 2964/26073 ( 11%)], Train Loss: 0.89154\n",
      "Epoch: 00 [ 3004/26073 ( 12%)], Train Loss: 0.88844\n",
      "Epoch: 00 [ 3044/26073 ( 12%)], Train Loss: 0.88333\n",
      "Epoch: 00 [ 3084/26073 ( 12%)], Train Loss: 0.87910\n",
      "Epoch: 00 [ 3124/26073 ( 12%)], Train Loss: 0.87621\n",
      "Epoch: 00 [ 3164/26073 ( 12%)], Train Loss: 0.86971\n",
      "Epoch: 00 [ 3204/26073 ( 12%)], Train Loss: 0.86509\n",
      "Epoch: 00 [ 3244/26073 ( 12%)], Train Loss: 0.86006\n",
      "Epoch: 00 [ 3284/26073 ( 13%)], Train Loss: 0.85873\n",
      "Epoch: 00 [ 3324/26073 ( 13%)], Train Loss: 0.85402\n",
      "Epoch: 00 [ 3364/26073 ( 13%)], Train Loss: 0.84998\n",
      "Epoch: 00 [ 3404/26073 ( 13%)], Train Loss: 0.84518\n",
      "Epoch: 00 [ 3444/26073 ( 13%)], Train Loss: 0.84320\n",
      "Epoch: 00 [ 3484/26073 ( 13%)], Train Loss: 0.83798\n",
      "Epoch: 00 [ 3524/26073 ( 14%)], Train Loss: 0.83295\n",
      "Epoch: 00 [ 3564/26073 ( 14%)], Train Loss: 0.82791\n",
      "Epoch: 00 [ 3604/26073 ( 14%)], Train Loss: 0.82384\n",
      "Epoch: 00 [ 3644/26073 ( 14%)], Train Loss: 0.81981\n",
      "Epoch: 00 [ 3684/26073 ( 14%)], Train Loss: 0.81673\n",
      "Epoch: 00 [ 3724/26073 ( 14%)], Train Loss: 0.81495\n",
      "Epoch: 00 [ 3764/26073 ( 14%)], Train Loss: 0.81122\n",
      "Epoch: 00 [ 3804/26073 ( 15%)], Train Loss: 0.80940\n",
      "Epoch: 00 [ 3844/26073 ( 15%)], Train Loss: 0.80718\n",
      "Epoch: 00 [ 3884/26073 ( 15%)], Train Loss: 0.80215\n",
      "Epoch: 00 [ 3924/26073 ( 15%)], Train Loss: 0.79676\n",
      "Epoch: 00 [ 3964/26073 ( 15%)], Train Loss: 0.79364\n",
      "Epoch: 00 [ 4004/26073 ( 15%)], Train Loss: 0.79129\n",
      "Epoch: 00 [ 4044/26073 ( 16%)], Train Loss: 0.78790\n",
      "Epoch: 00 [ 4084/26073 ( 16%)], Train Loss: 0.78288\n",
      "Epoch: 00 [ 4124/26073 ( 16%)], Train Loss: 0.77956\n",
      "Epoch: 00 [ 4164/26073 ( 16%)], Train Loss: 0.77798\n",
      "Epoch: 00 [ 4204/26073 ( 16%)], Train Loss: 0.77502\n",
      "Epoch: 00 [ 4244/26073 ( 16%)], Train Loss: 0.77266\n",
      "Epoch: 00 [ 4284/26073 ( 16%)], Train Loss: 0.77258\n",
      "Epoch: 00 [ 4324/26073 ( 17%)], Train Loss: 0.76987\n",
      "Epoch: 00 [ 4364/26073 ( 17%)], Train Loss: 0.76724\n",
      "Epoch: 00 [ 4404/26073 ( 17%)], Train Loss: 0.76609\n",
      "Epoch: 00 [ 4444/26073 ( 17%)], Train Loss: 0.76455\n",
      "Epoch: 00 [ 4484/26073 ( 17%)], Train Loss: 0.76145\n",
      "Epoch: 00 [ 4524/26073 ( 17%)], Train Loss: 0.75883\n",
      "Epoch: 00 [ 4564/26073 ( 18%)], Train Loss: 0.75670\n",
      "Epoch: 00 [ 4604/26073 ( 18%)], Train Loss: 0.75393\n",
      "Epoch: 00 [ 4644/26073 ( 18%)], Train Loss: 0.75108\n",
      "Epoch: 00 [ 4684/26073 ( 18%)], Train Loss: 0.74763\n",
      "Epoch: 00 [ 4724/26073 ( 18%)], Train Loss: 0.74530\n",
      "Epoch: 00 [ 4764/26073 ( 18%)], Train Loss: 0.74485\n",
      "Epoch: 00 [ 4804/26073 ( 18%)], Train Loss: 0.74505\n",
      "Epoch: 00 [ 4844/26073 ( 19%)], Train Loss: 0.74269\n",
      "Epoch: 00 [ 4884/26073 ( 19%)], Train Loss: 0.74022\n",
      "Epoch: 00 [ 4924/26073 ( 19%)], Train Loss: 0.73854\n",
      "Epoch: 00 [ 4964/26073 ( 19%)], Train Loss: 0.73632\n",
      "Epoch: 00 [ 5004/26073 ( 19%)], Train Loss: 0.73384\n",
      "Epoch: 00 [ 5044/26073 ( 19%)], Train Loss: 0.73367\n",
      "Epoch: 00 [ 5084/26073 ( 19%)], Train Loss: 0.73148\n",
      "Epoch: 00 [ 5124/26073 ( 20%)], Train Loss: 0.72871\n",
      "Epoch: 00 [ 5164/26073 ( 20%)], Train Loss: 0.72734\n",
      "Epoch: 00 [ 5204/26073 ( 20%)], Train Loss: 0.72502\n",
      "Epoch: 00 [ 5244/26073 ( 20%)], Train Loss: 0.72291\n",
      "Epoch: 00 [ 5284/26073 ( 20%)], Train Loss: 0.72201\n",
      "Epoch: 00 [ 5324/26073 ( 20%)], Train Loss: 0.72033\n",
      "Epoch: 00 [ 5364/26073 ( 21%)], Train Loss: 0.71942\n",
      "Epoch: 00 [ 5404/26073 ( 21%)], Train Loss: 0.71714\n",
      "Epoch: 00 [ 5444/26073 ( 21%)], Train Loss: 0.71459\n",
      "Epoch: 00 [ 5484/26073 ( 21%)], Train Loss: 0.71188\n",
      "Epoch: 00 [ 5524/26073 ( 21%)], Train Loss: 0.71170\n",
      "Epoch: 00 [ 5564/26073 ( 21%)], Train Loss: 0.71121\n",
      "Epoch: 00 [ 5604/26073 ( 21%)], Train Loss: 0.70818\n",
      "Epoch: 00 [ 5644/26073 ( 22%)], Train Loss: 0.70582\n",
      "Epoch: 00 [ 5684/26073 ( 22%)], Train Loss: 0.70485\n",
      "Epoch: 00 [ 5724/26073 ( 22%)], Train Loss: 0.70371\n",
      "Epoch: 00 [ 5764/26073 ( 22%)], Train Loss: 0.70069\n",
      "Epoch: 00 [ 5804/26073 ( 22%)], Train Loss: 0.69880\n",
      "Epoch: 00 [ 5844/26073 ( 22%)], Train Loss: 0.69677\n",
      "Epoch: 00 [ 5884/26073 ( 23%)], Train Loss: 0.69510\n",
      "Epoch: 00 [ 5924/26073 ( 23%)], Train Loss: 0.69330\n",
      "Epoch: 00 [ 5964/26073 ( 23%)], Train Loss: 0.69306\n",
      "Epoch: 00 [ 6004/26073 ( 23%)], Train Loss: 0.69055\n",
      "Epoch: 00 [ 6044/26073 ( 23%)], Train Loss: 0.68846\n",
      "Epoch: 00 [ 6084/26073 ( 23%)], Train Loss: 0.68653\n",
      "Epoch: 00 [ 6124/26073 ( 23%)], Train Loss: 0.68589\n",
      "Epoch: 00 [ 6164/26073 ( 24%)], Train Loss: 0.68516\n",
      "Epoch: 00 [ 6204/26073 ( 24%)], Train Loss: 0.68433\n",
      "Epoch: 00 [ 6244/26073 ( 24%)], Train Loss: 0.68277\n",
      "Epoch: 00 [ 6284/26073 ( 24%)], Train Loss: 0.68249\n",
      "Epoch: 00 [ 6324/26073 ( 24%)], Train Loss: 0.68107\n",
      "Epoch: 00 [ 6364/26073 ( 24%)], Train Loss: 0.68077\n",
      "Epoch: 00 [ 6404/26073 ( 25%)], Train Loss: 0.67981\n",
      "Epoch: 00 [ 6444/26073 ( 25%)], Train Loss: 0.67877\n",
      "Epoch: 00 [ 6484/26073 ( 25%)], Train Loss: 0.67826\n",
      "Epoch: 00 [ 6524/26073 ( 25%)], Train Loss: 0.67613\n",
      "Epoch: 00 [ 6564/26073 ( 25%)], Train Loss: 0.67529\n",
      "Epoch: 00 [ 6604/26073 ( 25%)], Train Loss: 0.67421\n",
      "Epoch: 00 [ 6644/26073 ( 25%)], Train Loss: 0.67326\n",
      "Epoch: 00 [ 6684/26073 ( 26%)], Train Loss: 0.67322\n",
      "Epoch: 00 [ 6724/26073 ( 26%)], Train Loss: 0.67086\n",
      "Epoch: 00 [ 6764/26073 ( 26%)], Train Loss: 0.67115\n",
      "Epoch: 00 [ 6804/26073 ( 26%)], Train Loss: 0.67030\n",
      "Epoch: 00 [ 6844/26073 ( 26%)], Train Loss: 0.66952\n",
      "Epoch: 00 [ 6884/26073 ( 26%)], Train Loss: 0.66796\n",
      "Epoch: 00 [ 6924/26073 ( 27%)], Train Loss: 0.66651\n",
      "Epoch: 00 [ 6964/26073 ( 27%)], Train Loss: 0.66719\n",
      "Epoch: 00 [ 7004/26073 ( 27%)], Train Loss: 0.66631\n",
      "Epoch: 00 [ 7044/26073 ( 27%)], Train Loss: 0.66468\n",
      "Epoch: 00 [ 7084/26073 ( 27%)], Train Loss: 0.66374\n",
      "Epoch: 00 [ 7124/26073 ( 27%)], Train Loss: 0.66180\n",
      "Epoch: 00 [ 7164/26073 ( 27%)], Train Loss: 0.66019\n",
      "Epoch: 00 [ 7204/26073 ( 28%)], Train Loss: 0.66155\n",
      "Epoch: 00 [ 7244/26073 ( 28%)], Train Loss: 0.66033\n",
      "Epoch: 00 [ 7284/26073 ( 28%)], Train Loss: 0.65956\n",
      "Epoch: 00 [ 7324/26073 ( 28%)], Train Loss: 0.65830\n",
      "Epoch: 00 [ 7364/26073 ( 28%)], Train Loss: 0.65768\n",
      "Epoch: 00 [ 7404/26073 ( 28%)], Train Loss: 0.65687\n",
      "Epoch: 00 [ 7444/26073 ( 29%)], Train Loss: 0.65492\n",
      "Epoch: 00 [ 7484/26073 ( 29%)], Train Loss: 0.65358\n",
      "Epoch: 00 [ 7524/26073 ( 29%)], Train Loss: 0.65126\n",
      "Epoch: 00 [ 7564/26073 ( 29%)], Train Loss: 0.65036\n",
      "Epoch: 00 [ 7604/26073 ( 29%)], Train Loss: 0.64865\n",
      "Epoch: 00 [ 7644/26073 ( 29%)], Train Loss: 0.64752\n",
      "Epoch: 00 [ 7684/26073 ( 29%)], Train Loss: 0.64496\n",
      "Epoch: 00 [ 7724/26073 ( 30%)], Train Loss: 0.64472\n",
      "Epoch: 00 [ 7764/26073 ( 30%)], Train Loss: 0.64379\n",
      "Epoch: 00 [ 7804/26073 ( 30%)], Train Loss: 0.64257\n",
      "Epoch: 00 [ 7844/26073 ( 30%)], Train Loss: 0.64211\n",
      "Epoch: 00 [ 7884/26073 ( 30%)], Train Loss: 0.63990\n",
      "Epoch: 00 [ 7924/26073 ( 30%)], Train Loss: 0.63991\n",
      "Epoch: 00 [ 7964/26073 ( 31%)], Train Loss: 0.63889\n",
      "Epoch: 00 [ 8004/26073 ( 31%)], Train Loss: 0.63757\n",
      "Epoch: 00 [ 8044/26073 ( 31%)], Train Loss: 0.63755\n",
      "Epoch: 00 [ 8084/26073 ( 31%)], Train Loss: 0.63704\n",
      "Epoch: 00 [ 8124/26073 ( 31%)], Train Loss: 0.63633\n",
      "Epoch: 00 [ 8164/26073 ( 31%)], Train Loss: 0.63562\n",
      "Epoch: 00 [ 8204/26073 ( 31%)], Train Loss: 0.63392\n",
      "Epoch: 00 [ 8244/26073 ( 32%)], Train Loss: 0.63428\n",
      "Epoch: 00 [ 8284/26073 ( 32%)], Train Loss: 0.63364\n",
      "Epoch: 00 [ 8324/26073 ( 32%)], Train Loss: 0.63298\n",
      "Epoch: 00 [ 8364/26073 ( 32%)], Train Loss: 0.63262\n",
      "Epoch: 00 [ 8404/26073 ( 32%)], Train Loss: 0.63181\n",
      "Epoch: 00 [ 8444/26073 ( 32%)], Train Loss: 0.63002\n",
      "Epoch: 00 [ 8484/26073 ( 33%)], Train Loss: 0.62992\n",
      "Epoch: 00 [ 8524/26073 ( 33%)], Train Loss: 0.62938\n",
      "Epoch: 00 [ 8564/26073 ( 33%)], Train Loss: 0.62934\n",
      "Epoch: 00 [ 8604/26073 ( 33%)], Train Loss: 0.62848\n",
      "Epoch: 00 [ 8644/26073 ( 33%)], Train Loss: 0.62821\n",
      "Epoch: 00 [ 8684/26073 ( 33%)], Train Loss: 0.62741\n",
      "Epoch: 00 [ 8724/26073 ( 33%)], Train Loss: 0.62634\n",
      "Epoch: 00 [ 8764/26073 ( 34%)], Train Loss: 0.62576\n",
      "Epoch: 00 [ 8804/26073 ( 34%)], Train Loss: 0.62536\n",
      "Epoch: 00 [ 8844/26073 ( 34%)], Train Loss: 0.62454\n",
      "Epoch: 00 [ 8884/26073 ( 34%)], Train Loss: 0.62336\n",
      "Epoch: 00 [ 8924/26073 ( 34%)], Train Loss: 0.62428\n",
      "Epoch: 00 [ 8964/26073 ( 34%)], Train Loss: 0.62449\n",
      "Epoch: 00 [ 9004/26073 ( 35%)], Train Loss: 0.62471\n",
      "Epoch: 00 [ 9044/26073 ( 35%)], Train Loss: 0.62343\n",
      "Epoch: 00 [ 9084/26073 ( 35%)], Train Loss: 0.62263\n",
      "Epoch: 00 [ 9124/26073 ( 35%)], Train Loss: 0.62228\n",
      "Epoch: 00 [ 9164/26073 ( 35%)], Train Loss: 0.62123\n",
      "Epoch: 00 [ 9204/26073 ( 35%)], Train Loss: 0.62076\n",
      "Epoch: 00 [ 9244/26073 ( 35%)], Train Loss: 0.62027\n",
      "Epoch: 00 [ 9284/26073 ( 36%)], Train Loss: 0.62002\n",
      "Epoch: 00 [ 9324/26073 ( 36%)], Train Loss: 0.61863\n",
      "Epoch: 00 [ 9364/26073 ( 36%)], Train Loss: 0.61773\n",
      "Epoch: 00 [ 9404/26073 ( 36%)], Train Loss: 0.61629\n",
      "Epoch: 00 [ 9444/26073 ( 36%)], Train Loss: 0.61509\n",
      "Epoch: 00 [ 9484/26073 ( 36%)], Train Loss: 0.61429\n",
      "Epoch: 00 [ 9524/26073 ( 37%)], Train Loss: 0.61449\n",
      "Epoch: 00 [ 9564/26073 ( 37%)], Train Loss: 0.61401\n",
      "Epoch: 00 [ 9604/26073 ( 37%)], Train Loss: 0.61428\n",
      "Epoch: 00 [ 9644/26073 ( 37%)], Train Loss: 0.61306\n",
      "Epoch: 00 [ 9684/26073 ( 37%)], Train Loss: 0.61428\n",
      "Epoch: 00 [ 9724/26073 ( 37%)], Train Loss: 0.61331\n",
      "Epoch: 00 [ 9764/26073 ( 37%)], Train Loss: 0.61247\n",
      "Epoch: 00 [ 9804/26073 ( 38%)], Train Loss: 0.61198\n",
      "Epoch: 00 [ 9844/26073 ( 38%)], Train Loss: 0.61075\n",
      "Epoch: 00 [ 9884/26073 ( 38%)], Train Loss: 0.61047\n",
      "Epoch: 00 [ 9924/26073 ( 38%)], Train Loss: 0.60954\n",
      "Epoch: 00 [ 9964/26073 ( 38%)], Train Loss: 0.60879\n",
      "Epoch: 00 [10004/26073 ( 38%)], Train Loss: 0.60878\n",
      "Epoch: 00 [10044/26073 ( 39%)], Train Loss: 0.60823\n",
      "Epoch: 00 [10084/26073 ( 39%)], Train Loss: 0.60717\n",
      "Epoch: 00 [10124/26073 ( 39%)], Train Loss: 0.60682\n",
      "Epoch: 00 [10164/26073 ( 39%)], Train Loss: 0.60667\n",
      "Epoch: 00 [10204/26073 ( 39%)], Train Loss: 0.60555\n",
      "Epoch: 00 [10244/26073 ( 39%)], Train Loss: 0.60583\n",
      "Epoch: 00 [10284/26073 ( 39%)], Train Loss: 0.60618\n",
      "Epoch: 00 [10324/26073 ( 40%)], Train Loss: 0.60657\n",
      "Epoch: 00 [10364/26073 ( 40%)], Train Loss: 0.60557\n",
      "Epoch: 00 [10404/26073 ( 40%)], Train Loss: 0.60448\n",
      "Epoch: 00 [10444/26073 ( 40%)], Train Loss: 0.60391\n",
      "Epoch: 00 [10484/26073 ( 40%)], Train Loss: 0.60390\n",
      "Epoch: 00 [10524/26073 ( 40%)], Train Loss: 0.60263\n",
      "Epoch: 00 [10564/26073 ( 41%)], Train Loss: 0.60287\n",
      "Epoch: 00 [10604/26073 ( 41%)], Train Loss: 0.60190\n",
      "Epoch: 00 [10644/26073 ( 41%)], Train Loss: 0.60122\n",
      "Epoch: 00 [10684/26073 ( 41%)], Train Loss: 0.59980\n",
      "Epoch: 00 [10724/26073 ( 41%)], Train Loss: 0.59888\n",
      "Epoch: 00 [10764/26073 ( 41%)], Train Loss: 0.59788\n",
      "Epoch: 00 [10804/26073 ( 41%)], Train Loss: 0.59652\n",
      "Epoch: 00 [10844/26073 ( 42%)], Train Loss: 0.59613\n",
      "Epoch: 00 [10884/26073 ( 42%)], Train Loss: 0.59520\n",
      "Epoch: 00 [10924/26073 ( 42%)], Train Loss: 0.59437\n",
      "Epoch: 00 [10964/26073 ( 42%)], Train Loss: 0.59374\n",
      "Epoch: 00 [11004/26073 ( 42%)], Train Loss: 0.59329\n",
      "Epoch: 00 [11044/26073 ( 42%)], Train Loss: 0.59291\n",
      "Epoch: 00 [11084/26073 ( 43%)], Train Loss: 0.59195\n",
      "Epoch: 00 [11124/26073 ( 43%)], Train Loss: 0.59111\n",
      "Epoch: 00 [11164/26073 ( 43%)], Train Loss: 0.59048\n",
      "Epoch: 00 [11204/26073 ( 43%)], Train Loss: 0.58981\n",
      "Epoch: 00 [11244/26073 ( 43%)], Train Loss: 0.58927\n",
      "Epoch: 00 [11284/26073 ( 43%)], Train Loss: 0.58861\n",
      "Epoch: 00 [11324/26073 ( 43%)], Train Loss: 0.58730\n",
      "Epoch: 00 [11364/26073 ( 44%)], Train Loss: 0.58697\n",
      "Epoch: 00 [11404/26073 ( 44%)], Train Loss: 0.58692\n",
      "Epoch: 00 [11444/26073 ( 44%)], Train Loss: 0.58575\n",
      "Epoch: 00 [11484/26073 ( 44%)], Train Loss: 0.58484\n",
      "Epoch: 00 [11524/26073 ( 44%)], Train Loss: 0.58408\n",
      "Epoch: 00 [11564/26073 ( 44%)], Train Loss: 0.58349\n",
      "Epoch: 00 [11604/26073 ( 45%)], Train Loss: 0.58293\n",
      "Epoch: 00 [11644/26073 ( 45%)], Train Loss: 0.58259\n",
      "Epoch: 00 [11684/26073 ( 45%)], Train Loss: 0.58117\n",
      "Epoch: 00 [11724/26073 ( 45%)], Train Loss: 0.58100\n",
      "Epoch: 00 [11764/26073 ( 45%)], Train Loss: 0.58006\n",
      "Epoch: 00 [11804/26073 ( 45%)], Train Loss: 0.57981\n",
      "Epoch: 00 [11844/26073 ( 45%)], Train Loss: 0.57910\n",
      "Epoch: 00 [11884/26073 ( 46%)], Train Loss: 0.57910\n",
      "Epoch: 00 [11924/26073 ( 46%)], Train Loss: 0.57882\n",
      "Epoch: 00 [11964/26073 ( 46%)], Train Loss: 0.57901\n",
      "Epoch: 00 [12004/26073 ( 46%)], Train Loss: 0.57859\n",
      "Epoch: 00 [12044/26073 ( 46%)], Train Loss: 0.57819\n",
      "Epoch: 00 [12084/26073 ( 46%)], Train Loss: 0.57745\n",
      "Epoch: 00 [12124/26073 ( 47%)], Train Loss: 0.57645\n",
      "Epoch: 00 [12164/26073 ( 47%)], Train Loss: 0.57606\n",
      "Epoch: 00 [12204/26073 ( 47%)], Train Loss: 0.57534\n",
      "Epoch: 00 [12244/26073 ( 47%)], Train Loss: 0.57481\n",
      "Epoch: 00 [12284/26073 ( 47%)], Train Loss: 0.57497\n",
      "Epoch: 00 [12324/26073 ( 47%)], Train Loss: 0.57409\n",
      "Epoch: 00 [12364/26073 ( 47%)], Train Loss: 0.57320\n",
      "Epoch: 00 [12404/26073 ( 48%)], Train Loss: 0.57329\n",
      "Epoch: 00 [12444/26073 ( 48%)], Train Loss: 0.57312\n",
      "Epoch: 00 [12484/26073 ( 48%)], Train Loss: 0.57218\n",
      "Epoch: 00 [12524/26073 ( 48%)], Train Loss: 0.57182\n",
      "Epoch: 00 [12564/26073 ( 48%)], Train Loss: 0.57100\n",
      "Epoch: 00 [12604/26073 ( 48%)], Train Loss: 0.57061\n",
      "Epoch: 00 [12644/26073 ( 48%)], Train Loss: 0.56985\n",
      "Epoch: 00 [12684/26073 ( 49%)], Train Loss: 0.57066\n",
      "Epoch: 00 [12724/26073 ( 49%)], Train Loss: 0.57019\n",
      "Epoch: 00 [12764/26073 ( 49%)], Train Loss: 0.57038\n",
      "Epoch: 00 [12804/26073 ( 49%)], Train Loss: 0.57011\n",
      "Epoch: 00 [12844/26073 ( 49%)], Train Loss: 0.57005\n",
      "Epoch: 00 [12884/26073 ( 49%)], Train Loss: 0.56930\n",
      "Epoch: 00 [12924/26073 ( 50%)], Train Loss: 0.56895\n",
      "Epoch: 00 [12964/26073 ( 50%)], Train Loss: 0.56871\n",
      "Epoch: 00 [13004/26073 ( 50%)], Train Loss: 0.56760\n",
      "Epoch: 00 [13044/26073 ( 50%)], Train Loss: 0.56688\n",
      "Epoch: 00 [13084/26073 ( 50%)], Train Loss: 0.56645\n",
      "Epoch: 00 [13124/26073 ( 50%)], Train Loss: 0.56634\n",
      "Epoch: 00 [13164/26073 ( 50%)], Train Loss: 0.56620\n",
      "Epoch: 00 [13204/26073 ( 51%)], Train Loss: 0.56540\n",
      "Epoch: 00 [13244/26073 ( 51%)], Train Loss: 0.56478\n",
      "Epoch: 00 [13284/26073 ( 51%)], Train Loss: 0.56421\n",
      "Epoch: 00 [13324/26073 ( 51%)], Train Loss: 0.56386\n",
      "Epoch: 00 [13364/26073 ( 51%)], Train Loss: 0.56315\n",
      "Epoch: 00 [13404/26073 ( 51%)], Train Loss: 0.56200\n",
      "Epoch: 00 [13444/26073 ( 52%)], Train Loss: 0.56206\n",
      "Epoch: 00 [13484/26073 ( 52%)], Train Loss: 0.56137\n",
      "Epoch: 00 [13524/26073 ( 52%)], Train Loss: 0.56072\n",
      "Epoch: 00 [13564/26073 ( 52%)], Train Loss: 0.56013\n",
      "Epoch: 00 [13604/26073 ( 52%)], Train Loss: 0.55965\n",
      "Epoch: 00 [13644/26073 ( 52%)], Train Loss: 0.55920\n",
      "Epoch: 00 [13684/26073 ( 52%)], Train Loss: 0.55937\n",
      "Epoch: 00 [13724/26073 ( 53%)], Train Loss: 0.55868\n",
      "Epoch: 00 [13764/26073 ( 53%)], Train Loss: 0.55837\n",
      "Epoch: 00 [13804/26073 ( 53%)], Train Loss: 0.55758\n",
      "Epoch: 00 [13844/26073 ( 53%)], Train Loss: 0.55717\n",
      "Epoch: 00 [13884/26073 ( 53%)], Train Loss: 0.55629\n",
      "Epoch: 00 [13924/26073 ( 53%)], Train Loss: 0.55574\n",
      "Epoch: 00 [13964/26073 ( 54%)], Train Loss: 0.55572\n",
      "Epoch: 00 [14004/26073 ( 54%)], Train Loss: 0.55566\n",
      "Epoch: 00 [14044/26073 ( 54%)], Train Loss: 0.55531\n",
      "Epoch: 00 [14084/26073 ( 54%)], Train Loss: 0.55489\n",
      "Epoch: 00 [14124/26073 ( 54%)], Train Loss: 0.55479\n",
      "Epoch: 00 [14164/26073 ( 54%)], Train Loss: 0.55421\n",
      "Epoch: 00 [14204/26073 ( 54%)], Train Loss: 0.55384\n",
      "Epoch: 00 [14244/26073 ( 55%)], Train Loss: 0.55267\n",
      "Epoch: 00 [14284/26073 ( 55%)], Train Loss: 0.55235\n",
      "Epoch: 00 [14324/26073 ( 55%)], Train Loss: 0.55193\n",
      "Epoch: 00 [14364/26073 ( 55%)], Train Loss: 0.55127\n",
      "Epoch: 00 [14404/26073 ( 55%)], Train Loss: 0.55065\n",
      "Epoch: 00 [14444/26073 ( 55%)], Train Loss: 0.55007\n",
      "Epoch: 00 [14484/26073 ( 56%)], Train Loss: 0.54971\n",
      "Epoch: 00 [14524/26073 ( 56%)], Train Loss: 0.54940\n",
      "Epoch: 00 [14564/26073 ( 56%)], Train Loss: 0.54886\n",
      "Epoch: 00 [14604/26073 ( 56%)], Train Loss: 0.54877\n",
      "Epoch: 00 [14644/26073 ( 56%)], Train Loss: 0.54853\n",
      "Epoch: 00 [14684/26073 ( 56%)], Train Loss: 0.54794\n",
      "Epoch: 00 [14724/26073 ( 56%)], Train Loss: 0.54764\n",
      "Epoch: 00 [14764/26073 ( 57%)], Train Loss: 0.54702\n",
      "Epoch: 00 [14804/26073 ( 57%)], Train Loss: 0.54656\n",
      "Epoch: 00 [14844/26073 ( 57%)], Train Loss: 0.54651\n",
      "Epoch: 00 [14884/26073 ( 57%)], Train Loss: 0.54654\n",
      "Epoch: 00 [14924/26073 ( 57%)], Train Loss: 0.54621\n",
      "Epoch: 00 [14964/26073 ( 57%)], Train Loss: 0.54584\n",
      "Epoch: 00 [15004/26073 ( 58%)], Train Loss: 0.54539\n",
      "Epoch: 00 [15044/26073 ( 58%)], Train Loss: 0.54526\n",
      "Epoch: 00 [15084/26073 ( 58%)], Train Loss: 0.54545\n",
      "Epoch: 00 [15124/26073 ( 58%)], Train Loss: 0.54484\n",
      "Epoch: 00 [15164/26073 ( 58%)], Train Loss: 0.54460\n",
      "Epoch: 00 [15204/26073 ( 58%)], Train Loss: 0.54442\n",
      "Epoch: 00 [15244/26073 ( 58%)], Train Loss: 0.54414\n",
      "Epoch: 00 [15284/26073 ( 59%)], Train Loss: 0.54402\n",
      "Epoch: 00 [15324/26073 ( 59%)], Train Loss: 0.54379\n",
      "Epoch: 00 [15364/26073 ( 59%)], Train Loss: 0.54346\n",
      "Epoch: 00 [15404/26073 ( 59%)], Train Loss: 0.54298\n",
      "Epoch: 00 [15444/26073 ( 59%)], Train Loss: 0.54280\n",
      "Epoch: 00 [15484/26073 ( 59%)], Train Loss: 0.54265\n",
      "Epoch: 00 [15524/26073 ( 60%)], Train Loss: 0.54231\n",
      "Epoch: 00 [15564/26073 ( 60%)], Train Loss: 0.54208\n",
      "Epoch: 00 [15604/26073 ( 60%)], Train Loss: 0.54170\n",
      "Epoch: 00 [15644/26073 ( 60%)], Train Loss: 0.54148\n",
      "Epoch: 00 [15684/26073 ( 60%)], Train Loss: 0.54072\n",
      "Epoch: 00 [15724/26073 ( 60%)], Train Loss: 0.54046\n",
      "Epoch: 00 [15764/26073 ( 60%)], Train Loss: 0.53987\n",
      "Epoch: 00 [15804/26073 ( 61%)], Train Loss: 0.53971\n",
      "Epoch: 00 [15844/26073 ( 61%)], Train Loss: 0.53937\n",
      "Epoch: 00 [15884/26073 ( 61%)], Train Loss: 0.53928\n",
      "Epoch: 00 [15924/26073 ( 61%)], Train Loss: 0.53890\n",
      "Epoch: 00 [15964/26073 ( 61%)], Train Loss: 0.53871\n",
      "Epoch: 00 [16004/26073 ( 61%)], Train Loss: 0.53818\n",
      "Epoch: 00 [16044/26073 ( 62%)], Train Loss: 0.53801\n",
      "Epoch: 00 [16084/26073 ( 62%)], Train Loss: 0.53752\n",
      "Epoch: 00 [16124/26073 ( 62%)], Train Loss: 0.53718\n",
      "Epoch: 00 [16164/26073 ( 62%)], Train Loss: 0.53704\n",
      "Epoch: 00 [16204/26073 ( 62%)], Train Loss: 0.53707\n",
      "Epoch: 00 [16244/26073 ( 62%)], Train Loss: 0.53669\n",
      "Epoch: 00 [16284/26073 ( 62%)], Train Loss: 0.53626\n",
      "Epoch: 00 [16324/26073 ( 63%)], Train Loss: 0.53620\n",
      "Epoch: 00 [16364/26073 ( 63%)], Train Loss: 0.53601\n",
      "Epoch: 00 [16404/26073 ( 63%)], Train Loss: 0.53559\n",
      "Epoch: 00 [16444/26073 ( 63%)], Train Loss: 0.53511\n",
      "Epoch: 00 [16484/26073 ( 63%)], Train Loss: 0.53507\n",
      "Epoch: 00 [16524/26073 ( 63%)], Train Loss: 0.53534\n",
      "Epoch: 00 [16564/26073 ( 64%)], Train Loss: 0.53501\n",
      "Epoch: 00 [16604/26073 ( 64%)], Train Loss: 0.53471\n",
      "Epoch: 00 [16644/26073 ( 64%)], Train Loss: 0.53438\n",
      "Epoch: 00 [16684/26073 ( 64%)], Train Loss: 0.53402\n",
      "Epoch: 00 [16724/26073 ( 64%)], Train Loss: 0.53357\n",
      "Epoch: 00 [16764/26073 ( 64%)], Train Loss: 0.53282\n",
      "Epoch: 00 [16804/26073 ( 64%)], Train Loss: 0.53281\n",
      "Epoch: 00 [16844/26073 ( 65%)], Train Loss: 0.53225\n",
      "Epoch: 00 [16884/26073 ( 65%)], Train Loss: 0.53145\n",
      "Epoch: 00 [16924/26073 ( 65%)], Train Loss: 0.53094\n",
      "Epoch: 00 [16964/26073 ( 65%)], Train Loss: 0.53060\n",
      "Epoch: 00 [17004/26073 ( 65%)], Train Loss: 0.52986\n",
      "Epoch: 00 [17044/26073 ( 65%)], Train Loss: 0.52912\n",
      "Epoch: 00 [17084/26073 ( 66%)], Train Loss: 0.52884\n",
      "Epoch: 00 [17124/26073 ( 66%)], Train Loss: 0.52839\n",
      "Epoch: 00 [17164/26073 ( 66%)], Train Loss: 0.52838\n",
      "Epoch: 00 [17204/26073 ( 66%)], Train Loss: 0.52795\n",
      "Epoch: 00 [17244/26073 ( 66%)], Train Loss: 0.52753\n",
      "Epoch: 00 [17284/26073 ( 66%)], Train Loss: 0.52692\n",
      "Epoch: 00 [17324/26073 ( 66%)], Train Loss: 0.52681\n",
      "Epoch: 00 [17364/26073 ( 67%)], Train Loss: 0.52631\n",
      "Epoch: 00 [17404/26073 ( 67%)], Train Loss: 0.52595\n",
      "Epoch: 00 [17444/26073 ( 67%)], Train Loss: 0.52577\n",
      "Epoch: 00 [17484/26073 ( 67%)], Train Loss: 0.52554\n",
      "Epoch: 00 [17524/26073 ( 67%)], Train Loss: 0.52531\n",
      "Epoch: 00 [17564/26073 ( 67%)], Train Loss: 0.52505\n",
      "Epoch: 00 [17604/26073 ( 68%)], Train Loss: 0.52434\n",
      "Epoch: 00 [17644/26073 ( 68%)], Train Loss: 0.52396\n",
      "Epoch: 00 [17684/26073 ( 68%)], Train Loss: 0.52357\n",
      "Epoch: 00 [17724/26073 ( 68%)], Train Loss: 0.52365\n",
      "Epoch: 00 [17764/26073 ( 68%)], Train Loss: 0.52340\n",
      "Epoch: 00 [17804/26073 ( 68%)], Train Loss: 0.52337\n",
      "Epoch: 00 [17844/26073 ( 68%)], Train Loss: 0.52288\n",
      "Epoch: 00 [17884/26073 ( 69%)], Train Loss: 0.52235\n",
      "Epoch: 00 [17924/26073 ( 69%)], Train Loss: 0.52204\n",
      "Epoch: 00 [17964/26073 ( 69%)], Train Loss: 0.52171\n",
      "Epoch: 00 [18004/26073 ( 69%)], Train Loss: 0.52156\n",
      "Epoch: 00 [18044/26073 ( 69%)], Train Loss: 0.52122\n",
      "Epoch: 00 [18084/26073 ( 69%)], Train Loss: 0.52078\n",
      "Epoch: 00 [18124/26073 ( 70%)], Train Loss: 0.52037\n",
      "Epoch: 00 [18164/26073 ( 70%)], Train Loss: 0.51966\n",
      "Epoch: 00 [18204/26073 ( 70%)], Train Loss: 0.51929\n",
      "Epoch: 00 [18244/26073 ( 70%)], Train Loss: 0.51882\n",
      "Epoch: 00 [18284/26073 ( 70%)], Train Loss: 0.51849\n",
      "Epoch: 00 [18324/26073 ( 70%)], Train Loss: 0.51812\n",
      "Epoch: 00 [18364/26073 ( 70%)], Train Loss: 0.51764\n",
      "Epoch: 00 [18404/26073 ( 71%)], Train Loss: 0.51742\n",
      "Epoch: 00 [18444/26073 ( 71%)], Train Loss: 0.51705\n",
      "Epoch: 00 [18484/26073 ( 71%)], Train Loss: 0.51712\n",
      "Epoch: 00 [18524/26073 ( 71%)], Train Loss: 0.51657\n",
      "Epoch: 00 [18564/26073 ( 71%)], Train Loss: 0.51651\n",
      "Epoch: 00 [18604/26073 ( 71%)], Train Loss: 0.51630\n",
      "Epoch: 00 [18644/26073 ( 72%)], Train Loss: 0.51621\n",
      "Epoch: 00 [18684/26073 ( 72%)], Train Loss: 0.51584\n",
      "Epoch: 00 [18724/26073 ( 72%)], Train Loss: 0.51560\n",
      "Epoch: 00 [18764/26073 ( 72%)], Train Loss: 0.51538\n",
      "Epoch: 00 [18804/26073 ( 72%)], Train Loss: 0.51495\n",
      "Epoch: 00 [18844/26073 ( 72%)], Train Loss: 0.51463\n",
      "Epoch: 00 [18884/26073 ( 72%)], Train Loss: 0.51441\n",
      "Epoch: 00 [18924/26073 ( 73%)], Train Loss: 0.51382\n",
      "Epoch: 00 [18964/26073 ( 73%)], Train Loss: 0.51347\n",
      "Epoch: 00 [19004/26073 ( 73%)], Train Loss: 0.51317\n",
      "Epoch: 00 [19044/26073 ( 73%)], Train Loss: 0.51318\n",
      "Epoch: 00 [19084/26073 ( 73%)], Train Loss: 0.51281\n",
      "Epoch: 00 [19124/26073 ( 73%)], Train Loss: 0.51237\n",
      "Epoch: 00 [19164/26073 ( 74%)], Train Loss: 0.51171\n",
      "Epoch: 00 [19204/26073 ( 74%)], Train Loss: 0.51151\n",
      "Epoch: 00 [19244/26073 ( 74%)], Train Loss: 0.51125\n",
      "Epoch: 00 [19284/26073 ( 74%)], Train Loss: 0.51130\n",
      "Epoch: 00 [19324/26073 ( 74%)], Train Loss: 0.51146\n",
      "Epoch: 00 [19364/26073 ( 74%)], Train Loss: 0.51101\n",
      "Epoch: 00 [19404/26073 ( 74%)], Train Loss: 0.51104\n",
      "Epoch: 00 [19444/26073 ( 75%)], Train Loss: 0.51079\n",
      "Epoch: 00 [19484/26073 ( 75%)], Train Loss: 0.51094\n",
      "Epoch: 00 [19524/26073 ( 75%)], Train Loss: 0.51057\n",
      "Epoch: 00 [19564/26073 ( 75%)], Train Loss: 0.51045\n",
      "Epoch: 00 [19604/26073 ( 75%)], Train Loss: 0.51019\n",
      "Epoch: 00 [19644/26073 ( 75%)], Train Loss: 0.50973\n",
      "Epoch: 00 [19684/26073 ( 75%)], Train Loss: 0.50929\n",
      "Epoch: 00 [19724/26073 ( 76%)], Train Loss: 0.50874\n",
      "Epoch: 00 [19764/26073 ( 76%)], Train Loss: 0.50857\n",
      "Epoch: 00 [19804/26073 ( 76%)], Train Loss: 0.50824\n",
      "Epoch: 00 [19844/26073 ( 76%)], Train Loss: 0.50789\n",
      "Epoch: 00 [19884/26073 ( 76%)], Train Loss: 0.50739\n",
      "Epoch: 00 [19924/26073 ( 76%)], Train Loss: 0.50706\n",
      "Epoch: 00 [19964/26073 ( 77%)], Train Loss: 0.50681\n",
      "Epoch: 00 [20004/26073 ( 77%)], Train Loss: 0.50666\n",
      "Epoch: 00 [20044/26073 ( 77%)], Train Loss: 0.50632\n",
      "Epoch: 00 [20084/26073 ( 77%)], Train Loss: 0.50650\n",
      "Epoch: 00 [20124/26073 ( 77%)], Train Loss: 0.50630\n",
      "Epoch: 00 [20164/26073 ( 77%)], Train Loss: 0.50599\n",
      "Epoch: 00 [20204/26073 ( 77%)], Train Loss: 0.50560\n",
      "Epoch: 00 [20244/26073 ( 78%)], Train Loss: 0.50523\n",
      "Epoch: 00 [20284/26073 ( 78%)], Train Loss: 0.50476\n",
      "Epoch: 00 [20324/26073 ( 78%)], Train Loss: 0.50443\n",
      "Epoch: 00 [20364/26073 ( 78%)], Train Loss: 0.50373\n",
      "Epoch: 00 [20404/26073 ( 78%)], Train Loss: 0.50345\n",
      "Epoch: 00 [20444/26073 ( 78%)], Train Loss: 0.50365\n",
      "Epoch: 00 [20484/26073 ( 79%)], Train Loss: 0.50324\n",
      "Epoch: 00 [20524/26073 ( 79%)], Train Loss: 0.50324\n",
      "Epoch: 00 [20564/26073 ( 79%)], Train Loss: 0.50287\n",
      "Epoch: 00 [20604/26073 ( 79%)], Train Loss: 0.50257\n",
      "Epoch: 00 [20644/26073 ( 79%)], Train Loss: 0.50220\n",
      "Epoch: 00 [20684/26073 ( 79%)], Train Loss: 0.50203\n",
      "Epoch: 00 [20724/26073 ( 79%)], Train Loss: 0.50175\n",
      "Epoch: 00 [20764/26073 ( 80%)], Train Loss: 0.50150\n",
      "Epoch: 00 [20804/26073 ( 80%)], Train Loss: 0.50098\n",
      "Epoch: 00 [20844/26073 ( 80%)], Train Loss: 0.50045\n",
      "Epoch: 00 [20884/26073 ( 80%)], Train Loss: 0.50024\n",
      "Epoch: 00 [20924/26073 ( 80%)], Train Loss: 0.49969\n",
      "Epoch: 00 [20964/26073 ( 80%)], Train Loss: 0.49938\n",
      "Epoch: 00 [21004/26073 ( 81%)], Train Loss: 0.49900\n",
      "Epoch: 00 [21044/26073 ( 81%)], Train Loss: 0.49885\n",
      "Epoch: 00 [21084/26073 ( 81%)], Train Loss: 0.49854\n",
      "Epoch: 00 [21124/26073 ( 81%)], Train Loss: 0.49845\n",
      "Epoch: 00 [21164/26073 ( 81%)], Train Loss: 0.49811\n",
      "Epoch: 00 [21204/26073 ( 81%)], Train Loss: 0.49811\n",
      "Epoch: 00 [21244/26073 ( 81%)], Train Loss: 0.49781\n",
      "Epoch: 00 [21284/26073 ( 82%)], Train Loss: 0.49750\n",
      "Epoch: 00 [21324/26073 ( 82%)], Train Loss: 0.49744\n",
      "Epoch: 00 [21364/26073 ( 82%)], Train Loss: 0.49693\n",
      "Epoch: 00 [21404/26073 ( 82%)], Train Loss: 0.49683\n",
      "Epoch: 00 [21444/26073 ( 82%)], Train Loss: 0.49651\n",
      "Epoch: 00 [21484/26073 ( 82%)], Train Loss: 0.49631\n",
      "Epoch: 00 [21524/26073 ( 83%)], Train Loss: 0.49598\n",
      "Epoch: 00 [21564/26073 ( 83%)], Train Loss: 0.49562\n",
      "Epoch: 00 [21604/26073 ( 83%)], Train Loss: 0.49545\n",
      "Epoch: 00 [21644/26073 ( 83%)], Train Loss: 0.49525\n",
      "Epoch: 00 [21684/26073 ( 83%)], Train Loss: 0.49485\n",
      "Epoch: 00 [21724/26073 ( 83%)], Train Loss: 0.49439\n",
      "Epoch: 00 [21764/26073 ( 83%)], Train Loss: 0.49427\n",
      "Epoch: 00 [21804/26073 ( 84%)], Train Loss: 0.49401\n",
      "Epoch: 00 [21844/26073 ( 84%)], Train Loss: 0.49373\n",
      "Epoch: 00 [21884/26073 ( 84%)], Train Loss: 0.49392\n",
      "Epoch: 00 [21924/26073 ( 84%)], Train Loss: 0.49371\n",
      "Epoch: 00 [21964/26073 ( 84%)], Train Loss: 0.49349\n",
      "Epoch: 00 [22004/26073 ( 84%)], Train Loss: 0.49312\n",
      "Epoch: 00 [22044/26073 ( 85%)], Train Loss: 0.49295\n",
      "Epoch: 00 [22084/26073 ( 85%)], Train Loss: 0.49242\n",
      "Epoch: 00 [22124/26073 ( 85%)], Train Loss: 0.49208\n",
      "Epoch: 00 [22164/26073 ( 85%)], Train Loss: 0.49167\n",
      "Epoch: 00 [22204/26073 ( 85%)], Train Loss: 0.49135\n",
      "Epoch: 00 [22244/26073 ( 85%)], Train Loss: 0.49104\n",
      "Epoch: 00 [22284/26073 ( 85%)], Train Loss: 0.49107\n",
      "Epoch: 00 [22324/26073 ( 86%)], Train Loss: 0.49069\n",
      "Epoch: 00 [22364/26073 ( 86%)], Train Loss: 0.49084\n",
      "Epoch: 00 [22404/26073 ( 86%)], Train Loss: 0.49044\n",
      "Epoch: 00 [22444/26073 ( 86%)], Train Loss: 0.49023\n",
      "Epoch: 00 [22484/26073 ( 86%)], Train Loss: 0.49003\n",
      "Epoch: 00 [22524/26073 ( 86%)], Train Loss: 0.48959\n",
      "Epoch: 00 [22564/26073 ( 87%)], Train Loss: 0.48973\n",
      "Epoch: 00 [22604/26073 ( 87%)], Train Loss: 0.48983\n",
      "Epoch: 00 [22644/26073 ( 87%)], Train Loss: 0.48970\n",
      "Epoch: 00 [22684/26073 ( 87%)], Train Loss: 0.48952\n",
      "Epoch: 00 [22724/26073 ( 87%)], Train Loss: 0.48961\n",
      "Epoch: 00 [22764/26073 ( 87%)], Train Loss: 0.48945\n",
      "Epoch: 00 [22804/26073 ( 87%)], Train Loss: 0.48920\n",
      "Epoch: 00 [22844/26073 ( 88%)], Train Loss: 0.48873\n",
      "Epoch: 00 [22884/26073 ( 88%)], Train Loss: 0.48844\n",
      "Epoch: 00 [22924/26073 ( 88%)], Train Loss: 0.48820\n",
      "Epoch: 00 [22964/26073 ( 88%)], Train Loss: 0.48798\n",
      "Epoch: 00 [23004/26073 ( 88%)], Train Loss: 0.48760\n",
      "Epoch: 00 [23044/26073 ( 88%)], Train Loss: 0.48723\n",
      "Epoch: 00 [23084/26073 ( 89%)], Train Loss: 0.48701\n",
      "Epoch: 00 [23124/26073 ( 89%)], Train Loss: 0.48685\n",
      "Epoch: 00 [23164/26073 ( 89%)], Train Loss: 0.48660\n",
      "Epoch: 00 [23204/26073 ( 89%)], Train Loss: 0.48624\n",
      "Epoch: 00 [23244/26073 ( 89%)], Train Loss: 0.48605\n",
      "Epoch: 00 [23284/26073 ( 89%)], Train Loss: 0.48579\n",
      "Epoch: 00 [23324/26073 ( 89%)], Train Loss: 0.48554\n",
      "Epoch: 00 [23364/26073 ( 90%)], Train Loss: 0.48516\n",
      "Epoch: 00 [23404/26073 ( 90%)], Train Loss: 0.48492\n",
      "Epoch: 00 [23444/26073 ( 90%)], Train Loss: 0.48450\n",
      "Epoch: 00 [23484/26073 ( 90%)], Train Loss: 0.48439\n",
      "Epoch: 00 [23524/26073 ( 90%)], Train Loss: 0.48418\n",
      "Epoch: 00 [23564/26073 ( 90%)], Train Loss: 0.48393\n",
      "Epoch: 00 [23604/26073 ( 91%)], Train Loss: 0.48409\n",
      "Epoch: 00 [23644/26073 ( 91%)], Train Loss: 0.48379\n",
      "Epoch: 00 [23684/26073 ( 91%)], Train Loss: 0.48335\n",
      "Epoch: 00 [23724/26073 ( 91%)], Train Loss: 0.48299\n",
      "Epoch: 00 [23764/26073 ( 91%)], Train Loss: 0.48288\n",
      "Epoch: 00 [23804/26073 ( 91%)], Train Loss: 0.48258\n",
      "Epoch: 00 [23844/26073 ( 91%)], Train Loss: 0.48235\n",
      "Epoch: 00 [23884/26073 ( 92%)], Train Loss: 0.48206\n",
      "Epoch: 00 [23924/26073 ( 92%)], Train Loss: 0.48176\n",
      "Epoch: 00 [23964/26073 ( 92%)], Train Loss: 0.48137\n",
      "Epoch: 00 [24004/26073 ( 92%)], Train Loss: 0.48099\n",
      "Epoch: 00 [24044/26073 ( 92%)], Train Loss: 0.48083\n",
      "Epoch: 00 [24084/26073 ( 92%)], Train Loss: 0.48049\n",
      "Epoch: 00 [24124/26073 ( 93%)], Train Loss: 0.48012\n",
      "Epoch: 00 [24164/26073 ( 93%)], Train Loss: 0.47992\n",
      "Epoch: 00 [24204/26073 ( 93%)], Train Loss: 0.47967\n",
      "Epoch: 00 [24244/26073 ( 93%)], Train Loss: 0.47964\n",
      "Epoch: 00 [24284/26073 ( 93%)], Train Loss: 0.47950\n",
      "Epoch: 00 [24324/26073 ( 93%)], Train Loss: 0.47933\n",
      "Epoch: 00 [24364/26073 ( 93%)], Train Loss: 0.47914\n",
      "Epoch: 00 [24404/26073 ( 94%)], Train Loss: 0.47903\n",
      "Epoch: 00 [24444/26073 ( 94%)], Train Loss: 0.47870\n",
      "Epoch: 00 [24484/26073 ( 94%)], Train Loss: 0.47888\n",
      "Epoch: 00 [24524/26073 ( 94%)], Train Loss: 0.47908\n",
      "Epoch: 00 [24564/26073 ( 94%)], Train Loss: 0.47883\n",
      "Epoch: 00 [24604/26073 ( 94%)], Train Loss: 0.47855\n",
      "Epoch: 00 [24644/26073 ( 95%)], Train Loss: 0.47849\n",
      "Epoch: 00 [24684/26073 ( 95%)], Train Loss: 0.47824\n",
      "Epoch: 00 [24724/26073 ( 95%)], Train Loss: 0.47809\n",
      "Epoch: 00 [24764/26073 ( 95%)], Train Loss: 0.47771\n",
      "Epoch: 00 [24804/26073 ( 95%)], Train Loss: 0.47740\n",
      "Epoch: 00 [24844/26073 ( 95%)], Train Loss: 0.47736\n",
      "Epoch: 00 [24884/26073 ( 95%)], Train Loss: 0.47743\n",
      "Epoch: 00 [24924/26073 ( 96%)], Train Loss: 0.47769\n",
      "Epoch: 00 [24964/26073 ( 96%)], Train Loss: 0.47761\n",
      "Epoch: 00 [25004/26073 ( 96%)], Train Loss: 0.47744\n",
      "Epoch: 00 [25044/26073 ( 96%)], Train Loss: 0.47743\n",
      "Epoch: 00 [25084/26073 ( 96%)], Train Loss: 0.47742\n",
      "Epoch: 00 [25124/26073 ( 96%)], Train Loss: 0.47710\n",
      "Epoch: 00 [25164/26073 ( 97%)], Train Loss: 0.47695\n",
      "Epoch: 00 [25204/26073 ( 97%)], Train Loss: 0.47691\n",
      "Epoch: 00 [25244/26073 ( 97%)], Train Loss: 0.47643\n",
      "Epoch: 00 [25284/26073 ( 97%)], Train Loss: 0.47617\n",
      "Epoch: 00 [25324/26073 ( 97%)], Train Loss: 0.47613\n",
      "Epoch: 00 [25364/26073 ( 97%)], Train Loss: 0.47623\n",
      "Epoch: 00 [25404/26073 ( 97%)], Train Loss: 0.47606\n",
      "Epoch: 00 [25444/26073 ( 98%)], Train Loss: 0.47609\n",
      "Epoch: 00 [25484/26073 ( 98%)], Train Loss: 0.47577\n",
      "Epoch: 00 [25524/26073 ( 98%)], Train Loss: 0.47532\n",
      "Epoch: 00 [25564/26073 ( 98%)], Train Loss: 0.47538\n",
      "Epoch: 00 [25604/26073 ( 98%)], Train Loss: 0.47526\n",
      "Epoch: 00 [25644/26073 ( 98%)], Train Loss: 0.47502\n",
      "Epoch: 00 [25684/26073 ( 99%)], Train Loss: 0.47497\n",
      "Epoch: 00 [25724/26073 ( 99%)], Train Loss: 0.47462\n",
      "Epoch: 00 [25764/26073 ( 99%)], Train Loss: 0.47471\n",
      "Epoch: 00 [25804/26073 ( 99%)], Train Loss: 0.47438\n",
      "Epoch: 00 [25844/26073 ( 99%)], Train Loss: 0.47420\n",
      "Epoch: 00 [25884/26073 ( 99%)], Train Loss: 0.47384\n",
      "Epoch: 00 [25924/26073 ( 99%)], Train Loss: 0.47360\n",
      "Epoch: 00 [25964/26073 (100%)], Train Loss: 0.47357\n",
      "Epoch: 00 [26004/26073 (100%)], Train Loss: 0.47327\n",
      "Epoch: 00 [26044/26073 (100%)], Train Loss: 0.47312\n",
      "Epoch: 00 [26073/26073 (100%)], Train Loss: 0.47311\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] Valid Loss: 0.27729\n",
      "Post-processing 223 example predictions split into 2427 features.\n",
      "valid jaccard:  0.6752473485657342\n",
      "0 Epoch, Best epoch was updated! Valid Loss: 0.27729\n",
      "Saving model checkpoint to output/checkpoint-fold-0-epoch-0.\n",
      "\n",
      "Total Training Time: 3833.1595861911774secs, Average Training Time per Epoch: 3833.1595861911774secs.\n",
      "Total Validation Time: 115.24001145362854secs, Average Validation Time per Epoch: 115.24001145362854secs.\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "FOLD: 1\n",
      "--------------------------------------------------\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Num examples Train= 25439, Num examples Valid=3061\n",
      "Total Training Steps: 3180, Total Warmup Steps: 318\n",
      "Epoch: 00 [    4/25439 (  0%)], Train Loss: 3.15411\n",
      "Epoch: 00 [   44/25439 (  0%)], Train Loss: 3.20302\n",
      "Epoch: 00 [   84/25439 (  0%)], Train Loss: 3.19916\n",
      "Epoch: 00 [  124/25439 (  0%)], Train Loss: 3.18911\n",
      "Epoch: 00 [  164/25439 (  1%)], Train Loss: 3.16002\n",
      "Epoch: 00 [  204/25439 (  1%)], Train Loss: 3.10645\n",
      "Epoch: 00 [  244/25439 (  1%)], Train Loss: 3.05624\n",
      "Epoch: 00 [  284/25439 (  1%)], Train Loss: 2.98766\n",
      "Epoch: 00 [  324/25439 (  1%)], Train Loss: 2.90448\n",
      "Epoch: 00 [  364/25439 (  1%)], Train Loss: 2.80073\n",
      "Epoch: 00 [  404/25439 (  2%)], Train Loss: 2.67665\n",
      "Epoch: 00 [  444/25439 (  2%)], Train Loss: 2.53886\n",
      "Epoch: 00 [  484/25439 (  2%)], Train Loss: 2.42816\n",
      "Epoch: 00 [  524/25439 (  2%)], Train Loss: 2.30220\n",
      "Epoch: 00 [  564/25439 (  2%)], Train Loss: 2.19973\n",
      "Epoch: 00 [  604/25439 (  2%)], Train Loss: 2.12679\n",
      "Epoch: 00 [  644/25439 (  3%)], Train Loss: 2.04092\n",
      "Epoch: 00 [  684/25439 (  3%)], Train Loss: 1.95386\n",
      "Epoch: 00 [  724/25439 (  3%)], Train Loss: 1.86787\n",
      "Epoch: 00 [  764/25439 (  3%)], Train Loss: 1.80201\n",
      "Epoch: 00 [  804/25439 (  3%)], Train Loss: 1.73988\n",
      "Epoch: 00 [  844/25439 (  3%)], Train Loss: 1.67932\n",
      "Epoch: 00 [  884/25439 (  3%)], Train Loss: 1.62474\n",
      "Epoch: 00 [  924/25439 (  4%)], Train Loss: 1.57775\n",
      "Epoch: 00 [  964/25439 (  4%)], Train Loss: 1.54731\n",
      "Epoch: 00 [ 1004/25439 (  4%)], Train Loss: 1.50794\n",
      "Epoch: 00 [ 1044/25439 (  4%)], Train Loss: 1.46992\n",
      "Epoch: 00 [ 1084/25439 (  4%)], Train Loss: 1.43117\n",
      "Epoch: 00 [ 1124/25439 (  4%)], Train Loss: 1.40174\n",
      "Epoch: 00 [ 1164/25439 (  5%)], Train Loss: 1.37565\n",
      "Epoch: 00 [ 1204/25439 (  5%)], Train Loss: 1.34591\n",
      "Epoch: 00 [ 1244/25439 (  5%)], Train Loss: 1.32300\n",
      "Epoch: 00 [ 1284/25439 (  5%)], Train Loss: 1.29869\n",
      "Epoch: 00 [ 1324/25439 (  5%)], Train Loss: 1.27193\n",
      "Epoch: 00 [ 1364/25439 (  5%)], Train Loss: 1.25091\n",
      "Epoch: 00 [ 1404/25439 (  6%)], Train Loss: 1.23114\n",
      "Epoch: 00 [ 1444/25439 (  6%)], Train Loss: 1.20540\n",
      "Epoch: 00 [ 1484/25439 (  6%)], Train Loss: 1.18753\n",
      "Epoch: 00 [ 1524/25439 (  6%)], Train Loss: 1.16763\n",
      "Epoch: 00 [ 1564/25439 (  6%)], Train Loss: 1.15308\n",
      "Epoch: 00 [ 1604/25439 (  6%)], Train Loss: 1.13884\n",
      "Epoch: 00 [ 1644/25439 (  6%)], Train Loss: 1.12708\n",
      "Epoch: 00 [ 1684/25439 (  7%)], Train Loss: 1.11807\n",
      "Epoch: 00 [ 1724/25439 (  7%)], Train Loss: 1.10589\n",
      "Epoch: 00 [ 1764/25439 (  7%)], Train Loss: 1.09684\n",
      "Epoch: 00 [ 1804/25439 (  7%)], Train Loss: 1.08424\n",
      "Epoch: 00 [ 1844/25439 (  7%)], Train Loss: 1.07184\n",
      "Epoch: 00 [ 1884/25439 (  7%)], Train Loss: 1.05833\n",
      "Epoch: 00 [ 1924/25439 (  8%)], Train Loss: 1.05004\n",
      "Epoch: 00 [ 1964/25439 (  8%)], Train Loss: 1.03948\n",
      "Epoch: 00 [ 2004/25439 (  8%)], Train Loss: 1.03153\n",
      "Epoch: 00 [ 2044/25439 (  8%)], Train Loss: 1.01993\n",
      "Epoch: 00 [ 2084/25439 (  8%)], Train Loss: 1.01133\n",
      "Epoch: 00 [ 2124/25439 (  8%)], Train Loss: 1.00247\n",
      "Epoch: 00 [ 2164/25439 (  9%)], Train Loss: 0.99538\n",
      "Epoch: 00 [ 2204/25439 (  9%)], Train Loss: 0.98494\n",
      "Epoch: 00 [ 2244/25439 (  9%)], Train Loss: 0.97402\n",
      "Epoch: 00 [ 2284/25439 (  9%)], Train Loss: 0.96749\n",
      "Epoch: 00 [ 2324/25439 (  9%)], Train Loss: 0.95629\n",
      "Epoch: 00 [ 2364/25439 (  9%)], Train Loss: 0.94887\n",
      "Epoch: 00 [ 2404/25439 (  9%)], Train Loss: 0.94281\n",
      "Epoch: 00 [ 2444/25439 ( 10%)], Train Loss: 0.93817\n",
      "Epoch: 00 [ 2484/25439 ( 10%)], Train Loss: 0.92987\n",
      "Epoch: 00 [ 2524/25439 ( 10%)], Train Loss: 0.92360\n",
      "Epoch: 00 [ 2564/25439 ( 10%)], Train Loss: 0.91858\n",
      "Epoch: 00 [ 2604/25439 ( 10%)], Train Loss: 0.91324\n",
      "Epoch: 00 [ 2644/25439 ( 10%)], Train Loss: 0.90741\n",
      "Epoch: 00 [ 2684/25439 ( 11%)], Train Loss: 0.90294\n",
      "Epoch: 00 [ 2724/25439 ( 11%)], Train Loss: 0.90086\n",
      "Epoch: 00 [ 2764/25439 ( 11%)], Train Loss: 0.89945\n",
      "Epoch: 00 [ 2804/25439 ( 11%)], Train Loss: 0.89644\n",
      "Epoch: 00 [ 2844/25439 ( 11%)], Train Loss: 0.89047\n",
      "Epoch: 00 [ 2884/25439 ( 11%)], Train Loss: 0.88620\n",
      "Epoch: 00 [ 2924/25439 ( 11%)], Train Loss: 0.88195\n",
      "Epoch: 00 [ 2964/25439 ( 12%)], Train Loss: 0.87860\n",
      "Epoch: 00 [ 3004/25439 ( 12%)], Train Loss: 0.87669\n",
      "Epoch: 00 [ 3044/25439 ( 12%)], Train Loss: 0.87170\n",
      "Epoch: 00 [ 3084/25439 ( 12%)], Train Loss: 0.86484\n",
      "Epoch: 00 [ 3124/25439 ( 12%)], Train Loss: 0.85817\n",
      "Epoch: 00 [ 3164/25439 ( 12%)], Train Loss: 0.85194\n",
      "Epoch: 00 [ 3204/25439 ( 13%)], Train Loss: 0.85049\n",
      "Epoch: 00 [ 3244/25439 ( 13%)], Train Loss: 0.84697\n",
      "Epoch: 00 [ 3284/25439 ( 13%)], Train Loss: 0.84365\n",
      "Epoch: 00 [ 3324/25439 ( 13%)], Train Loss: 0.84022\n",
      "Epoch: 00 [ 3364/25439 ( 13%)], Train Loss: 0.83816\n",
      "Epoch: 00 [ 3404/25439 ( 13%)], Train Loss: 0.83460\n",
      "Epoch: 00 [ 3444/25439 ( 14%)], Train Loss: 0.83054\n",
      "Epoch: 00 [ 3484/25439 ( 14%)], Train Loss: 0.82893\n",
      "Epoch: 00 [ 3524/25439 ( 14%)], Train Loss: 0.82446\n",
      "Epoch: 00 [ 3564/25439 ( 14%)], Train Loss: 0.82097\n",
      "Epoch: 00 [ 3604/25439 ( 14%)], Train Loss: 0.81596\n",
      "Epoch: 00 [ 3644/25439 ( 14%)], Train Loss: 0.81511\n",
      "Epoch: 00 [ 3684/25439 ( 14%)], Train Loss: 0.81102\n",
      "Epoch: 00 [ 3724/25439 ( 15%)], Train Loss: 0.80638\n",
      "Epoch: 00 [ 3764/25439 ( 15%)], Train Loss: 0.80253\n",
      "Epoch: 00 [ 3804/25439 ( 15%)], Train Loss: 0.79870\n",
      "Epoch: 00 [ 3844/25439 ( 15%)], Train Loss: 0.79470\n",
      "Epoch: 00 [ 3884/25439 ( 15%)], Train Loss: 0.78962\n",
      "Epoch: 00 [ 3924/25439 ( 15%)], Train Loss: 0.78569\n",
      "Epoch: 00 [ 3964/25439 ( 16%)], Train Loss: 0.78336\n",
      "Epoch: 00 [ 4004/25439 ( 16%)], Train Loss: 0.78179\n",
      "Epoch: 00 [ 4044/25439 ( 16%)], Train Loss: 0.77834\n",
      "Epoch: 00 [ 4084/25439 ( 16%)], Train Loss: 0.77564\n",
      "Epoch: 00 [ 4124/25439 ( 16%)], Train Loss: 0.77160\n",
      "Epoch: 00 [ 4164/25439 ( 16%)], Train Loss: 0.77013\n",
      "Epoch: 00 [ 4204/25439 ( 17%)], Train Loss: 0.76697\n",
      "Epoch: 00 [ 4244/25439 ( 17%)], Train Loss: 0.76460\n",
      "Epoch: 00 [ 4284/25439 ( 17%)], Train Loss: 0.76228\n",
      "Epoch: 00 [ 4324/25439 ( 17%)], Train Loss: 0.75997\n",
      "Epoch: 00 [ 4364/25439 ( 17%)], Train Loss: 0.75583\n",
      "Epoch: 00 [ 4404/25439 ( 17%)], Train Loss: 0.75235\n",
      "Epoch: 00 [ 4444/25439 ( 17%)], Train Loss: 0.75157\n",
      "Epoch: 00 [ 4484/25439 ( 18%)], Train Loss: 0.74856\n",
      "Epoch: 00 [ 4524/25439 ( 18%)], Train Loss: 0.74847\n",
      "Epoch: 00 [ 4564/25439 ( 18%)], Train Loss: 0.74790\n",
      "Epoch: 00 [ 4604/25439 ( 18%)], Train Loss: 0.74584\n",
      "Epoch: 00 [ 4644/25439 ( 18%)], Train Loss: 0.74386\n",
      "Epoch: 00 [ 4684/25439 ( 18%)], Train Loss: 0.74208\n",
      "Epoch: 00 [ 4724/25439 ( 19%)], Train Loss: 0.73839\n",
      "Epoch: 00 [ 4764/25439 ( 19%)], Train Loss: 0.73578\n",
      "Epoch: 00 [ 4804/25439 ( 19%)], Train Loss: 0.73446\n",
      "Epoch: 00 [ 4844/25439 ( 19%)], Train Loss: 0.73330\n",
      "Epoch: 00 [ 4884/25439 ( 19%)], Train Loss: 0.73094\n",
      "Epoch: 00 [ 4924/25439 ( 19%)], Train Loss: 0.72947\n",
      "Epoch: 00 [ 4964/25439 ( 20%)], Train Loss: 0.72699\n",
      "Epoch: 00 [ 5004/25439 ( 20%)], Train Loss: 0.72517\n",
      "Epoch: 00 [ 5044/25439 ( 20%)], Train Loss: 0.72416\n",
      "Epoch: 00 [ 5084/25439 ( 20%)], Train Loss: 0.72270\n",
      "Epoch: 00 [ 5124/25439 ( 20%)], Train Loss: 0.72323\n",
      "Epoch: 00 [ 5164/25439 ( 20%)], Train Loss: 0.72123\n",
      "Epoch: 00 [ 5204/25439 ( 20%)], Train Loss: 0.71947\n",
      "Epoch: 00 [ 5244/25439 ( 21%)], Train Loss: 0.71757\n",
      "Epoch: 00 [ 5284/25439 ( 21%)], Train Loss: 0.71617\n",
      "Epoch: 00 [ 5324/25439 ( 21%)], Train Loss: 0.71408\n",
      "Epoch: 00 [ 5364/25439 ( 21%)], Train Loss: 0.71412\n",
      "Epoch: 00 [ 5404/25439 ( 21%)], Train Loss: 0.71344\n",
      "Epoch: 00 [ 5444/25439 ( 21%)], Train Loss: 0.71089\n",
      "Epoch: 00 [ 5484/25439 ( 22%)], Train Loss: 0.71155\n",
      "Epoch: 00 [ 5524/25439 ( 22%)], Train Loss: 0.70973\n",
      "Epoch: 00 [ 5564/25439 ( 22%)], Train Loss: 0.70868\n",
      "Epoch: 00 [ 5604/25439 ( 22%)], Train Loss: 0.70757\n",
      "Epoch: 00 [ 5644/25439 ( 22%)], Train Loss: 0.70644\n",
      "Epoch: 00 [ 5684/25439 ( 22%)], Train Loss: 0.70480\n",
      "Epoch: 00 [ 5724/25439 ( 23%)], Train Loss: 0.70354\n",
      "Epoch: 00 [ 5764/25439 ( 23%)], Train Loss: 0.70404\n",
      "Epoch: 00 [ 5804/25439 ( 23%)], Train Loss: 0.70204\n",
      "Epoch: 00 [ 5844/25439 ( 23%)], Train Loss: 0.70023\n",
      "Epoch: 00 [ 5884/25439 ( 23%)], Train Loss: 0.69987\n",
      "Epoch: 00 [ 5924/25439 ( 23%)], Train Loss: 0.69926\n",
      "Epoch: 00 [ 5964/25439 ( 23%)], Train Loss: 0.69742\n",
      "Epoch: 00 [ 6004/25439 ( 24%)], Train Loss: 0.69529\n",
      "Epoch: 00 [ 6044/25439 ( 24%)], Train Loss: 0.69397\n",
      "Epoch: 00 [ 6084/25439 ( 24%)], Train Loss: 0.69205\n",
      "Epoch: 00 [ 6124/25439 ( 24%)], Train Loss: 0.69093\n",
      "Epoch: 00 [ 6164/25439 ( 24%)], Train Loss: 0.69058\n",
      "Epoch: 00 [ 6204/25439 ( 24%)], Train Loss: 0.68987\n",
      "Epoch: 00 [ 6244/25439 ( 25%)], Train Loss: 0.68936\n",
      "Epoch: 00 [ 6284/25439 ( 25%)], Train Loss: 0.68890\n",
      "Epoch: 00 [ 6324/25439 ( 25%)], Train Loss: 0.68808\n",
      "Epoch: 00 [ 6364/25439 ( 25%)], Train Loss: 0.68675\n",
      "Epoch: 00 [ 6404/25439 ( 25%)], Train Loss: 0.68607\n",
      "Epoch: 00 [ 6444/25439 ( 25%)], Train Loss: 0.68480\n",
      "Epoch: 00 [ 6484/25439 ( 25%)], Train Loss: 0.68269\n",
      "Epoch: 00 [ 6524/25439 ( 26%)], Train Loss: 0.68136\n",
      "Epoch: 00 [ 6564/25439 ( 26%)], Train Loss: 0.68063\n",
      "Epoch: 00 [ 6604/25439 ( 26%)], Train Loss: 0.67917\n",
      "Epoch: 00 [ 6644/25439 ( 26%)], Train Loss: 0.67693\n",
      "Epoch: 00 [ 6684/25439 ( 26%)], Train Loss: 0.67662\n",
      "Epoch: 00 [ 6724/25439 ( 26%)], Train Loss: 0.67504\n",
      "Epoch: 00 [ 6764/25439 ( 27%)], Train Loss: 0.67488\n",
      "Epoch: 00 [ 6804/25439 ( 27%)], Train Loss: 0.67360\n",
      "Epoch: 00 [ 6844/25439 ( 27%)], Train Loss: 0.67273\n",
      "Epoch: 00 [ 6884/25439 ( 27%)], Train Loss: 0.67239\n",
      "Epoch: 00 [ 6924/25439 ( 27%)], Train Loss: 0.67203\n",
      "Epoch: 00 [ 6964/25439 ( 27%)], Train Loss: 0.67139\n",
      "Epoch: 00 [ 7004/25439 ( 28%)], Train Loss: 0.67181\n",
      "Epoch: 00 [ 7044/25439 ( 28%)], Train Loss: 0.67139\n",
      "Epoch: 00 [ 7084/25439 ( 28%)], Train Loss: 0.67032\n",
      "Epoch: 00 [ 7124/25439 ( 28%)], Train Loss: 0.67018\n",
      "Epoch: 00 [ 7164/25439 ( 28%)], Train Loss: 0.66978\n",
      "Epoch: 00 [ 7204/25439 ( 28%)], Train Loss: 0.67037\n",
      "Epoch: 00 [ 7244/25439 ( 28%)], Train Loss: 0.67019\n",
      "Epoch: 00 [ 7284/25439 ( 29%)], Train Loss: 0.66801\n",
      "Epoch: 00 [ 7324/25439 ( 29%)], Train Loss: 0.66683\n",
      "Epoch: 00 [ 7364/25439 ( 29%)], Train Loss: 0.66495\n",
      "Epoch: 00 [ 7404/25439 ( 29%)], Train Loss: 0.66435\n",
      "Epoch: 00 [ 7444/25439 ( 29%)], Train Loss: 0.66378\n",
      "Epoch: 00 [ 7484/25439 ( 29%)], Train Loss: 0.66219\n",
      "Epoch: 00 [ 7524/25439 ( 30%)], Train Loss: 0.66090\n",
      "Epoch: 00 [ 7564/25439 ( 30%)], Train Loss: 0.66005\n",
      "Epoch: 00 [ 7604/25439 ( 30%)], Train Loss: 0.65849\n",
      "Epoch: 00 [ 7644/25439 ( 30%)], Train Loss: 0.65775\n",
      "Epoch: 00 [ 7684/25439 ( 30%)], Train Loss: 0.65635\n",
      "Epoch: 00 [ 7724/25439 ( 30%)], Train Loss: 0.65496\n",
      "Epoch: 00 [ 7764/25439 ( 31%)], Train Loss: 0.65417\n",
      "Epoch: 00 [ 7804/25439 ( 31%)], Train Loss: 0.65376\n",
      "Epoch: 00 [ 7844/25439 ( 31%)], Train Loss: 0.65268\n",
      "Epoch: 00 [ 7884/25439 ( 31%)], Train Loss: 0.65207\n",
      "Epoch: 00 [ 7924/25439 ( 31%)], Train Loss: 0.65083\n",
      "Epoch: 00 [ 7964/25439 ( 31%)], Train Loss: 0.65029\n",
      "Epoch: 00 [ 8004/25439 ( 31%)], Train Loss: 0.64919\n",
      "Epoch: 00 [ 8044/25439 ( 32%)], Train Loss: 0.64792\n",
      "Epoch: 00 [ 8084/25439 ( 32%)], Train Loss: 0.64665\n",
      "Epoch: 00 [ 8124/25439 ( 32%)], Train Loss: 0.64538\n",
      "Epoch: 00 [ 8164/25439 ( 32%)], Train Loss: 0.64512\n",
      "Epoch: 00 [ 8204/25439 ( 32%)], Train Loss: 0.64562\n",
      "Epoch: 00 [ 8244/25439 ( 32%)], Train Loss: 0.64495\n",
      "Epoch: 00 [ 8284/25439 ( 33%)], Train Loss: 0.64386\n",
      "Epoch: 00 [ 8324/25439 ( 33%)], Train Loss: 0.64230\n",
      "Epoch: 00 [ 8364/25439 ( 33%)], Train Loss: 0.64175\n",
      "Epoch: 00 [ 8404/25439 ( 33%)], Train Loss: 0.64214\n",
      "Epoch: 00 [ 8444/25439 ( 33%)], Train Loss: 0.64128\n",
      "Epoch: 00 [ 8484/25439 ( 33%)], Train Loss: 0.64036\n",
      "Epoch: 00 [ 8524/25439 ( 34%)], Train Loss: 0.63927\n",
      "Epoch: 00 [ 8564/25439 ( 34%)], Train Loss: 0.63986\n",
      "Epoch: 00 [ 8604/25439 ( 34%)], Train Loss: 0.63874\n",
      "Epoch: 00 [ 8644/25439 ( 34%)], Train Loss: 0.63767\n",
      "Epoch: 00 [ 8684/25439 ( 34%)], Train Loss: 0.63594\n",
      "Epoch: 00 [ 8724/25439 ( 34%)], Train Loss: 0.63603\n",
      "Epoch: 00 [ 8764/25439 ( 34%)], Train Loss: 0.63532\n",
      "Epoch: 00 [ 8804/25439 ( 35%)], Train Loss: 0.63490\n",
      "Epoch: 00 [ 8844/25439 ( 35%)], Train Loss: 0.63323\n",
      "Epoch: 00 [ 8884/25439 ( 35%)], Train Loss: 0.63220\n",
      "Epoch: 00 [ 8924/25439 ( 35%)], Train Loss: 0.63134\n",
      "Epoch: 00 [ 8964/25439 ( 35%)], Train Loss: 0.62974\n",
      "Epoch: 00 [ 9004/25439 ( 35%)], Train Loss: 0.62845\n",
      "Epoch: 00 [ 9044/25439 ( 36%)], Train Loss: 0.62718\n",
      "Epoch: 00 [ 9084/25439 ( 36%)], Train Loss: 0.62734\n",
      "Epoch: 00 [ 9124/25439 ( 36%)], Train Loss: 0.62616\n",
      "Epoch: 00 [ 9164/25439 ( 36%)], Train Loss: 0.62466\n",
      "Epoch: 00 [ 9204/25439 ( 36%)], Train Loss: 0.62422\n",
      "Epoch: 00 [ 9244/25439 ( 36%)], Train Loss: 0.62395\n",
      "Epoch: 00 [ 9284/25439 ( 36%)], Train Loss: 0.62342\n",
      "Epoch: 00 [ 9324/25439 ( 37%)], Train Loss: 0.62372\n",
      "Epoch: 00 [ 9364/25439 ( 37%)], Train Loss: 0.62380\n",
      "Epoch: 00 [ 9404/25439 ( 37%)], Train Loss: 0.62427\n",
      "Epoch: 00 [ 9444/25439 ( 37%)], Train Loss: 0.62409\n",
      "Epoch: 00 [ 9484/25439 ( 37%)], Train Loss: 0.62320\n",
      "Epoch: 00 [ 9524/25439 ( 37%)], Train Loss: 0.62178\n",
      "Epoch: 00 [ 9564/25439 ( 38%)], Train Loss: 0.62086\n",
      "Epoch: 00 [ 9604/25439 ( 38%)], Train Loss: 0.62034\n",
      "Epoch: 00 [ 9644/25439 ( 38%)], Train Loss: 0.62019\n",
      "Epoch: 00 [ 9684/25439 ( 38%)], Train Loss: 0.61942\n",
      "Epoch: 00 [ 9724/25439 ( 38%)], Train Loss: 0.61803\n",
      "Epoch: 00 [ 9764/25439 ( 38%)], Train Loss: 0.61776\n",
      "Epoch: 00 [ 9804/25439 ( 39%)], Train Loss: 0.61622\n",
      "Epoch: 00 [ 9844/25439 ( 39%)], Train Loss: 0.61532\n",
      "Epoch: 00 [ 9884/25439 ( 39%)], Train Loss: 0.61500\n",
      "Epoch: 00 [ 9924/25439 ( 39%)], Train Loss: 0.61547\n",
      "Epoch: 00 [ 9964/25439 ( 39%)], Train Loss: 0.61630\n",
      "Epoch: 00 [10004/25439 ( 39%)], Train Loss: 0.61604\n",
      "Epoch: 00 [10044/25439 ( 39%)], Train Loss: 0.61499\n",
      "Epoch: 00 [10084/25439 ( 40%)], Train Loss: 0.61460\n",
      "Epoch: 00 [10124/25439 ( 40%)], Train Loss: 0.61407\n",
      "Epoch: 00 [10164/25439 ( 40%)], Train Loss: 0.61447\n",
      "Epoch: 00 [10204/25439 ( 40%)], Train Loss: 0.61308\n",
      "Epoch: 00 [10244/25439 ( 40%)], Train Loss: 0.61227\n",
      "Epoch: 00 [10284/25439 ( 40%)], Train Loss: 0.61285\n",
      "Epoch: 00 [10324/25439 ( 41%)], Train Loss: 0.61274\n",
      "Epoch: 00 [10364/25439 ( 41%)], Train Loss: 0.61297\n",
      "Epoch: 00 [10404/25439 ( 41%)], Train Loss: 0.61236\n",
      "Epoch: 00 [10444/25439 ( 41%)], Train Loss: 0.61211\n",
      "Epoch: 00 [10484/25439 ( 41%)], Train Loss: 0.61104\n",
      "Epoch: 00 [10524/25439 ( 41%)], Train Loss: 0.61043\n",
      "Epoch: 00 [10564/25439 ( 42%)], Train Loss: 0.60975\n",
      "Epoch: 00 [10604/25439 ( 42%)], Train Loss: 0.60924\n",
      "Epoch: 00 [10644/25439 ( 42%)], Train Loss: 0.60782\n",
      "Epoch: 00 [10684/25439 ( 42%)], Train Loss: 0.60685\n",
      "Epoch: 00 [10724/25439 ( 42%)], Train Loss: 0.60662\n",
      "Epoch: 00 [10764/25439 ( 42%)], Train Loss: 0.60615\n",
      "Epoch: 00 [10804/25439 ( 42%)], Train Loss: 0.60531\n",
      "Epoch: 00 [10844/25439 ( 43%)], Train Loss: 0.60580\n",
      "Epoch: 00 [10884/25439 ( 43%)], Train Loss: 0.60521\n",
      "Epoch: 00 [10924/25439 ( 43%)], Train Loss: 0.60561\n",
      "Epoch: 00 [10964/25439 ( 43%)], Train Loss: 0.60488\n",
      "Epoch: 00 [11004/25439 ( 43%)], Train Loss: 0.60381\n",
      "Epoch: 00 [11044/25439 ( 43%)], Train Loss: 0.60281\n",
      "Epoch: 00 [11084/25439 ( 44%)], Train Loss: 0.60267\n",
      "Epoch: 00 [11124/25439 ( 44%)], Train Loss: 0.60377\n",
      "Epoch: 00 [11164/25439 ( 44%)], Train Loss: 0.60320\n",
      "Epoch: 00 [11204/25439 ( 44%)], Train Loss: 0.60254\n",
      "Epoch: 00 [11244/25439 ( 44%)], Train Loss: 0.60202\n",
      "Epoch: 00 [11284/25439 ( 44%)], Train Loss: 0.60143\n",
      "Epoch: 00 [11324/25439 ( 45%)], Train Loss: 0.60110\n",
      "Epoch: 00 [11364/25439 ( 45%)], Train Loss: 0.60062\n",
      "Epoch: 00 [11404/25439 ( 45%)], Train Loss: 0.59990\n",
      "Epoch: 00 [11444/25439 ( 45%)], Train Loss: 0.59975\n",
      "Epoch: 00 [11484/25439 ( 45%)], Train Loss: 0.59956\n",
      "Epoch: 00 [11524/25439 ( 45%)], Train Loss: 0.59874\n",
      "Epoch: 00 [11564/25439 ( 45%)], Train Loss: 0.59828\n",
      "Epoch: 00 [11604/25439 ( 46%)], Train Loss: 0.59778\n",
      "Epoch: 00 [11644/25439 ( 46%)], Train Loss: 0.59733\n",
      "Epoch: 00 [11684/25439 ( 46%)], Train Loss: 0.59624\n",
      "Epoch: 00 [11724/25439 ( 46%)], Train Loss: 0.59601\n",
      "Epoch: 00 [11764/25439 ( 46%)], Train Loss: 0.59507\n",
      "Epoch: 00 [11804/25439 ( 46%)], Train Loss: 0.59518\n",
      "Epoch: 00 [11844/25439 ( 47%)], Train Loss: 0.59416\n",
      "Epoch: 00 [11884/25439 ( 47%)], Train Loss: 0.59359\n",
      "Epoch: 00 [11924/25439 ( 47%)], Train Loss: 0.59368\n",
      "Epoch: 00 [11964/25439 ( 47%)], Train Loss: 0.59279\n",
      "Epoch: 00 [12004/25439 ( 47%)], Train Loss: 0.59174\n",
      "Epoch: 00 [12044/25439 ( 47%)], Train Loss: 0.59120\n",
      "Epoch: 00 [12084/25439 ( 48%)], Train Loss: 0.59119\n",
      "Epoch: 00 [12124/25439 ( 48%)], Train Loss: 0.59108\n",
      "Epoch: 00 [12164/25439 ( 48%)], Train Loss: 0.59064\n",
      "Epoch: 00 [12204/25439 ( 48%)], Train Loss: 0.59018\n",
      "Epoch: 00 [12244/25439 ( 48%)], Train Loss: 0.58968\n",
      "Epoch: 00 [12284/25439 ( 48%)], Train Loss: 0.58925\n",
      "Epoch: 00 [12324/25439 ( 48%)], Train Loss: 0.58822\n",
      "Epoch: 00 [12364/25439 ( 49%)], Train Loss: 0.58781\n",
      "Epoch: 00 [12404/25439 ( 49%)], Train Loss: 0.58760\n",
      "Epoch: 00 [12444/25439 ( 49%)], Train Loss: 0.58723\n",
      "Epoch: 00 [12484/25439 ( 49%)], Train Loss: 0.58686\n",
      "Epoch: 00 [12524/25439 ( 49%)], Train Loss: 0.58656\n",
      "Epoch: 00 [12564/25439 ( 49%)], Train Loss: 0.58584\n",
      "Epoch: 00 [12604/25439 ( 50%)], Train Loss: 0.58485\n",
      "Epoch: 00 [12644/25439 ( 50%)], Train Loss: 0.58406\n",
      "Epoch: 00 [12684/25439 ( 50%)], Train Loss: 0.58275\n",
      "Epoch: 00 [12724/25439 ( 50%)], Train Loss: 0.58205\n",
      "Epoch: 00 [12764/25439 ( 50%)], Train Loss: 0.58177\n",
      "Epoch: 00 [12804/25439 ( 50%)], Train Loss: 0.58107\n",
      "Epoch: 00 [12844/25439 ( 50%)], Train Loss: 0.58011\n",
      "Epoch: 00 [12884/25439 ( 51%)], Train Loss: 0.57958\n",
      "Epoch: 00 [12924/25439 ( 51%)], Train Loss: 0.57904\n",
      "Epoch: 00 [12964/25439 ( 51%)], Train Loss: 0.57867\n",
      "Epoch: 00 [13004/25439 ( 51%)], Train Loss: 0.57791\n",
      "Epoch: 00 [13044/25439 ( 51%)], Train Loss: 0.57686\n",
      "Epoch: 00 [13084/25439 ( 51%)], Train Loss: 0.57609\n",
      "Epoch: 00 [13124/25439 ( 52%)], Train Loss: 0.57638\n",
      "Epoch: 00 [13164/25439 ( 52%)], Train Loss: 0.57593\n",
      "Epoch: 00 [13204/25439 ( 52%)], Train Loss: 0.57573\n",
      "Epoch: 00 [13244/25439 ( 52%)], Train Loss: 0.57505\n",
      "Epoch: 00 [13284/25439 ( 52%)], Train Loss: 0.57460\n",
      "Epoch: 00 [13324/25439 ( 52%)], Train Loss: 0.57438\n",
      "Epoch: 00 [13364/25439 ( 53%)], Train Loss: 0.57446\n",
      "Epoch: 00 [13404/25439 ( 53%)], Train Loss: 0.57434\n",
      "Epoch: 00 [13444/25439 ( 53%)], Train Loss: 0.57327\n",
      "Epoch: 00 [13484/25439 ( 53%)], Train Loss: 0.57259\n",
      "Epoch: 00 [13524/25439 ( 53%)], Train Loss: 0.57245\n",
      "Epoch: 00 [13564/25439 ( 53%)], Train Loss: 0.57154\n",
      "Epoch: 00 [13604/25439 ( 53%)], Train Loss: 0.57077\n",
      "Epoch: 00 [13644/25439 ( 54%)], Train Loss: 0.57053\n",
      "Epoch: 00 [13684/25439 ( 54%)], Train Loss: 0.57081\n",
      "Epoch: 00 [13724/25439 ( 54%)], Train Loss: 0.56980\n",
      "Epoch: 00 [13764/25439 ( 54%)], Train Loss: 0.56906\n",
      "Epoch: 00 [13804/25439 ( 54%)], Train Loss: 0.56919\n",
      "Epoch: 00 [13844/25439 ( 54%)], Train Loss: 0.56900\n",
      "Epoch: 00 [13884/25439 ( 55%)], Train Loss: 0.56892\n",
      "Epoch: 00 [13924/25439 ( 55%)], Train Loss: 0.56851\n",
      "Epoch: 00 [13964/25439 ( 55%)], Train Loss: 0.56831\n",
      "Epoch: 00 [14004/25439 ( 55%)], Train Loss: 0.56771\n",
      "Epoch: 00 [14044/25439 ( 55%)], Train Loss: 0.56755\n",
      "Epoch: 00 [14084/25439 ( 55%)], Train Loss: 0.56689\n",
      "Epoch: 00 [14124/25439 ( 56%)], Train Loss: 0.56600\n",
      "Epoch: 00 [14164/25439 ( 56%)], Train Loss: 0.56540\n",
      "Epoch: 00 [14204/25439 ( 56%)], Train Loss: 0.56528\n",
      "Epoch: 00 [14244/25439 ( 56%)], Train Loss: 0.56516\n",
      "Epoch: 00 [14284/25439 ( 56%)], Train Loss: 0.56479\n",
      "Epoch: 00 [14324/25439 ( 56%)], Train Loss: 0.56373\n",
      "Epoch: 00 [14364/25439 ( 56%)], Train Loss: 0.56333\n",
      "Epoch: 00 [14404/25439 ( 57%)], Train Loss: 0.56312\n",
      "Epoch: 00 [14444/25439 ( 57%)], Train Loss: 0.56289\n",
      "Epoch: 00 [14484/25439 ( 57%)], Train Loss: 0.56219\n",
      "Epoch: 00 [14524/25439 ( 57%)], Train Loss: 0.56217\n",
      "Epoch: 00 [14564/25439 ( 57%)], Train Loss: 0.56201\n",
      "Epoch: 00 [14604/25439 ( 57%)], Train Loss: 0.56169\n",
      "Epoch: 00 [14644/25439 ( 58%)], Train Loss: 0.56095\n",
      "Epoch: 00 [14684/25439 ( 58%)], Train Loss: 0.56085\n",
      "Epoch: 00 [14724/25439 ( 58%)], Train Loss: 0.56024\n",
      "Epoch: 00 [14764/25439 ( 58%)], Train Loss: 0.56039\n",
      "Epoch: 00 [14804/25439 ( 58%)], Train Loss: 0.55963\n",
      "Epoch: 00 [14844/25439 ( 58%)], Train Loss: 0.55962\n",
      "Epoch: 00 [14884/25439 ( 59%)], Train Loss: 0.55890\n",
      "Epoch: 00 [14924/25439 ( 59%)], Train Loss: 0.55878\n",
      "Epoch: 00 [14964/25439 ( 59%)], Train Loss: 0.55845\n",
      "Epoch: 00 [15004/25439 ( 59%)], Train Loss: 0.55872\n",
      "Epoch: 00 [15044/25439 ( 59%)], Train Loss: 0.55789\n",
      "Epoch: 00 [15084/25439 ( 59%)], Train Loss: 0.55759\n",
      "Epoch: 00 [15124/25439 ( 59%)], Train Loss: 0.55715\n",
      "Epoch: 00 [15164/25439 ( 60%)], Train Loss: 0.55628\n",
      "Epoch: 00 [15204/25439 ( 60%)], Train Loss: 0.55577\n",
      "Epoch: 00 [15244/25439 ( 60%)], Train Loss: 0.55568\n",
      "Epoch: 00 [15284/25439 ( 60%)], Train Loss: 0.55529\n",
      "Epoch: 00 [15324/25439 ( 60%)], Train Loss: 0.55462\n",
      "Epoch: 00 [15364/25439 ( 60%)], Train Loss: 0.55398\n",
      "Epoch: 00 [15404/25439 ( 61%)], Train Loss: 0.55360\n",
      "Epoch: 00 [15444/25439 ( 61%)], Train Loss: 0.55316\n",
      "Epoch: 00 [15484/25439 ( 61%)], Train Loss: 0.55308\n",
      "Epoch: 00 [15524/25439 ( 61%)], Train Loss: 0.55331\n",
      "Epoch: 00 [15564/25439 ( 61%)], Train Loss: 0.55307\n",
      "Epoch: 00 [15604/25439 ( 61%)], Train Loss: 0.55280\n",
      "Epoch: 00 [15644/25439 ( 61%)], Train Loss: 0.55227\n",
      "Epoch: 00 [15684/25439 ( 62%)], Train Loss: 0.55179\n",
      "Epoch: 00 [15724/25439 ( 62%)], Train Loss: 0.55140\n",
      "Epoch: 00 [15764/25439 ( 62%)], Train Loss: 0.55113\n",
      "Epoch: 00 [15804/25439 ( 62%)], Train Loss: 0.55125\n",
      "Epoch: 00 [15844/25439 ( 62%)], Train Loss: 0.55115\n",
      "Epoch: 00 [15884/25439 ( 62%)], Train Loss: 0.55124\n",
      "Epoch: 00 [15924/25439 ( 63%)], Train Loss: 0.55104\n",
      "Epoch: 00 [15964/25439 ( 63%)], Train Loss: 0.55089\n",
      "Epoch: 00 [16004/25439 ( 63%)], Train Loss: 0.55080\n",
      "Epoch: 00 [16044/25439 ( 63%)], Train Loss: 0.55032\n",
      "Epoch: 00 [16084/25439 ( 63%)], Train Loss: 0.54954\n",
      "Epoch: 00 [16124/25439 ( 63%)], Train Loss: 0.54888\n",
      "Epoch: 00 [16164/25439 ( 64%)], Train Loss: 0.54843\n",
      "Epoch: 00 [16204/25439 ( 64%)], Train Loss: 0.54814\n",
      "Epoch: 00 [16244/25439 ( 64%)], Train Loss: 0.54831\n",
      "Epoch: 00 [16284/25439 ( 64%)], Train Loss: 0.54804\n",
      "Epoch: 00 [16324/25439 ( 64%)], Train Loss: 0.54786\n",
      "Epoch: 00 [16364/25439 ( 64%)], Train Loss: 0.54730\n",
      "Epoch: 00 [16404/25439 ( 64%)], Train Loss: 0.54663\n",
      "Epoch: 00 [16444/25439 ( 65%)], Train Loss: 0.54650\n",
      "Epoch: 00 [16484/25439 ( 65%)], Train Loss: 0.54585\n",
      "Epoch: 00 [16524/25439 ( 65%)], Train Loss: 0.54559\n",
      "Epoch: 00 [16564/25439 ( 65%)], Train Loss: 0.54542\n",
      "Epoch: 00 [16604/25439 ( 65%)], Train Loss: 0.54560\n",
      "Epoch: 00 [16644/25439 ( 65%)], Train Loss: 0.54533\n",
      "Epoch: 00 [16684/25439 ( 66%)], Train Loss: 0.54494\n",
      "Epoch: 00 [16724/25439 ( 66%)], Train Loss: 0.54431\n",
      "Epoch: 00 [16764/25439 ( 66%)], Train Loss: 0.54391\n",
      "Epoch: 00 [16804/25439 ( 66%)], Train Loss: 0.54368\n",
      "Epoch: 00 [16844/25439 ( 66%)], Train Loss: 0.54346\n",
      "Epoch: 00 [16884/25439 ( 66%)], Train Loss: 0.54334\n",
      "Epoch: 00 [16924/25439 ( 67%)], Train Loss: 0.54273\n",
      "Epoch: 00 [16964/25439 ( 67%)], Train Loss: 0.54239\n",
      "Epoch: 00 [17004/25439 ( 67%)], Train Loss: 0.54233\n",
      "Epoch: 00 [17044/25439 ( 67%)], Train Loss: 0.54214\n",
      "Epoch: 00 [17084/25439 ( 67%)], Train Loss: 0.54217\n",
      "Epoch: 00 [17124/25439 ( 67%)], Train Loss: 0.54191\n",
      "Epoch: 00 [17164/25439 ( 67%)], Train Loss: 0.54156\n",
      "Epoch: 00 [17204/25439 ( 68%)], Train Loss: 0.54164\n",
      "Epoch: 00 [17244/25439 ( 68%)], Train Loss: 0.54121\n",
      "Epoch: 00 [17284/25439 ( 68%)], Train Loss: 0.54151\n",
      "Epoch: 00 [17324/25439 ( 68%)], Train Loss: 0.54108\n",
      "Epoch: 00 [17364/25439 ( 68%)], Train Loss: 0.54081\n",
      "Epoch: 00 [17404/25439 ( 68%)], Train Loss: 0.54087\n",
      "Epoch: 00 [17444/25439 ( 69%)], Train Loss: 0.54065\n",
      "Epoch: 00 [17484/25439 ( 69%)], Train Loss: 0.53971\n",
      "Epoch: 00 [17524/25439 ( 69%)], Train Loss: 0.53906\n",
      "Epoch: 00 [17564/25439 ( 69%)], Train Loss: 0.53910\n",
      "Epoch: 00 [17604/25439 ( 69%)], Train Loss: 0.53871\n",
      "Epoch: 00 [17644/25439 ( 69%)], Train Loss: 0.53802\n",
      "Epoch: 00 [17684/25439 ( 70%)], Train Loss: 0.53766\n",
      "Epoch: 00 [17724/25439 ( 70%)], Train Loss: 0.53734\n",
      "Epoch: 00 [17764/25439 ( 70%)], Train Loss: 0.53689\n",
      "Epoch: 00 [17804/25439 ( 70%)], Train Loss: 0.53649\n",
      "Epoch: 00 [17844/25439 ( 70%)], Train Loss: 0.53626\n",
      "Epoch: 00 [17884/25439 ( 70%)], Train Loss: 0.53587\n",
      "Epoch: 00 [17924/25439 ( 70%)], Train Loss: 0.53561\n",
      "Epoch: 00 [17964/25439 ( 71%)], Train Loss: 0.53539\n",
      "Epoch: 00 [18004/25439 ( 71%)], Train Loss: 0.53525\n",
      "Epoch: 00 [18044/25439 ( 71%)], Train Loss: 0.53528\n",
      "Epoch: 00 [18084/25439 ( 71%)], Train Loss: 0.53520\n",
      "Epoch: 00 [18124/25439 ( 71%)], Train Loss: 0.53489\n",
      "Epoch: 00 [18164/25439 ( 71%)], Train Loss: 0.53470\n",
      "Epoch: 00 [18204/25439 ( 72%)], Train Loss: 0.53445\n",
      "Epoch: 00 [18244/25439 ( 72%)], Train Loss: 0.53405\n",
      "Epoch: 00 [18284/25439 ( 72%)], Train Loss: 0.53363\n",
      "Epoch: 00 [18324/25439 ( 72%)], Train Loss: 0.53357\n",
      "Epoch: 00 [18364/25439 ( 72%)], Train Loss: 0.53312\n",
      "Epoch: 00 [18404/25439 ( 72%)], Train Loss: 0.53261\n",
      "Epoch: 00 [18444/25439 ( 73%)], Train Loss: 0.53233\n",
      "Epoch: 00 [18484/25439 ( 73%)], Train Loss: 0.53202\n",
      "Epoch: 00 [18524/25439 ( 73%)], Train Loss: 0.53194\n",
      "Epoch: 00 [18564/25439 ( 73%)], Train Loss: 0.53143\n",
      "Epoch: 00 [18604/25439 ( 73%)], Train Loss: 0.53148\n",
      "Epoch: 00 [18644/25439 ( 73%)], Train Loss: 0.53154\n",
      "Epoch: 00 [18684/25439 ( 73%)], Train Loss: 0.53081\n",
      "Epoch: 00 [18724/25439 ( 74%)], Train Loss: 0.53063\n",
      "Epoch: 00 [18764/25439 ( 74%)], Train Loss: 0.53015\n",
      "Epoch: 00 [18804/25439 ( 74%)], Train Loss: 0.53004\n",
      "Epoch: 00 [18844/25439 ( 74%)], Train Loss: 0.52986\n",
      "Epoch: 00 [18884/25439 ( 74%)], Train Loss: 0.52942\n",
      "Epoch: 00 [18924/25439 ( 74%)], Train Loss: 0.52893\n",
      "Epoch: 00 [18964/25439 ( 75%)], Train Loss: 0.52868\n",
      "Epoch: 00 [19004/25439 ( 75%)], Train Loss: 0.52826\n",
      "Epoch: 00 [19044/25439 ( 75%)], Train Loss: 0.52805\n",
      "Epoch: 00 [19084/25439 ( 75%)], Train Loss: 0.52823\n",
      "Epoch: 00 [19124/25439 ( 75%)], Train Loss: 0.52783\n",
      "Epoch: 00 [19164/25439 ( 75%)], Train Loss: 0.52722\n",
      "Epoch: 00 [19204/25439 ( 75%)], Train Loss: 0.52673\n",
      "Epoch: 00 [19244/25439 ( 76%)], Train Loss: 0.52619\n",
      "Epoch: 00 [19284/25439 ( 76%)], Train Loss: 0.52595\n",
      "Epoch: 00 [19324/25439 ( 76%)], Train Loss: 0.52550\n",
      "Epoch: 00 [19364/25439 ( 76%)], Train Loss: 0.52521\n",
      "Epoch: 00 [19404/25439 ( 76%)], Train Loss: 0.52553\n",
      "Epoch: 00 [19444/25439 ( 76%)], Train Loss: 0.52520\n",
      "Epoch: 00 [19484/25439 ( 77%)], Train Loss: 0.52468\n",
      "Epoch: 00 [19524/25439 ( 77%)], Train Loss: 0.52419\n",
      "Epoch: 00 [19564/25439 ( 77%)], Train Loss: 0.52397\n",
      "Epoch: 00 [19604/25439 ( 77%)], Train Loss: 0.52379\n",
      "Epoch: 00 [19644/25439 ( 77%)], Train Loss: 0.52339\n",
      "Epoch: 00 [19684/25439 ( 77%)], Train Loss: 0.52311\n",
      "Epoch: 00 [19724/25439 ( 78%)], Train Loss: 0.52247\n",
      "Epoch: 00 [19764/25439 ( 78%)], Train Loss: 0.52225\n",
      "Epoch: 00 [19804/25439 ( 78%)], Train Loss: 0.52234\n",
      "Epoch: 00 [19844/25439 ( 78%)], Train Loss: 0.52197\n",
      "Epoch: 00 [19884/25439 ( 78%)], Train Loss: 0.52180\n",
      "Epoch: 00 [19924/25439 ( 78%)], Train Loss: 0.52113\n",
      "Epoch: 00 [19964/25439 ( 78%)], Train Loss: 0.52048\n",
      "Epoch: 00 [20004/25439 ( 79%)], Train Loss: 0.51985\n",
      "Epoch: 00 [20044/25439 ( 79%)], Train Loss: 0.51930\n",
      "Epoch: 00 [20084/25439 ( 79%)], Train Loss: 0.51880\n",
      "Epoch: 00 [20124/25439 ( 79%)], Train Loss: 0.51840\n",
      "Epoch: 00 [20164/25439 ( 79%)], Train Loss: 0.51809\n",
      "Epoch: 00 [20204/25439 ( 79%)], Train Loss: 0.51786\n",
      "Epoch: 00 [20244/25439 ( 80%)], Train Loss: 0.51748\n",
      "Epoch: 00 [20284/25439 ( 80%)], Train Loss: 0.51715\n",
      "Epoch: 00 [20324/25439 ( 80%)], Train Loss: 0.51690\n",
      "Epoch: 00 [20364/25439 ( 80%)], Train Loss: 0.51668\n",
      "Epoch: 00 [20404/25439 ( 80%)], Train Loss: 0.51707\n",
      "Epoch: 00 [20444/25439 ( 80%)], Train Loss: 0.51675\n",
      "Epoch: 00 [20484/25439 ( 81%)], Train Loss: 0.51638\n",
      "Epoch: 00 [20524/25439 ( 81%)], Train Loss: 0.51631\n",
      "Epoch: 00 [20564/25439 ( 81%)], Train Loss: 0.51591\n",
      "Epoch: 00 [20604/25439 ( 81%)], Train Loss: 0.51564\n",
      "Epoch: 00 [20644/25439 ( 81%)], Train Loss: 0.51527\n",
      "Epoch: 00 [20684/25439 ( 81%)], Train Loss: 0.51501\n",
      "Epoch: 00 [20724/25439 ( 81%)], Train Loss: 0.51456\n",
      "Epoch: 00 [20764/25439 ( 82%)], Train Loss: 0.51412\n",
      "Epoch: 00 [20804/25439 ( 82%)], Train Loss: 0.51392\n",
      "Epoch: 00 [20844/25439 ( 82%)], Train Loss: 0.51335\n",
      "Epoch: 00 [20884/25439 ( 82%)], Train Loss: 0.51304\n",
      "Epoch: 00 [20924/25439 ( 82%)], Train Loss: 0.51291\n",
      "Epoch: 00 [20964/25439 ( 82%)], Train Loss: 0.51228\n",
      "Epoch: 00 [21004/25439 ( 83%)], Train Loss: 0.51212\n",
      "Epoch: 00 [21044/25439 ( 83%)], Train Loss: 0.51170\n",
      "Epoch: 00 [21084/25439 ( 83%)], Train Loss: 0.51156\n",
      "Epoch: 00 [21124/25439 ( 83%)], Train Loss: 0.51111\n",
      "Epoch: 00 [21164/25439 ( 83%)], Train Loss: 0.51126\n",
      "Epoch: 00 [21204/25439 ( 83%)], Train Loss: 0.51110\n",
      "Epoch: 00 [21244/25439 ( 84%)], Train Loss: 0.51072\n",
      "Epoch: 00 [21284/25439 ( 84%)], Train Loss: 0.51040\n",
      "Epoch: 00 [21324/25439 ( 84%)], Train Loss: 0.51028\n",
      "Epoch: 00 [21364/25439 ( 84%)], Train Loss: 0.50997\n",
      "Epoch: 00 [21404/25439 ( 84%)], Train Loss: 0.51001\n",
      "Epoch: 00 [21444/25439 ( 84%)], Train Loss: 0.51001\n",
      "Epoch: 00 [21484/25439 ( 84%)], Train Loss: 0.50959\n",
      "Epoch: 00 [21524/25439 ( 85%)], Train Loss: 0.50916\n",
      "Epoch: 00 [21564/25439 ( 85%)], Train Loss: 0.50903\n",
      "Epoch: 00 [21604/25439 ( 85%)], Train Loss: 0.50884\n",
      "Epoch: 00 [21644/25439 ( 85%)], Train Loss: 0.50824\n",
      "Epoch: 00 [21684/25439 ( 85%)], Train Loss: 0.50761\n",
      "Epoch: 00 [21724/25439 ( 85%)], Train Loss: 0.50718\n",
      "Epoch: 00 [21764/25439 ( 86%)], Train Loss: 0.50652\n",
      "Epoch: 00 [21804/25439 ( 86%)], Train Loss: 0.50612\n",
      "Epoch: 00 [21844/25439 ( 86%)], Train Loss: 0.50574\n",
      "Epoch: 00 [21884/25439 ( 86%)], Train Loss: 0.50526\n",
      "Epoch: 00 [21924/25439 ( 86%)], Train Loss: 0.50479\n",
      "Epoch: 00 [21964/25439 ( 86%)], Train Loss: 0.50452\n",
      "Epoch: 00 [22004/25439 ( 86%)], Train Loss: 0.50445\n",
      "Epoch: 00 [22044/25439 ( 87%)], Train Loss: 0.50427\n",
      "Epoch: 00 [22084/25439 ( 87%)], Train Loss: 0.50382\n",
      "Epoch: 00 [22124/25439 ( 87%)], Train Loss: 0.50369\n",
      "Epoch: 00 [22164/25439 ( 87%)], Train Loss: 0.50351\n",
      "Epoch: 00 [22204/25439 ( 87%)], Train Loss: 0.50324\n",
      "Epoch: 00 [22244/25439 ( 87%)], Train Loss: 0.50299\n",
      "Epoch: 00 [22284/25439 ( 88%)], Train Loss: 0.50313\n",
      "Epoch: 00 [22324/25439 ( 88%)], Train Loss: 0.50276\n",
      "Epoch: 00 [22364/25439 ( 88%)], Train Loss: 0.50268\n",
      "Epoch: 00 [22404/25439 ( 88%)], Train Loss: 0.50251\n",
      "Epoch: 00 [22444/25439 ( 88%)], Train Loss: 0.50217\n",
      "Epoch: 00 [22484/25439 ( 88%)], Train Loss: 0.50178\n",
      "Epoch: 00 [22524/25439 ( 89%)], Train Loss: 0.50116\n",
      "Epoch: 00 [22564/25439 ( 89%)], Train Loss: 0.50133\n",
      "Epoch: 00 [22604/25439 ( 89%)], Train Loss: 0.50090\n",
      "Epoch: 00 [22644/25439 ( 89%)], Train Loss: 0.50056\n",
      "Epoch: 00 [22684/25439 ( 89%)], Train Loss: 0.50033\n",
      "Epoch: 00 [22724/25439 ( 89%)], Train Loss: 0.50011\n",
      "Epoch: 00 [22764/25439 ( 89%)], Train Loss: 0.49971\n",
      "Epoch: 00 [22804/25439 ( 90%)], Train Loss: 0.49953\n",
      "Epoch: 00 [22844/25439 ( 90%)], Train Loss: 0.49933\n",
      "Epoch: 00 [22884/25439 ( 90%)], Train Loss: 0.49937\n",
      "Epoch: 00 [22924/25439 ( 90%)], Train Loss: 0.49912\n",
      "Epoch: 00 [22964/25439 ( 90%)], Train Loss: 0.49916\n",
      "Epoch: 00 [23004/25439 ( 90%)], Train Loss: 0.49877\n",
      "Epoch: 00 [23044/25439 ( 91%)], Train Loss: 0.49883\n",
      "Epoch: 00 [23084/25439 ( 91%)], Train Loss: 0.49827\n",
      "Epoch: 00 [23124/25439 ( 91%)], Train Loss: 0.49821\n",
      "Epoch: 00 [23164/25439 ( 91%)], Train Loss: 0.49800\n",
      "Epoch: 00 [23204/25439 ( 91%)], Train Loss: 0.49794\n",
      "Epoch: 00 [23244/25439 ( 91%)], Train Loss: 0.49758\n",
      "Epoch: 00 [23284/25439 ( 92%)], Train Loss: 0.49731\n",
      "Epoch: 00 [23324/25439 ( 92%)], Train Loss: 0.49724\n",
      "Epoch: 00 [23364/25439 ( 92%)], Train Loss: 0.49708\n",
      "Epoch: 00 [23404/25439 ( 92%)], Train Loss: 0.49653\n",
      "Epoch: 00 [23444/25439 ( 92%)], Train Loss: 0.49602\n",
      "Epoch: 00 [23484/25439 ( 92%)], Train Loss: 0.49561\n",
      "Epoch: 00 [23524/25439 ( 92%)], Train Loss: 0.49533\n",
      "Epoch: 00 [23564/25439 ( 93%)], Train Loss: 0.49489\n",
      "Epoch: 00 [23604/25439 ( 93%)], Train Loss: 0.49453\n",
      "Epoch: 00 [23644/25439 ( 93%)], Train Loss: 0.49416\n",
      "Epoch: 00 [23684/25439 ( 93%)], Train Loss: 0.49374\n",
      "Epoch: 00 [23724/25439 ( 93%)], Train Loss: 0.49341\n",
      "Epoch: 00 [23764/25439 ( 93%)], Train Loss: 0.49331\n",
      "Epoch: 00 [23804/25439 ( 94%)], Train Loss: 0.49307\n",
      "Epoch: 00 [23844/25439 ( 94%)], Train Loss: 0.49316\n",
      "Epoch: 00 [23884/25439 ( 94%)], Train Loss: 0.49335\n",
      "Epoch: 00 [23924/25439 ( 94%)], Train Loss: 0.49316\n",
      "Epoch: 00 [23964/25439 ( 94%)], Train Loss: 0.49354\n",
      "Epoch: 00 [24004/25439 ( 94%)], Train Loss: 0.49314\n",
      "Epoch: 00 [24044/25439 ( 95%)], Train Loss: 0.49279\n",
      "Epoch: 00 [24084/25439 ( 95%)], Train Loss: 0.49271\n",
      "Epoch: 00 [24124/25439 ( 95%)], Train Loss: 0.49227\n",
      "Epoch: 00 [24164/25439 ( 95%)], Train Loss: 0.49220\n",
      "Epoch: 00 [24204/25439 ( 95%)], Train Loss: 0.49177\n",
      "Epoch: 00 [24244/25439 ( 95%)], Train Loss: 0.49184\n",
      "Epoch: 00 [24284/25439 ( 95%)], Train Loss: 0.49166\n",
      "Epoch: 00 [24324/25439 ( 96%)], Train Loss: 0.49134\n",
      "Epoch: 00 [24364/25439 ( 96%)], Train Loss: 0.49149\n",
      "Epoch: 00 [24404/25439 ( 96%)], Train Loss: 0.49109\n",
      "Epoch: 00 [24444/25439 ( 96%)], Train Loss: 0.49094\n",
      "Epoch: 00 [24484/25439 ( 96%)], Train Loss: 0.49087\n",
      "Epoch: 00 [24524/25439 ( 96%)], Train Loss: 0.49045\n",
      "Epoch: 00 [24564/25439 ( 97%)], Train Loss: 0.48998\n",
      "Epoch: 00 [24604/25439 ( 97%)], Train Loss: 0.48968\n",
      "Epoch: 00 [24644/25439 ( 97%)], Train Loss: 0.48926\n",
      "Epoch: 00 [24684/25439 ( 97%)], Train Loss: 0.48886\n",
      "Epoch: 00 [24724/25439 ( 97%)], Train Loss: 0.48849\n",
      "Epoch: 00 [24764/25439 ( 97%)], Train Loss: 0.48834\n",
      "Epoch: 00 [24804/25439 ( 98%)], Train Loss: 0.48808\n",
      "Epoch: 00 [24844/25439 ( 98%)], Train Loss: 0.48812\n",
      "Epoch: 00 [24884/25439 ( 98%)], Train Loss: 0.48779\n",
      "Epoch: 00 [24924/25439 ( 98%)], Train Loss: 0.48782\n",
      "Epoch: 00 [24964/25439 ( 98%)], Train Loss: 0.48757\n",
      "Epoch: 00 [25004/25439 ( 98%)], Train Loss: 0.48718\n",
      "Epoch: 00 [25044/25439 ( 98%)], Train Loss: 0.48699\n",
      "Epoch: 00 [25084/25439 ( 99%)], Train Loss: 0.48678\n",
      "Epoch: 00 [25124/25439 ( 99%)], Train Loss: 0.48637\n",
      "Epoch: 00 [25164/25439 ( 99%)], Train Loss: 0.48601\n",
      "Epoch: 00 [25204/25439 ( 99%)], Train Loss: 0.48574\n",
      "Epoch: 00 [25244/25439 ( 99%)], Train Loss: 0.48532\n",
      "Epoch: 00 [25284/25439 ( 99%)], Train Loss: 0.48510\n",
      "Epoch: 00 [25324/25439 (100%)], Train Loss: 0.48495\n",
      "Epoch: 00 [25364/25439 (100%)], Train Loss: 0.48501\n",
      "Epoch: 00 [25404/25439 (100%)], Train Loss: 0.48493\n",
      "Epoch: 00 [25439/25439 (100%)], Train Loss: 0.48499\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] Valid Loss: 0.20509\n",
      "Post-processing 223 example predictions split into 3061 features.\n",
      "valid jaccard:  0.709434903661361\n",
      "0 Epoch, Best epoch was updated! Valid Loss: 0.20509\n",
      "Saving model checkpoint to output/checkpoint-fold-1-epoch-0.\n",
      "\n",
      "Total Training Time: 3741.11159992218secs, Average Training Time per Epoch: 3741.11159992218secs.\n",
      "Total Validation Time: 146.3530113697052secs, Average Validation Time per Epoch: 146.3530113697052secs.\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "FOLD: 2\n",
      "--------------------------------------------------\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Num examples Train= 25676, Num examples Valid=2824\n",
      "Total Training Steps: 3210, Total Warmup Steps: 321\n",
      "Epoch: 00 [    4/25676 (  0%)], Train Loss: 3.55252\n",
      "Epoch: 00 [   44/25676 (  0%)], Train Loss: 3.32949\n",
      "Epoch: 00 [   84/25676 (  0%)], Train Loss: 3.28230\n",
      "Epoch: 00 [  124/25676 (  0%)], Train Loss: 3.26465\n",
      "Epoch: 00 [  164/25676 (  1%)], Train Loss: 3.20717\n",
      "Epoch: 00 [  204/25676 (  1%)], Train Loss: 3.15453\n",
      "Epoch: 00 [  244/25676 (  1%)], Train Loss: 3.09605\n",
      "Epoch: 00 [  284/25676 (  1%)], Train Loss: 3.01372\n",
      "Epoch: 00 [  324/25676 (  1%)], Train Loss: 2.92227\n",
      "Epoch: 00 [  364/25676 (  1%)], Train Loss: 2.82914\n",
      "Epoch: 00 [  404/25676 (  2%)], Train Loss: 2.72173\n",
      "Epoch: 00 [  444/25676 (  2%)], Train Loss: 2.59992\n",
      "Epoch: 00 [  484/25676 (  2%)], Train Loss: 2.44762\n",
      "Epoch: 00 [  524/25676 (  2%)], Train Loss: 2.34768\n",
      "Epoch: 00 [  564/25676 (  2%)], Train Loss: 2.24361\n",
      "Epoch: 00 [  604/25676 (  2%)], Train Loss: 2.15018\n",
      "Epoch: 00 [  644/25676 (  3%)], Train Loss: 2.05344\n",
      "Epoch: 00 [  684/25676 (  3%)], Train Loss: 1.96804\n",
      "Epoch: 00 [  724/25676 (  3%)], Train Loss: 1.89701\n",
      "Epoch: 00 [  764/25676 (  3%)], Train Loss: 1.82419\n",
      "Epoch: 00 [  804/25676 (  3%)], Train Loss: 1.77207\n",
      "Epoch: 00 [  844/25676 (  3%)], Train Loss: 1.71323\n",
      "Epoch: 00 [  884/25676 (  3%)], Train Loss: 1.66890\n",
      "Epoch: 00 [  924/25676 (  4%)], Train Loss: 1.62707\n",
      "Epoch: 00 [  964/25676 (  4%)], Train Loss: 1.57951\n",
      "Epoch: 00 [ 1004/25676 (  4%)], Train Loss: 1.53701\n",
      "Epoch: 00 [ 1044/25676 (  4%)], Train Loss: 1.49415\n",
      "Epoch: 00 [ 1084/25676 (  4%)], Train Loss: 1.45517\n",
      "Epoch: 00 [ 1124/25676 (  4%)], Train Loss: 1.41256\n",
      "Epoch: 00 [ 1164/25676 (  5%)], Train Loss: 1.38422\n",
      "Epoch: 00 [ 1204/25676 (  5%)], Train Loss: 1.34848\n",
      "Epoch: 00 [ 1244/25676 (  5%)], Train Loss: 1.32852\n",
      "Epoch: 00 [ 1284/25676 (  5%)], Train Loss: 1.30513\n",
      "Epoch: 00 [ 1324/25676 (  5%)], Train Loss: 1.27881\n",
      "Epoch: 00 [ 1364/25676 (  5%)], Train Loss: 1.24765\n",
      "Epoch: 00 [ 1404/25676 (  5%)], Train Loss: 1.22406\n",
      "Epoch: 00 [ 1444/25676 (  6%)], Train Loss: 1.20915\n",
      "Epoch: 00 [ 1484/25676 (  6%)], Train Loss: 1.19222\n",
      "Epoch: 00 [ 1524/25676 (  6%)], Train Loss: 1.17664\n",
      "Epoch: 00 [ 1564/25676 (  6%)], Train Loss: 1.16189\n",
      "Epoch: 00 [ 1604/25676 (  6%)], Train Loss: 1.14473\n",
      "Epoch: 00 [ 1644/25676 (  6%)], Train Loss: 1.13276\n",
      "Epoch: 00 [ 1684/25676 (  7%)], Train Loss: 1.12247\n",
      "Epoch: 00 [ 1724/25676 (  7%)], Train Loss: 1.10985\n",
      "Epoch: 00 [ 1764/25676 (  7%)], Train Loss: 1.09870\n",
      "Epoch: 00 [ 1804/25676 (  7%)], Train Loss: 1.08361\n",
      "Epoch: 00 [ 1844/25676 (  7%)], Train Loss: 1.06798\n",
      "Epoch: 00 [ 1884/25676 (  7%)], Train Loss: 1.05534\n",
      "Epoch: 00 [ 1924/25676 (  7%)], Train Loss: 1.04840\n",
      "Epoch: 00 [ 1964/25676 (  8%)], Train Loss: 1.04063\n",
      "Epoch: 00 [ 2004/25676 (  8%)], Train Loss: 1.03196\n",
      "Epoch: 00 [ 2044/25676 (  8%)], Train Loss: 1.02527\n",
      "Epoch: 00 [ 2084/25676 (  8%)], Train Loss: 1.01476\n",
      "Epoch: 00 [ 2124/25676 (  8%)], Train Loss: 1.00496\n",
      "Epoch: 00 [ 2164/25676 (  8%)], Train Loss: 0.99631\n",
      "Epoch: 00 [ 2204/25676 (  9%)], Train Loss: 0.98582\n",
      "Epoch: 00 [ 2244/25676 (  9%)], Train Loss: 0.97303\n",
      "Epoch: 00 [ 2284/25676 (  9%)], Train Loss: 0.96562\n",
      "Epoch: 00 [ 2324/25676 (  9%)], Train Loss: 0.96390\n",
      "Epoch: 00 [ 2364/25676 (  9%)], Train Loss: 0.95566\n",
      "Epoch: 00 [ 2404/25676 (  9%)], Train Loss: 0.94984\n",
      "Epoch: 00 [ 2444/25676 ( 10%)], Train Loss: 0.94646\n",
      "Epoch: 00 [ 2484/25676 ( 10%)], Train Loss: 0.94153\n",
      "Epoch: 00 [ 2524/25676 ( 10%)], Train Loss: 0.94092\n",
      "Epoch: 00 [ 2564/25676 ( 10%)], Train Loss: 0.93564\n",
      "Epoch: 00 [ 2604/25676 ( 10%)], Train Loss: 0.93196\n",
      "Epoch: 00 [ 2644/25676 ( 10%)], Train Loss: 0.92460\n",
      "Epoch: 00 [ 2684/25676 ( 10%)], Train Loss: 0.92144\n",
      "Epoch: 00 [ 2724/25676 ( 11%)], Train Loss: 0.91614\n",
      "Epoch: 00 [ 2764/25676 ( 11%)], Train Loss: 0.91035\n",
      "Epoch: 00 [ 2804/25676 ( 11%)], Train Loss: 0.90377\n",
      "Epoch: 00 [ 2844/25676 ( 11%)], Train Loss: 0.89562\n",
      "Epoch: 00 [ 2884/25676 ( 11%)], Train Loss: 0.89158\n",
      "Epoch: 00 [ 2924/25676 ( 11%)], Train Loss: 0.88615\n",
      "Epoch: 00 [ 2964/25676 ( 12%)], Train Loss: 0.88138\n",
      "Epoch: 00 [ 3004/25676 ( 12%)], Train Loss: 0.87656\n",
      "Epoch: 00 [ 3044/25676 ( 12%)], Train Loss: 0.87166\n",
      "Epoch: 00 [ 3084/25676 ( 12%)], Train Loss: 0.86839\n",
      "Epoch: 00 [ 3124/25676 ( 12%)], Train Loss: 0.86633\n",
      "Epoch: 00 [ 3164/25676 ( 12%)], Train Loss: 0.86403\n",
      "Epoch: 00 [ 3204/25676 ( 12%)], Train Loss: 0.86119\n",
      "Epoch: 00 [ 3244/25676 ( 13%)], Train Loss: 0.85858\n",
      "Epoch: 00 [ 3284/25676 ( 13%)], Train Loss: 0.85358\n",
      "Epoch: 00 [ 3324/25676 ( 13%)], Train Loss: 0.85273\n",
      "Epoch: 00 [ 3364/25676 ( 13%)], Train Loss: 0.84853\n",
      "Epoch: 00 [ 3404/25676 ( 13%)], Train Loss: 0.84215\n",
      "Epoch: 00 [ 3444/25676 ( 13%)], Train Loss: 0.83610\n",
      "Epoch: 00 [ 3484/25676 ( 14%)], Train Loss: 0.83096\n",
      "Epoch: 00 [ 3524/25676 ( 14%)], Train Loss: 0.82600\n",
      "Epoch: 00 [ 3564/25676 ( 14%)], Train Loss: 0.82343\n",
      "Epoch: 00 [ 3604/25676 ( 14%)], Train Loss: 0.82022\n",
      "Epoch: 00 [ 3644/25676 ( 14%)], Train Loss: 0.81812\n",
      "Epoch: 00 [ 3684/25676 ( 14%)], Train Loss: 0.81481\n",
      "Epoch: 00 [ 3724/25676 ( 15%)], Train Loss: 0.81240\n",
      "Epoch: 00 [ 3764/25676 ( 15%)], Train Loss: 0.80821\n",
      "Epoch: 00 [ 3804/25676 ( 15%)], Train Loss: 0.80497\n",
      "Epoch: 00 [ 3844/25676 ( 15%)], Train Loss: 0.80267\n",
      "Epoch: 00 [ 3884/25676 ( 15%)], Train Loss: 0.79854\n",
      "Epoch: 00 [ 3924/25676 ( 15%)], Train Loss: 0.79643\n",
      "Epoch: 00 [ 3964/25676 ( 15%)], Train Loss: 0.79679\n",
      "Epoch: 00 [ 4004/25676 ( 16%)], Train Loss: 0.79515\n",
      "Epoch: 00 [ 4044/25676 ( 16%)], Train Loss: 0.79009\n",
      "Epoch: 00 [ 4084/25676 ( 16%)], Train Loss: 0.78695\n",
      "Epoch: 00 [ 4124/25676 ( 16%)], Train Loss: 0.78596\n",
      "Epoch: 00 [ 4164/25676 ( 16%)], Train Loss: 0.78283\n",
      "Epoch: 00 [ 4204/25676 ( 16%)], Train Loss: 0.77986\n",
      "Epoch: 00 [ 4244/25676 ( 17%)], Train Loss: 0.78059\n",
      "Epoch: 00 [ 4284/25676 ( 17%)], Train Loss: 0.77812\n",
      "Epoch: 00 [ 4324/25676 ( 17%)], Train Loss: 0.77457\n",
      "Epoch: 00 [ 4364/25676 ( 17%)], Train Loss: 0.77133\n",
      "Epoch: 00 [ 4404/25676 ( 17%)], Train Loss: 0.76908\n",
      "Epoch: 00 [ 4444/25676 ( 17%)], Train Loss: 0.76721\n",
      "Epoch: 00 [ 4484/25676 ( 17%)], Train Loss: 0.76777\n",
      "Epoch: 00 [ 4524/25676 ( 18%)], Train Loss: 0.76684\n",
      "Epoch: 00 [ 4564/25676 ( 18%)], Train Loss: 0.76658\n",
      "Epoch: 00 [ 4604/25676 ( 18%)], Train Loss: 0.76510\n",
      "Epoch: 00 [ 4644/25676 ( 18%)], Train Loss: 0.76272\n",
      "Epoch: 00 [ 4684/25676 ( 18%)], Train Loss: 0.76134\n",
      "Epoch: 00 [ 4724/25676 ( 18%)], Train Loss: 0.75839\n",
      "Epoch: 00 [ 4764/25676 ( 19%)], Train Loss: 0.75727\n",
      "Epoch: 00 [ 4804/25676 ( 19%)], Train Loss: 0.75619\n",
      "Epoch: 00 [ 4844/25676 ( 19%)], Train Loss: 0.75382\n",
      "Epoch: 00 [ 4884/25676 ( 19%)], Train Loss: 0.75213\n",
      "Epoch: 00 [ 4924/25676 ( 19%)], Train Loss: 0.75065\n",
      "Epoch: 00 [ 4964/25676 ( 19%)], Train Loss: 0.74789\n",
      "Epoch: 00 [ 5004/25676 ( 19%)], Train Loss: 0.74563\n",
      "Epoch: 00 [ 5044/25676 ( 20%)], Train Loss: 0.74242\n",
      "Epoch: 00 [ 5084/25676 ( 20%)], Train Loss: 0.73806\n",
      "Epoch: 00 [ 5124/25676 ( 20%)], Train Loss: 0.73611\n",
      "Epoch: 00 [ 5164/25676 ( 20%)], Train Loss: 0.73392\n",
      "Epoch: 00 [ 5204/25676 ( 20%)], Train Loss: 0.73094\n",
      "Epoch: 00 [ 5244/25676 ( 20%)], Train Loss: 0.72828\n",
      "Epoch: 00 [ 5284/25676 ( 21%)], Train Loss: 0.72596\n",
      "Epoch: 00 [ 5324/25676 ( 21%)], Train Loss: 0.72479\n",
      "Epoch: 00 [ 5364/25676 ( 21%)], Train Loss: 0.72472\n",
      "Epoch: 00 [ 5404/25676 ( 21%)], Train Loss: 0.72454\n",
      "Epoch: 00 [ 5444/25676 ( 21%)], Train Loss: 0.72442\n",
      "Epoch: 00 [ 5484/25676 ( 21%)], Train Loss: 0.72133\n",
      "Epoch: 00 [ 5524/25676 ( 22%)], Train Loss: 0.71949\n",
      "Epoch: 00 [ 5564/25676 ( 22%)], Train Loss: 0.71750\n",
      "Epoch: 00 [ 5604/25676 ( 22%)], Train Loss: 0.71669\n",
      "Epoch: 00 [ 5644/25676 ( 22%)], Train Loss: 0.71527\n",
      "Epoch: 00 [ 5684/25676 ( 22%)], Train Loss: 0.71350\n",
      "Epoch: 00 [ 5724/25676 ( 22%)], Train Loss: 0.71342\n",
      "Epoch: 00 [ 5764/25676 ( 22%)], Train Loss: 0.71186\n",
      "Epoch: 00 [ 5804/25676 ( 23%)], Train Loss: 0.71035\n",
      "Epoch: 00 [ 5844/25676 ( 23%)], Train Loss: 0.70806\n",
      "Epoch: 00 [ 5884/25676 ( 23%)], Train Loss: 0.70647\n",
      "Epoch: 00 [ 5924/25676 ( 23%)], Train Loss: 0.70645\n",
      "Epoch: 00 [ 5964/25676 ( 23%)], Train Loss: 0.70425\n",
      "Epoch: 00 [ 6004/25676 ( 23%)], Train Loss: 0.70132\n",
      "Epoch: 00 [ 6044/25676 ( 24%)], Train Loss: 0.70083\n",
      "Epoch: 00 [ 6084/25676 ( 24%)], Train Loss: 0.69931\n",
      "Epoch: 00 [ 6124/25676 ( 24%)], Train Loss: 0.69858\n",
      "Epoch: 00 [ 6164/25676 ( 24%)], Train Loss: 0.69927\n",
      "Epoch: 00 [ 6204/25676 ( 24%)], Train Loss: 0.69783\n",
      "Epoch: 00 [ 6244/25676 ( 24%)], Train Loss: 0.69660\n",
      "Epoch: 00 [ 6284/25676 ( 24%)], Train Loss: 0.69502\n",
      "Epoch: 00 [ 6324/25676 ( 25%)], Train Loss: 0.69345\n",
      "Epoch: 00 [ 6364/25676 ( 25%)], Train Loss: 0.69442\n",
      "Epoch: 00 [ 6404/25676 ( 25%)], Train Loss: 0.69295\n",
      "Epoch: 00 [ 6444/25676 ( 25%)], Train Loss: 0.69137\n",
      "Epoch: 00 [ 6484/25676 ( 25%)], Train Loss: 0.69195\n",
      "Epoch: 00 [ 6524/25676 ( 25%)], Train Loss: 0.69008\n",
      "Epoch: 00 [ 6564/25676 ( 26%)], Train Loss: 0.68819\n",
      "Epoch: 00 [ 6604/25676 ( 26%)], Train Loss: 0.68872\n",
      "Epoch: 00 [ 6644/25676 ( 26%)], Train Loss: 0.68784\n",
      "Epoch: 00 [ 6684/25676 ( 26%)], Train Loss: 0.68653\n",
      "Epoch: 00 [ 6724/25676 ( 26%)], Train Loss: 0.68514\n",
      "Epoch: 00 [ 6764/25676 ( 26%)], Train Loss: 0.68354\n",
      "Epoch: 00 [ 6804/25676 ( 26%)], Train Loss: 0.68239\n",
      "Epoch: 00 [ 6844/25676 ( 27%)], Train Loss: 0.68010\n",
      "Epoch: 00 [ 6884/25676 ( 27%)], Train Loss: 0.67950\n",
      "Epoch: 00 [ 6924/25676 ( 27%)], Train Loss: 0.67712\n",
      "Epoch: 00 [ 6964/25676 ( 27%)], Train Loss: 0.67572\n",
      "Epoch: 00 [ 7004/25676 ( 27%)], Train Loss: 0.67448\n",
      "Epoch: 00 [ 7044/25676 ( 27%)], Train Loss: 0.67307\n",
      "Epoch: 00 [ 7084/25676 ( 28%)], Train Loss: 0.67214\n",
      "Epoch: 00 [ 7124/25676 ( 28%)], Train Loss: 0.67082\n",
      "Epoch: 00 [ 7164/25676 ( 28%)], Train Loss: 0.66978\n",
      "Epoch: 00 [ 7204/25676 ( 28%)], Train Loss: 0.66842\n",
      "Epoch: 00 [ 7244/25676 ( 28%)], Train Loss: 0.66717\n",
      "Epoch: 00 [ 7284/25676 ( 28%)], Train Loss: 0.66628\n",
      "Epoch: 00 [ 7324/25676 ( 29%)], Train Loss: 0.66546\n",
      "Epoch: 00 [ 7364/25676 ( 29%)], Train Loss: 0.66400\n",
      "Epoch: 00 [ 7404/25676 ( 29%)], Train Loss: 0.66211\n",
      "Epoch: 00 [ 7444/25676 ( 29%)], Train Loss: 0.66097\n",
      "Epoch: 00 [ 7484/25676 ( 29%)], Train Loss: 0.65855\n",
      "Epoch: 00 [ 7524/25676 ( 29%)], Train Loss: 0.65728\n",
      "Epoch: 00 [ 7564/25676 ( 29%)], Train Loss: 0.65735\n",
      "Epoch: 00 [ 7604/25676 ( 30%)], Train Loss: 0.65724\n",
      "Epoch: 00 [ 7644/25676 ( 30%)], Train Loss: 0.65701\n",
      "Epoch: 00 [ 7684/25676 ( 30%)], Train Loss: 0.65584\n",
      "Epoch: 00 [ 7724/25676 ( 30%)], Train Loss: 0.65478\n",
      "Epoch: 00 [ 7764/25676 ( 30%)], Train Loss: 0.65362\n",
      "Epoch: 00 [ 7804/25676 ( 30%)], Train Loss: 0.65197\n",
      "Epoch: 00 [ 7844/25676 ( 31%)], Train Loss: 0.65139\n",
      "Epoch: 00 [ 7884/25676 ( 31%)], Train Loss: 0.65044\n",
      "Epoch: 00 [ 7924/25676 ( 31%)], Train Loss: 0.64864\n",
      "Epoch: 00 [ 7964/25676 ( 31%)], Train Loss: 0.64804\n",
      "Epoch: 00 [ 8004/25676 ( 31%)], Train Loss: 0.64719\n",
      "Epoch: 00 [ 8044/25676 ( 31%)], Train Loss: 0.64583\n",
      "Epoch: 00 [ 8084/25676 ( 31%)], Train Loss: 0.64559\n",
      "Epoch: 00 [ 8124/25676 ( 32%)], Train Loss: 0.64536\n",
      "Epoch: 00 [ 8164/25676 ( 32%)], Train Loss: 0.64420\n",
      "Epoch: 00 [ 8204/25676 ( 32%)], Train Loss: 0.64362\n",
      "Epoch: 00 [ 8244/25676 ( 32%)], Train Loss: 0.64363\n",
      "Epoch: 00 [ 8284/25676 ( 32%)], Train Loss: 0.64382\n",
      "Epoch: 00 [ 8324/25676 ( 32%)], Train Loss: 0.64280\n",
      "Epoch: 00 [ 8364/25676 ( 33%)], Train Loss: 0.64172\n",
      "Epoch: 00 [ 8404/25676 ( 33%)], Train Loss: 0.64100\n",
      "Epoch: 00 [ 8444/25676 ( 33%)], Train Loss: 0.64091\n",
      "Epoch: 00 [ 8484/25676 ( 33%)], Train Loss: 0.63984\n",
      "Epoch: 00 [ 8524/25676 ( 33%)], Train Loss: 0.63876\n",
      "Epoch: 00 [ 8564/25676 ( 33%)], Train Loss: 0.63779\n",
      "Epoch: 00 [ 8604/25676 ( 34%)], Train Loss: 0.63660\n",
      "Epoch: 00 [ 8644/25676 ( 34%)], Train Loss: 0.63565\n",
      "Epoch: 00 [ 8684/25676 ( 34%)], Train Loss: 0.63619\n",
      "Epoch: 00 [ 8724/25676 ( 34%)], Train Loss: 0.63596\n",
      "Epoch: 00 [ 8764/25676 ( 34%)], Train Loss: 0.63499\n",
      "Epoch: 00 [ 8804/25676 ( 34%)], Train Loss: 0.63453\n",
      "Epoch: 00 [ 8844/25676 ( 34%)], Train Loss: 0.63364\n",
      "Epoch: 00 [ 8884/25676 ( 35%)], Train Loss: 0.63206\n",
      "Epoch: 00 [ 8924/25676 ( 35%)], Train Loss: 0.63274\n",
      "Epoch: 00 [ 8964/25676 ( 35%)], Train Loss: 0.63257\n",
      "Epoch: 00 [ 9004/25676 ( 35%)], Train Loss: 0.63227\n",
      "Epoch: 00 [ 9044/25676 ( 35%)], Train Loss: 0.63151\n",
      "Epoch: 00 [ 9084/25676 ( 35%)], Train Loss: 0.63025\n",
      "Epoch: 00 [ 9124/25676 ( 36%)], Train Loss: 0.62978\n",
      "Epoch: 00 [ 9164/25676 ( 36%)], Train Loss: 0.62957\n",
      "Epoch: 00 [ 9204/25676 ( 36%)], Train Loss: 0.62899\n",
      "Epoch: 00 [ 9244/25676 ( 36%)], Train Loss: 0.62875\n",
      "Epoch: 00 [ 9284/25676 ( 36%)], Train Loss: 0.62805\n",
      "Epoch: 00 [ 9324/25676 ( 36%)], Train Loss: 0.62812\n",
      "Epoch: 00 [ 9364/25676 ( 36%)], Train Loss: 0.62861\n",
      "Epoch: 00 [ 9404/25676 ( 37%)], Train Loss: 0.62798\n",
      "Epoch: 00 [ 9444/25676 ( 37%)], Train Loss: 0.62712\n",
      "Epoch: 00 [ 9484/25676 ( 37%)], Train Loss: 0.62634\n",
      "Epoch: 00 [ 9524/25676 ( 37%)], Train Loss: 0.62566\n",
      "Epoch: 00 [ 9564/25676 ( 37%)], Train Loss: 0.62470\n",
      "Epoch: 00 [ 9604/25676 ( 37%)], Train Loss: 0.62466\n",
      "Epoch: 00 [ 9644/25676 ( 38%)], Train Loss: 0.62396\n",
      "Epoch: 00 [ 9684/25676 ( 38%)], Train Loss: 0.62353\n",
      "Epoch: 00 [ 9724/25676 ( 38%)], Train Loss: 0.62295\n",
      "Epoch: 00 [ 9764/25676 ( 38%)], Train Loss: 0.62243\n",
      "Epoch: 00 [ 9804/25676 ( 38%)], Train Loss: 0.62165\n",
      "Epoch: 00 [ 9844/25676 ( 38%)], Train Loss: 0.62089\n",
      "Epoch: 00 [ 9884/25676 ( 38%)], Train Loss: 0.62048\n",
      "Epoch: 00 [ 9924/25676 ( 39%)], Train Loss: 0.62042\n",
      "Epoch: 00 [ 9964/25676 ( 39%)], Train Loss: 0.61884\n",
      "Epoch: 00 [10004/25676 ( 39%)], Train Loss: 0.61836\n",
      "Epoch: 00 [10044/25676 ( 39%)], Train Loss: 0.61786\n",
      "Epoch: 00 [10084/25676 ( 39%)], Train Loss: 0.61733\n",
      "Epoch: 00 [10124/25676 ( 39%)], Train Loss: 0.61678\n",
      "Epoch: 00 [10164/25676 ( 40%)], Train Loss: 0.61629\n",
      "Epoch: 00 [10204/25676 ( 40%)], Train Loss: 0.61564\n",
      "Epoch: 00 [10244/25676 ( 40%)], Train Loss: 0.61463\n",
      "Epoch: 00 [10284/25676 ( 40%)], Train Loss: 0.61404\n",
      "Epoch: 00 [10324/25676 ( 40%)], Train Loss: 0.61367\n",
      "Epoch: 00 [10364/25676 ( 40%)], Train Loss: 0.61314\n",
      "Epoch: 00 [10404/25676 ( 41%)], Train Loss: 0.61212\n",
      "Epoch: 00 [10444/25676 ( 41%)], Train Loss: 0.61115\n",
      "Epoch: 00 [10484/25676 ( 41%)], Train Loss: 0.61039\n",
      "Epoch: 00 [10524/25676 ( 41%)], Train Loss: 0.60863\n",
      "Epoch: 00 [10564/25676 ( 41%)], Train Loss: 0.60799\n",
      "Epoch: 00 [10604/25676 ( 41%)], Train Loss: 0.60759\n",
      "Epoch: 00 [10644/25676 ( 41%)], Train Loss: 0.60716\n",
      "Epoch: 00 [10684/25676 ( 42%)], Train Loss: 0.60715\n",
      "Epoch: 00 [10724/25676 ( 42%)], Train Loss: 0.60656\n",
      "Epoch: 00 [10764/25676 ( 42%)], Train Loss: 0.60646\n",
      "Epoch: 00 [10804/25676 ( 42%)], Train Loss: 0.60656\n",
      "Epoch: 00 [10844/25676 ( 42%)], Train Loss: 0.60545\n",
      "Epoch: 00 [10884/25676 ( 42%)], Train Loss: 0.60500\n",
      "Epoch: 00 [10924/25676 ( 43%)], Train Loss: 0.60419\n",
      "Epoch: 00 [10964/25676 ( 43%)], Train Loss: 0.60316\n",
      "Epoch: 00 [11004/25676 ( 43%)], Train Loss: 0.60251\n",
      "Epoch: 00 [11044/25676 ( 43%)], Train Loss: 0.60249\n",
      "Epoch: 00 [11084/25676 ( 43%)], Train Loss: 0.60202\n",
      "Epoch: 00 [11124/25676 ( 43%)], Train Loss: 0.60149\n",
      "Epoch: 00 [11164/25676 ( 43%)], Train Loss: 0.60026\n",
      "Epoch: 00 [11204/25676 ( 44%)], Train Loss: 0.59964\n",
      "Epoch: 00 [11244/25676 ( 44%)], Train Loss: 0.59926\n",
      "Epoch: 00 [11284/25676 ( 44%)], Train Loss: 0.59877\n",
      "Epoch: 00 [11324/25676 ( 44%)], Train Loss: 0.59771\n",
      "Epoch: 00 [11364/25676 ( 44%)], Train Loss: 0.59710\n",
      "Epoch: 00 [11404/25676 ( 44%)], Train Loss: 0.59589\n",
      "Epoch: 00 [11444/25676 ( 45%)], Train Loss: 0.59580\n",
      "Epoch: 00 [11484/25676 ( 45%)], Train Loss: 0.59536\n",
      "Epoch: 00 [11524/25676 ( 45%)], Train Loss: 0.59533\n",
      "Epoch: 00 [11564/25676 ( 45%)], Train Loss: 0.59489\n",
      "Epoch: 00 [11604/25676 ( 45%)], Train Loss: 0.59395\n",
      "Epoch: 00 [11644/25676 ( 45%)], Train Loss: 0.59354\n",
      "Epoch: 00 [11684/25676 ( 46%)], Train Loss: 0.59281\n",
      "Epoch: 00 [11724/25676 ( 46%)], Train Loss: 0.59240\n",
      "Epoch: 00 [11764/25676 ( 46%)], Train Loss: 0.59172\n",
      "Epoch: 00 [11804/25676 ( 46%)], Train Loss: 0.59117\n",
      "Epoch: 00 [11844/25676 ( 46%)], Train Loss: 0.59098\n",
      "Epoch: 00 [11884/25676 ( 46%)], Train Loss: 0.59022\n",
      "Epoch: 00 [11924/25676 ( 46%)], Train Loss: 0.59047\n",
      "Epoch: 00 [11964/25676 ( 47%)], Train Loss: 0.59093\n",
      "Epoch: 00 [12004/25676 ( 47%)], Train Loss: 0.59041\n",
      "Epoch: 00 [12044/25676 ( 47%)], Train Loss: 0.58978\n",
      "Epoch: 00 [12084/25676 ( 47%)], Train Loss: 0.58889\n",
      "Epoch: 00 [12124/25676 ( 47%)], Train Loss: 0.58860\n",
      "Epoch: 00 [12164/25676 ( 47%)], Train Loss: 0.58791\n",
      "Epoch: 00 [12204/25676 ( 48%)], Train Loss: 0.58678\n",
      "Epoch: 00 [12244/25676 ( 48%)], Train Loss: 0.58617\n",
      "Epoch: 00 [12284/25676 ( 48%)], Train Loss: 0.58513\n",
      "Epoch: 00 [12324/25676 ( 48%)], Train Loss: 0.58429\n",
      "Epoch: 00 [12364/25676 ( 48%)], Train Loss: 0.58355\n",
      "Epoch: 00 [12404/25676 ( 48%)], Train Loss: 0.58300\n",
      "Epoch: 00 [12444/25676 ( 48%)], Train Loss: 0.58212\n",
      "Epoch: 00 [12484/25676 ( 49%)], Train Loss: 0.58142\n",
      "Epoch: 00 [12524/25676 ( 49%)], Train Loss: 0.58069\n",
      "Epoch: 00 [12564/25676 ( 49%)], Train Loss: 0.58014\n",
      "Epoch: 00 [12604/25676 ( 49%)], Train Loss: 0.57937\n",
      "Epoch: 00 [12644/25676 ( 49%)], Train Loss: 0.57972\n",
      "Epoch: 00 [12684/25676 ( 49%)], Train Loss: 0.57957\n",
      "Epoch: 00 [12724/25676 ( 50%)], Train Loss: 0.57901\n",
      "Epoch: 00 [12764/25676 ( 50%)], Train Loss: 0.57825\n",
      "Epoch: 00 [12804/25676 ( 50%)], Train Loss: 0.57764\n",
      "Epoch: 00 [12844/25676 ( 50%)], Train Loss: 0.57668\n",
      "Epoch: 00 [12884/25676 ( 50%)], Train Loss: 0.57607\n",
      "Epoch: 00 [12924/25676 ( 50%)], Train Loss: 0.57517\n",
      "Epoch: 00 [12964/25676 ( 50%)], Train Loss: 0.57472\n",
      "Epoch: 00 [13004/25676 ( 51%)], Train Loss: 0.57440\n",
      "Epoch: 00 [13044/25676 ( 51%)], Train Loss: 0.57364\n",
      "Epoch: 00 [13084/25676 ( 51%)], Train Loss: 0.57307\n",
      "Epoch: 00 [13124/25676 ( 51%)], Train Loss: 0.57299\n",
      "Epoch: 00 [13164/25676 ( 51%)], Train Loss: 0.57213\n",
      "Epoch: 00 [13204/25676 ( 51%)], Train Loss: 0.57279\n",
      "Epoch: 00 [13244/25676 ( 52%)], Train Loss: 0.57209\n",
      "Epoch: 00 [13284/25676 ( 52%)], Train Loss: 0.57196\n",
      "Epoch: 00 [13324/25676 ( 52%)], Train Loss: 0.57127\n",
      "Epoch: 00 [13364/25676 ( 52%)], Train Loss: 0.57086\n",
      "Epoch: 00 [13404/25676 ( 52%)], Train Loss: 0.56980\n",
      "Epoch: 00 [13444/25676 ( 52%)], Train Loss: 0.56933\n",
      "Epoch: 00 [13484/25676 ( 53%)], Train Loss: 0.56834\n",
      "Epoch: 00 [13524/25676 ( 53%)], Train Loss: 0.56798\n",
      "Epoch: 00 [13564/25676 ( 53%)], Train Loss: 0.56798\n",
      "Epoch: 00 [13604/25676 ( 53%)], Train Loss: 0.56747\n",
      "Epoch: 00 [13644/25676 ( 53%)], Train Loss: 0.56762\n",
      "Epoch: 00 [13684/25676 ( 53%)], Train Loss: 0.56714\n",
      "Epoch: 00 [13724/25676 ( 53%)], Train Loss: 0.56685\n",
      "Epoch: 00 [13764/25676 ( 54%)], Train Loss: 0.56594\n",
      "Epoch: 00 [13804/25676 ( 54%)], Train Loss: 0.56525\n",
      "Epoch: 00 [13844/25676 ( 54%)], Train Loss: 0.56464\n",
      "Epoch: 00 [13884/25676 ( 54%)], Train Loss: 0.56429\n",
      "Epoch: 00 [13924/25676 ( 54%)], Train Loss: 0.56361\n",
      "Epoch: 00 [13964/25676 ( 54%)], Train Loss: 0.56285\n",
      "Epoch: 00 [14004/25676 ( 55%)], Train Loss: 0.56211\n",
      "Epoch: 00 [14044/25676 ( 55%)], Train Loss: 0.56162\n",
      "Epoch: 00 [14084/25676 ( 55%)], Train Loss: 0.56147\n",
      "Epoch: 00 [14124/25676 ( 55%)], Train Loss: 0.56107\n",
      "Epoch: 00 [14164/25676 ( 55%)], Train Loss: 0.56112\n",
      "Epoch: 00 [14204/25676 ( 55%)], Train Loss: 0.56104\n",
      "Epoch: 00 [14244/25676 ( 55%)], Train Loss: 0.56028\n",
      "Epoch: 00 [14284/25676 ( 56%)], Train Loss: 0.55942\n",
      "Epoch: 00 [14324/25676 ( 56%)], Train Loss: 0.55877\n",
      "Epoch: 00 [14364/25676 ( 56%)], Train Loss: 0.55840\n",
      "Epoch: 00 [14404/25676 ( 56%)], Train Loss: 0.55840\n",
      "Epoch: 00 [14444/25676 ( 56%)], Train Loss: 0.55804\n",
      "Epoch: 00 [14484/25676 ( 56%)], Train Loss: 0.55786\n",
      "Epoch: 00 [14524/25676 ( 57%)], Train Loss: 0.55800\n",
      "Epoch: 00 [14564/25676 ( 57%)], Train Loss: 0.55771\n",
      "Epoch: 00 [14604/25676 ( 57%)], Train Loss: 0.55745\n",
      "Epoch: 00 [14644/25676 ( 57%)], Train Loss: 0.55751\n",
      "Epoch: 00 [14684/25676 ( 57%)], Train Loss: 0.55709\n",
      "Epoch: 00 [14724/25676 ( 57%)], Train Loss: 0.55719\n",
      "Epoch: 00 [14764/25676 ( 58%)], Train Loss: 0.55731\n",
      "Epoch: 00 [14804/25676 ( 58%)], Train Loss: 0.55653\n",
      "Epoch: 00 [14844/25676 ( 58%)], Train Loss: 0.55608\n",
      "Epoch: 00 [14884/25676 ( 58%)], Train Loss: 0.55533\n",
      "Epoch: 00 [14924/25676 ( 58%)], Train Loss: 0.55476\n",
      "Epoch: 00 [14964/25676 ( 58%)], Train Loss: 0.55413\n",
      "Epoch: 00 [15004/25676 ( 58%)], Train Loss: 0.55333\n",
      "Epoch: 00 [15044/25676 ( 59%)], Train Loss: 0.55320\n",
      "Epoch: 00 [15084/25676 ( 59%)], Train Loss: 0.55283\n",
      "Epoch: 00 [15124/25676 ( 59%)], Train Loss: 0.55217\n",
      "Epoch: 00 [15164/25676 ( 59%)], Train Loss: 0.55211\n",
      "Epoch: 00 [15204/25676 ( 59%)], Train Loss: 0.55158\n",
      "Epoch: 00 [15244/25676 ( 59%)], Train Loss: 0.55137\n",
      "Epoch: 00 [15284/25676 ( 60%)], Train Loss: 0.55087\n",
      "Epoch: 00 [15324/25676 ( 60%)], Train Loss: 0.55011\n",
      "Epoch: 00 [15364/25676 ( 60%)], Train Loss: 0.54893\n",
      "Epoch: 00 [15404/25676 ( 60%)], Train Loss: 0.54866\n",
      "Epoch: 00 [15444/25676 ( 60%)], Train Loss: 0.54843\n",
      "Epoch: 00 [15484/25676 ( 60%)], Train Loss: 0.54858\n",
      "Epoch: 00 [15524/25676 ( 60%)], Train Loss: 0.54803\n",
      "Epoch: 00 [15564/25676 ( 61%)], Train Loss: 0.54757\n",
      "Epoch: 00 [15604/25676 ( 61%)], Train Loss: 0.54683\n",
      "Epoch: 00 [15644/25676 ( 61%)], Train Loss: 0.54601\n",
      "Epoch: 00 [15684/25676 ( 61%)], Train Loss: 0.54592\n",
      "Epoch: 00 [15724/25676 ( 61%)], Train Loss: 0.54546\n",
      "Epoch: 00 [15764/25676 ( 61%)], Train Loss: 0.54517\n",
      "Epoch: 00 [15804/25676 ( 62%)], Train Loss: 0.54473\n",
      "Epoch: 00 [15844/25676 ( 62%)], Train Loss: 0.54459\n",
      "Epoch: 00 [15884/25676 ( 62%)], Train Loss: 0.54422\n",
      "Epoch: 00 [15924/25676 ( 62%)], Train Loss: 0.54463\n",
      "Epoch: 00 [15964/25676 ( 62%)], Train Loss: 0.54420\n",
      "Epoch: 00 [16004/25676 ( 62%)], Train Loss: 0.54395\n",
      "Epoch: 00 [16044/25676 ( 62%)], Train Loss: 0.54337\n",
      "Epoch: 00 [16084/25676 ( 63%)], Train Loss: 0.54280\n",
      "Epoch: 00 [16124/25676 ( 63%)], Train Loss: 0.54219\n",
      "Epoch: 00 [16164/25676 ( 63%)], Train Loss: 0.54203\n",
      "Epoch: 00 [16204/25676 ( 63%)], Train Loss: 0.54230\n",
      "Epoch: 00 [16244/25676 ( 63%)], Train Loss: 0.54188\n",
      "Epoch: 00 [16284/25676 ( 63%)], Train Loss: 0.54149\n",
      "Epoch: 00 [16324/25676 ( 64%)], Train Loss: 0.54144\n",
      "Epoch: 00 [16364/25676 ( 64%)], Train Loss: 0.54118\n",
      "Epoch: 00 [16404/25676 ( 64%)], Train Loss: 0.54099\n",
      "Epoch: 00 [16444/25676 ( 64%)], Train Loss: 0.54079\n",
      "Epoch: 00 [16484/25676 ( 64%)], Train Loss: 0.54039\n",
      "Epoch: 00 [16524/25676 ( 64%)], Train Loss: 0.53941\n",
      "Epoch: 00 [16564/25676 ( 65%)], Train Loss: 0.53895\n",
      "Epoch: 00 [16604/25676 ( 65%)], Train Loss: 0.53901\n",
      "Epoch: 00 [16644/25676 ( 65%)], Train Loss: 0.53897\n",
      "Epoch: 00 [16684/25676 ( 65%)], Train Loss: 0.53827\n",
      "Epoch: 00 [16724/25676 ( 65%)], Train Loss: 0.53830\n",
      "Epoch: 00 [16764/25676 ( 65%)], Train Loss: 0.53817\n",
      "Epoch: 00 [16804/25676 ( 65%)], Train Loss: 0.53795\n",
      "Epoch: 00 [16844/25676 ( 66%)], Train Loss: 0.53735\n",
      "Epoch: 00 [16884/25676 ( 66%)], Train Loss: 0.53717\n",
      "Epoch: 00 [16924/25676 ( 66%)], Train Loss: 0.53632\n",
      "Epoch: 00 [16964/25676 ( 66%)], Train Loss: 0.53648\n",
      "Epoch: 00 [17004/25676 ( 66%)], Train Loss: 0.53621\n",
      "Epoch: 00 [17044/25676 ( 66%)], Train Loss: 0.53572\n",
      "Epoch: 00 [17084/25676 ( 67%)], Train Loss: 0.53556\n",
      "Epoch: 00 [17124/25676 ( 67%)], Train Loss: 0.53534\n",
      "Epoch: 00 [17164/25676 ( 67%)], Train Loss: 0.53479\n",
      "Epoch: 00 [17204/25676 ( 67%)], Train Loss: 0.53423\n",
      "Epoch: 00 [17244/25676 ( 67%)], Train Loss: 0.53429\n",
      "Epoch: 00 [17284/25676 ( 67%)], Train Loss: 0.53367\n",
      "Epoch: 00 [17324/25676 ( 67%)], Train Loss: 0.53318\n",
      "Epoch: 00 [17364/25676 ( 68%)], Train Loss: 0.53344\n",
      "Epoch: 00 [17404/25676 ( 68%)], Train Loss: 0.53281\n",
      "Epoch: 00 [17444/25676 ( 68%)], Train Loss: 0.53232\n",
      "Epoch: 00 [17484/25676 ( 68%)], Train Loss: 0.53215\n",
      "Epoch: 00 [17524/25676 ( 68%)], Train Loss: 0.53205\n",
      "Epoch: 00 [17564/25676 ( 68%)], Train Loss: 0.53212\n",
      "Epoch: 00 [17604/25676 ( 69%)], Train Loss: 0.53210\n",
      "Epoch: 00 [17644/25676 ( 69%)], Train Loss: 0.53154\n",
      "Epoch: 00 [17684/25676 ( 69%)], Train Loss: 0.53127\n",
      "Epoch: 00 [17724/25676 ( 69%)], Train Loss: 0.53083\n",
      "Epoch: 00 [17764/25676 ( 69%)], Train Loss: 0.53028\n",
      "Epoch: 00 [17804/25676 ( 69%)], Train Loss: 0.52989\n",
      "Epoch: 00 [17844/25676 ( 69%)], Train Loss: 0.52944\n",
      "Epoch: 00 [17884/25676 ( 70%)], Train Loss: 0.52889\n",
      "Epoch: 00 [17924/25676 ( 70%)], Train Loss: 0.52853\n",
      "Epoch: 00 [17964/25676 ( 70%)], Train Loss: 0.52867\n",
      "Epoch: 00 [18004/25676 ( 70%)], Train Loss: 0.52837\n",
      "Epoch: 00 [18044/25676 ( 70%)], Train Loss: 0.52848\n",
      "Epoch: 00 [18084/25676 ( 70%)], Train Loss: 0.52809\n",
      "Epoch: 00 [18124/25676 ( 71%)], Train Loss: 0.52754\n",
      "Epoch: 00 [18164/25676 ( 71%)], Train Loss: 0.52726\n",
      "Epoch: 00 [18204/25676 ( 71%)], Train Loss: 0.52661\n",
      "Epoch: 00 [18244/25676 ( 71%)], Train Loss: 0.52588\n",
      "Epoch: 00 [18284/25676 ( 71%)], Train Loss: 0.52619\n",
      "Epoch: 00 [18324/25676 ( 71%)], Train Loss: 0.52596\n",
      "Epoch: 00 [18364/25676 ( 72%)], Train Loss: 0.52569\n",
      "Epoch: 00 [18404/25676 ( 72%)], Train Loss: 0.52549\n",
      "Epoch: 00 [18444/25676 ( 72%)], Train Loss: 0.52517\n",
      "Epoch: 00 [18484/25676 ( 72%)], Train Loss: 0.52473\n",
      "Epoch: 00 [18524/25676 ( 72%)], Train Loss: 0.52423\n",
      "Epoch: 00 [18564/25676 ( 72%)], Train Loss: 0.52452\n",
      "Epoch: 00 [18604/25676 ( 72%)], Train Loss: 0.52415\n",
      "Epoch: 00 [18644/25676 ( 73%)], Train Loss: 0.52369\n",
      "Epoch: 00 [18684/25676 ( 73%)], Train Loss: 0.52357\n",
      "Epoch: 00 [18724/25676 ( 73%)], Train Loss: 0.52331\n",
      "Epoch: 00 [18764/25676 ( 73%)], Train Loss: 0.52297\n",
      "Epoch: 00 [18804/25676 ( 73%)], Train Loss: 0.52268\n",
      "Epoch: 00 [18844/25676 ( 73%)], Train Loss: 0.52221\n",
      "Epoch: 00 [18884/25676 ( 74%)], Train Loss: 0.52195\n",
      "Epoch: 00 [18924/25676 ( 74%)], Train Loss: 0.52150\n",
      "Epoch: 00 [18964/25676 ( 74%)], Train Loss: 0.52094\n",
      "Epoch: 00 [19004/25676 ( 74%)], Train Loss: 0.52040\n",
      "Epoch: 00 [19044/25676 ( 74%)], Train Loss: 0.52026\n",
      "Epoch: 00 [19084/25676 ( 74%)], Train Loss: 0.52002\n",
      "Epoch: 00 [19124/25676 ( 74%)], Train Loss: 0.51958\n",
      "Epoch: 00 [19164/25676 ( 75%)], Train Loss: 0.51940\n",
      "Epoch: 00 [19204/25676 ( 75%)], Train Loss: 0.51891\n",
      "Epoch: 00 [19244/25676 ( 75%)], Train Loss: 0.51890\n",
      "Epoch: 00 [19284/25676 ( 75%)], Train Loss: 0.51838\n",
      "Epoch: 00 [19324/25676 ( 75%)], Train Loss: 0.51825\n",
      "Epoch: 00 [19364/25676 ( 75%)], Train Loss: 0.51777\n",
      "Epoch: 00 [19404/25676 ( 76%)], Train Loss: 0.51725\n",
      "Epoch: 00 [19444/25676 ( 76%)], Train Loss: 0.51680\n",
      "Epoch: 00 [19484/25676 ( 76%)], Train Loss: 0.51660\n",
      "Epoch: 00 [19524/25676 ( 76%)], Train Loss: 0.51624\n",
      "Epoch: 00 [19564/25676 ( 76%)], Train Loss: 0.51604\n",
      "Epoch: 00 [19604/25676 ( 76%)], Train Loss: 0.51593\n",
      "Epoch: 00 [19644/25676 ( 77%)], Train Loss: 0.51559\n",
      "Epoch: 00 [19684/25676 ( 77%)], Train Loss: 0.51537\n",
      "Epoch: 00 [19724/25676 ( 77%)], Train Loss: 0.51520\n",
      "Epoch: 00 [19764/25676 ( 77%)], Train Loss: 0.51491\n",
      "Epoch: 00 [19804/25676 ( 77%)], Train Loss: 0.51449\n",
      "Epoch: 00 [19844/25676 ( 77%)], Train Loss: 0.51412\n",
      "Epoch: 00 [19884/25676 ( 77%)], Train Loss: 0.51371\n",
      "Epoch: 00 [19924/25676 ( 78%)], Train Loss: 0.51321\n",
      "Epoch: 00 [19964/25676 ( 78%)], Train Loss: 0.51297\n",
      "Epoch: 00 [20004/25676 ( 78%)], Train Loss: 0.51254\n",
      "Epoch: 00 [20044/25676 ( 78%)], Train Loss: 0.51216\n",
      "Epoch: 00 [20084/25676 ( 78%)], Train Loss: 0.51208\n",
      "Epoch: 00 [20124/25676 ( 78%)], Train Loss: 0.51210\n",
      "Epoch: 00 [20164/25676 ( 79%)], Train Loss: 0.51168\n",
      "Epoch: 00 [20204/25676 ( 79%)], Train Loss: 0.51108\n",
      "Epoch: 00 [20244/25676 ( 79%)], Train Loss: 0.51099\n",
      "Epoch: 00 [20284/25676 ( 79%)], Train Loss: 0.51061\n",
      "Epoch: 00 [20324/25676 ( 79%)], Train Loss: 0.51057\n",
      "Epoch: 00 [20364/25676 ( 79%)], Train Loss: 0.51040\n",
      "Epoch: 00 [20404/25676 ( 79%)], Train Loss: 0.51028\n",
      "Epoch: 00 [20444/25676 ( 80%)], Train Loss: 0.51002\n",
      "Epoch: 00 [20484/25676 ( 80%)], Train Loss: 0.50965\n",
      "Epoch: 00 [20524/25676 ( 80%)], Train Loss: 0.50976\n",
      "Epoch: 00 [20564/25676 ( 80%)], Train Loss: 0.50955\n",
      "Epoch: 00 [20604/25676 ( 80%)], Train Loss: 0.50930\n",
      "Epoch: 00 [20644/25676 ( 80%)], Train Loss: 0.50899\n",
      "Epoch: 00 [20684/25676 ( 81%)], Train Loss: 0.50882\n",
      "Epoch: 00 [20724/25676 ( 81%)], Train Loss: 0.50871\n",
      "Epoch: 00 [20764/25676 ( 81%)], Train Loss: 0.50858\n",
      "Epoch: 00 [20804/25676 ( 81%)], Train Loss: 0.50907\n",
      "Epoch: 00 [20844/25676 ( 81%)], Train Loss: 0.50876\n",
      "Epoch: 00 [20884/25676 ( 81%)], Train Loss: 0.50848\n",
      "Epoch: 00 [20924/25676 ( 81%)], Train Loss: 0.50855\n",
      "Epoch: 00 [20964/25676 ( 82%)], Train Loss: 0.50829\n",
      "Epoch: 00 [21004/25676 ( 82%)], Train Loss: 0.50797\n",
      "Epoch: 00 [21044/25676 ( 82%)], Train Loss: 0.50774\n",
      "Epoch: 00 [21084/25676 ( 82%)], Train Loss: 0.50734\n",
      "Epoch: 00 [21124/25676 ( 82%)], Train Loss: 0.50696\n",
      "Epoch: 00 [21164/25676 ( 82%)], Train Loss: 0.50720\n",
      "Epoch: 00 [21204/25676 ( 83%)], Train Loss: 0.50723\n",
      "Epoch: 00 [21244/25676 ( 83%)], Train Loss: 0.50710\n",
      "Epoch: 00 [21284/25676 ( 83%)], Train Loss: 0.50690\n",
      "Epoch: 00 [21324/25676 ( 83%)], Train Loss: 0.50648\n",
      "Epoch: 00 [21364/25676 ( 83%)], Train Loss: 0.50670\n",
      "Epoch: 00 [21404/25676 ( 83%)], Train Loss: 0.50645\n",
      "Epoch: 00 [21444/25676 ( 84%)], Train Loss: 0.50590\n",
      "Epoch: 00 [21484/25676 ( 84%)], Train Loss: 0.50576\n",
      "Epoch: 00 [21524/25676 ( 84%)], Train Loss: 0.50551\n",
      "Epoch: 00 [21564/25676 ( 84%)], Train Loss: 0.50512\n",
      "Epoch: 00 [21604/25676 ( 84%)], Train Loss: 0.50487\n",
      "Epoch: 00 [21644/25676 ( 84%)], Train Loss: 0.50487\n",
      "Epoch: 00 [21684/25676 ( 84%)], Train Loss: 0.50448\n",
      "Epoch: 00 [21724/25676 ( 85%)], Train Loss: 0.50409\n",
      "Epoch: 00 [21764/25676 ( 85%)], Train Loss: 0.50378\n",
      "Epoch: 00 [21804/25676 ( 85%)], Train Loss: 0.50338\n",
      "Epoch: 00 [21844/25676 ( 85%)], Train Loss: 0.50322\n",
      "Epoch: 00 [21884/25676 ( 85%)], Train Loss: 0.50293\n",
      "Epoch: 00 [21924/25676 ( 85%)], Train Loss: 0.50254\n",
      "Epoch: 00 [21964/25676 ( 86%)], Train Loss: 0.50252\n",
      "Epoch: 00 [22004/25676 ( 86%)], Train Loss: 0.50218\n",
      "Epoch: 00 [22044/25676 ( 86%)], Train Loss: 0.50191\n",
      "Epoch: 00 [22084/25676 ( 86%)], Train Loss: 0.50166\n",
      "Epoch: 00 [22124/25676 ( 86%)], Train Loss: 0.50175\n",
      "Epoch: 00 [22164/25676 ( 86%)], Train Loss: 0.50146\n",
      "Epoch: 00 [22204/25676 ( 86%)], Train Loss: 0.50113\n",
      "Epoch: 00 [22244/25676 ( 87%)], Train Loss: 0.50074\n",
      "Epoch: 00 [22284/25676 ( 87%)], Train Loss: 0.50093\n",
      "Epoch: 00 [22324/25676 ( 87%)], Train Loss: 0.50080\n",
      "Epoch: 00 [22364/25676 ( 87%)], Train Loss: 0.50067\n",
      "Epoch: 00 [22404/25676 ( 87%)], Train Loss: 0.50090\n",
      "Epoch: 00 [22444/25676 ( 87%)], Train Loss: 0.50034\n",
      "Epoch: 00 [22484/25676 ( 88%)], Train Loss: 0.50006\n",
      "Epoch: 00 [22524/25676 ( 88%)], Train Loss: 0.49982\n",
      "Epoch: 00 [22564/25676 ( 88%)], Train Loss: 0.49944\n",
      "Epoch: 00 [22604/25676 ( 88%)], Train Loss: 0.49919\n",
      "Epoch: 00 [22644/25676 ( 88%)], Train Loss: 0.49874\n",
      "Epoch: 00 [22684/25676 ( 88%)], Train Loss: 0.49835\n",
      "Epoch: 00 [22724/25676 ( 89%)], Train Loss: 0.49823\n",
      "Epoch: 00 [22764/25676 ( 89%)], Train Loss: 0.49807\n",
      "Epoch: 00 [22804/25676 ( 89%)], Train Loss: 0.49802\n",
      "Epoch: 00 [22844/25676 ( 89%)], Train Loss: 0.49785\n",
      "Epoch: 00 [22884/25676 ( 89%)], Train Loss: 0.49784\n",
      "Epoch: 00 [22924/25676 ( 89%)], Train Loss: 0.49748\n",
      "Epoch: 00 [22964/25676 ( 89%)], Train Loss: 0.49771\n",
      "Epoch: 00 [23004/25676 ( 90%)], Train Loss: 0.49762\n",
      "Epoch: 00 [23044/25676 ( 90%)], Train Loss: 0.49733\n",
      "Epoch: 00 [23084/25676 ( 90%)], Train Loss: 0.49741\n",
      "Epoch: 00 [23124/25676 ( 90%)], Train Loss: 0.49738\n",
      "Epoch: 00 [23164/25676 ( 90%)], Train Loss: 0.49760\n",
      "Epoch: 00 [23204/25676 ( 90%)], Train Loss: 0.49736\n",
      "Epoch: 00 [23244/25676 ( 91%)], Train Loss: 0.49720\n",
      "Epoch: 00 [23284/25676 ( 91%)], Train Loss: 0.49703\n",
      "Epoch: 00 [23324/25676 ( 91%)], Train Loss: 0.49663\n",
      "Epoch: 00 [23364/25676 ( 91%)], Train Loss: 0.49641\n",
      "Epoch: 00 [23404/25676 ( 91%)], Train Loss: 0.49611\n",
      "Epoch: 00 [23444/25676 ( 91%)], Train Loss: 0.49593\n",
      "Epoch: 00 [23484/25676 ( 91%)], Train Loss: 0.49591\n",
      "Epoch: 00 [23524/25676 ( 92%)], Train Loss: 0.49580\n",
      "Epoch: 00 [23564/25676 ( 92%)], Train Loss: 0.49589\n",
      "Epoch: 00 [23604/25676 ( 92%)], Train Loss: 0.49565\n",
      "Epoch: 00 [23644/25676 ( 92%)], Train Loss: 0.49523\n",
      "Epoch: 00 [23684/25676 ( 92%)], Train Loss: 0.49517\n",
      "Epoch: 00 [23724/25676 ( 92%)], Train Loss: 0.49497\n",
      "Epoch: 00 [23764/25676 ( 93%)], Train Loss: 0.49463\n",
      "Epoch: 00 [23804/25676 ( 93%)], Train Loss: 0.49455\n",
      "Epoch: 00 [23844/25676 ( 93%)], Train Loss: 0.49427\n",
      "Epoch: 00 [23884/25676 ( 93%)], Train Loss: 0.49393\n",
      "Epoch: 00 [23924/25676 ( 93%)], Train Loss: 0.49367\n",
      "Epoch: 00 [23964/25676 ( 93%)], Train Loss: 0.49343\n",
      "Epoch: 00 [24004/25676 ( 93%)], Train Loss: 0.49319\n",
      "Epoch: 00 [24044/25676 ( 94%)], Train Loss: 0.49296\n",
      "Epoch: 00 [24084/25676 ( 94%)], Train Loss: 0.49306\n",
      "Epoch: 00 [24124/25676 ( 94%)], Train Loss: 0.49283\n",
      "Epoch: 00 [24164/25676 ( 94%)], Train Loss: 0.49237\n",
      "Epoch: 00 [24204/25676 ( 94%)], Train Loss: 0.49191\n",
      "Epoch: 00 [24244/25676 ( 94%)], Train Loss: 0.49140\n",
      "Epoch: 00 [24284/25676 ( 95%)], Train Loss: 0.49114\n",
      "Epoch: 00 [24324/25676 ( 95%)], Train Loss: 0.49089\n",
      "Epoch: 00 [24364/25676 ( 95%)], Train Loss: 0.49128\n",
      "Epoch: 00 [24404/25676 ( 95%)], Train Loss: 0.49124\n",
      "Epoch: 00 [24444/25676 ( 95%)], Train Loss: 0.49097\n",
      "Epoch: 00 [24484/25676 ( 95%)], Train Loss: 0.49050\n",
      "Epoch: 00 [24524/25676 ( 96%)], Train Loss: 0.49049\n",
      "Epoch: 00 [24564/25676 ( 96%)], Train Loss: 0.49059\n",
      "Epoch: 00 [24604/25676 ( 96%)], Train Loss: 0.49036\n",
      "Epoch: 00 [24644/25676 ( 96%)], Train Loss: 0.49039\n",
      "Epoch: 00 [24684/25676 ( 96%)], Train Loss: 0.49034\n",
      "Epoch: 00 [24724/25676 ( 96%)], Train Loss: 0.49048\n",
      "Epoch: 00 [24764/25676 ( 96%)], Train Loss: 0.49026\n",
      "Epoch: 00 [24804/25676 ( 97%)], Train Loss: 0.48995\n",
      "Epoch: 00 [24844/25676 ( 97%)], Train Loss: 0.49000\n",
      "Epoch: 00 [24884/25676 ( 97%)], Train Loss: 0.48994\n",
      "Epoch: 00 [24924/25676 ( 97%)], Train Loss: 0.48976\n",
      "Epoch: 00 [24964/25676 ( 97%)], Train Loss: 0.48938\n",
      "Epoch: 00 [25004/25676 ( 97%)], Train Loss: 0.48889\n",
      "Epoch: 00 [25044/25676 ( 98%)], Train Loss: 0.48871\n",
      "Epoch: 00 [25084/25676 ( 98%)], Train Loss: 0.48853\n",
      "Epoch: 00 [25124/25676 ( 98%)], Train Loss: 0.48814\n",
      "Epoch: 00 [25164/25676 ( 98%)], Train Loss: 0.48790\n",
      "Epoch: 00 [25204/25676 ( 98%)], Train Loss: 0.48779\n",
      "Epoch: 00 [25244/25676 ( 98%)], Train Loss: 0.48766\n",
      "Epoch: 00 [25284/25676 ( 98%)], Train Loss: 0.48741\n",
      "Epoch: 00 [25324/25676 ( 99%)], Train Loss: 0.48721\n",
      "Epoch: 00 [25364/25676 ( 99%)], Train Loss: 0.48711\n",
      "Epoch: 00 [25404/25676 ( 99%)], Train Loss: 0.48689\n",
      "Epoch: 00 [25444/25676 ( 99%)], Train Loss: 0.48683\n",
      "Epoch: 00 [25484/25676 ( 99%)], Train Loss: 0.48636\n",
      "Epoch: 00 [25524/25676 ( 99%)], Train Loss: 0.48605\n",
      "Epoch: 00 [25564/25676 (100%)], Train Loss: 0.48607\n",
      "Epoch: 00 [25604/25676 (100%)], Train Loss: 0.48588\n",
      "Epoch: 00 [25644/25676 (100%)], Train Loss: 0.48568\n",
      "Epoch: 00 [25676/25676 (100%)], Train Loss: 0.48541\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] Valid Loss: 0.22268\n",
      "Post-processing 223 example predictions split into 2824 features.\n",
      "valid jaccard:  0.6857569933803119\n",
      "0 Epoch, Best epoch was updated! Valid Loss: 0.22268\n",
      "Saving model checkpoint to output/checkpoint-fold-2-epoch-0.\n",
      "\n",
      "Total Training Time: 3775.903554201126secs, Average Training Time per Epoch: 3775.903554201126secs.\n",
      "Total Validation Time: 134.03500247001648secs, Average Validation Time per Epoch: 134.03500247001648secs.\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "FOLD: 3\n",
      "--------------------------------------------------\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Num examples Train= 25556, Num examples Valid=2944\n",
      "Total Training Steps: 3195, Total Warmup Steps: 319\n",
      "Epoch: 00 [    4/25556 (  0%)], Train Loss: 3.22703\n",
      "Epoch: 00 [   44/25556 (  0%)], Train Loss: 3.23851\n",
      "Epoch: 00 [   84/25556 (  0%)], Train Loss: 3.23416\n",
      "Epoch: 00 [  124/25556 (  0%)], Train Loss: 3.21727\n",
      "Epoch: 00 [  164/25556 (  1%)], Train Loss: 3.18484\n",
      "Epoch: 00 [  204/25556 (  1%)], Train Loss: 3.12983\n",
      "Epoch: 00 [  244/25556 (  1%)], Train Loss: 3.07307\n",
      "Epoch: 00 [  284/25556 (  1%)], Train Loss: 3.01422\n",
      "Epoch: 00 [  324/25556 (  1%)], Train Loss: 2.92604\n",
      "Epoch: 00 [  364/25556 (  1%)], Train Loss: 2.81388\n",
      "Epoch: 00 [  404/25556 (  2%)], Train Loss: 2.67985\n",
      "Epoch: 00 [  444/25556 (  2%)], Train Loss: 2.55288\n",
      "Epoch: 00 [  484/25556 (  2%)], Train Loss: 2.42740\n",
      "Epoch: 00 [  524/25556 (  2%)], Train Loss: 2.31967\n",
      "Epoch: 00 [  564/25556 (  2%)], Train Loss: 2.21068\n",
      "Epoch: 00 [  604/25556 (  2%)], Train Loss: 2.12760\n",
      "Epoch: 00 [  644/25556 (  3%)], Train Loss: 2.04085\n",
      "Epoch: 00 [  684/25556 (  3%)], Train Loss: 1.96316\n",
      "Epoch: 00 [  724/25556 (  3%)], Train Loss: 1.88602\n",
      "Epoch: 00 [  764/25556 (  3%)], Train Loss: 1.80940\n",
      "Epoch: 00 [  804/25556 (  3%)], Train Loss: 1.74834\n",
      "Epoch: 00 [  844/25556 (  3%)], Train Loss: 1.68965\n",
      "Epoch: 00 [  884/25556 (  3%)], Train Loss: 1.63775\n",
      "Epoch: 00 [  924/25556 (  4%)], Train Loss: 1.58810\n",
      "Epoch: 00 [  964/25556 (  4%)], Train Loss: 1.54252\n",
      "Epoch: 00 [ 1004/25556 (  4%)], Train Loss: 1.49970\n",
      "Epoch: 00 [ 1044/25556 (  4%)], Train Loss: 1.46749\n",
      "Epoch: 00 [ 1084/25556 (  4%)], Train Loss: 1.43198\n",
      "Epoch: 00 [ 1124/25556 (  4%)], Train Loss: 1.39890\n",
      "Epoch: 00 [ 1164/25556 (  5%)], Train Loss: 1.37104\n",
      "Epoch: 00 [ 1204/25556 (  5%)], Train Loss: 1.34810\n",
      "Epoch: 00 [ 1244/25556 (  5%)], Train Loss: 1.32528\n",
      "Epoch: 00 [ 1284/25556 (  5%)], Train Loss: 1.29649\n",
      "Epoch: 00 [ 1324/25556 (  5%)], Train Loss: 1.28191\n",
      "Epoch: 00 [ 1364/25556 (  5%)], Train Loss: 1.26189\n",
      "Epoch: 00 [ 1404/25556 (  5%)], Train Loss: 1.24096\n",
      "Epoch: 00 [ 1444/25556 (  6%)], Train Loss: 1.22695\n",
      "Epoch: 00 [ 1484/25556 (  6%)], Train Loss: 1.20436\n",
      "Epoch: 00 [ 1524/25556 (  6%)], Train Loss: 1.18015\n",
      "Epoch: 00 [ 1564/25556 (  6%)], Train Loss: 1.16688\n",
      "Epoch: 00 [ 1604/25556 (  6%)], Train Loss: 1.15696\n",
      "Epoch: 00 [ 1644/25556 (  6%)], Train Loss: 1.14275\n",
      "Epoch: 00 [ 1684/25556 (  7%)], Train Loss: 1.13019\n",
      "Epoch: 00 [ 1724/25556 (  7%)], Train Loss: 1.11662\n",
      "Epoch: 00 [ 1764/25556 (  7%)], Train Loss: 1.09757\n",
      "Epoch: 00 [ 1804/25556 (  7%)], Train Loss: 1.08710\n",
      "Epoch: 00 [ 1844/25556 (  7%)], Train Loss: 1.07187\n",
      "Epoch: 00 [ 1884/25556 (  7%)], Train Loss: 1.05568\n",
      "Epoch: 00 [ 1924/25556 (  8%)], Train Loss: 1.04437\n",
      "Epoch: 00 [ 1964/25556 (  8%)], Train Loss: 1.02989\n",
      "Epoch: 00 [ 2004/25556 (  8%)], Train Loss: 1.01646\n",
      "Epoch: 00 [ 2044/25556 (  8%)], Train Loss: 1.00278\n",
      "Epoch: 00 [ 2084/25556 (  8%)], Train Loss: 0.99237\n",
      "Epoch: 00 [ 2124/25556 (  8%)], Train Loss: 0.98127\n",
      "Epoch: 00 [ 2164/25556 (  8%)], Train Loss: 0.97616\n",
      "Epoch: 00 [ 2204/25556 (  9%)], Train Loss: 0.96881\n",
      "Epoch: 00 [ 2244/25556 (  9%)], Train Loss: 0.96226\n",
      "Epoch: 00 [ 2284/25556 (  9%)], Train Loss: 0.95303\n",
      "Epoch: 00 [ 2324/25556 (  9%)], Train Loss: 0.94058\n",
      "Epoch: 00 [ 2364/25556 (  9%)], Train Loss: 0.93916\n",
      "Epoch: 00 [ 2404/25556 (  9%)], Train Loss: 0.93198\n",
      "Epoch: 00 [ 2444/25556 ( 10%)], Train Loss: 0.92422\n",
      "Epoch: 00 [ 2484/25556 ( 10%)], Train Loss: 0.92038\n",
      "Epoch: 00 [ 2524/25556 ( 10%)], Train Loss: 0.91376\n",
      "Epoch: 00 [ 2564/25556 ( 10%)], Train Loss: 0.90446\n",
      "Epoch: 00 [ 2604/25556 ( 10%)], Train Loss: 0.90185\n",
      "Epoch: 00 [ 2644/25556 ( 10%)], Train Loss: 0.89373\n",
      "Epoch: 00 [ 2684/25556 ( 11%)], Train Loss: 0.88519\n",
      "Epoch: 00 [ 2724/25556 ( 11%)], Train Loss: 0.87992\n",
      "Epoch: 00 [ 2764/25556 ( 11%)], Train Loss: 0.87819\n",
      "Epoch: 00 [ 2804/25556 ( 11%)], Train Loss: 0.87520\n",
      "Epoch: 00 [ 2844/25556 ( 11%)], Train Loss: 0.87054\n",
      "Epoch: 00 [ 2884/25556 ( 11%)], Train Loss: 0.86881\n",
      "Epoch: 00 [ 2924/25556 ( 11%)], Train Loss: 0.86088\n",
      "Epoch: 00 [ 2964/25556 ( 12%)], Train Loss: 0.85570\n",
      "Epoch: 00 [ 3004/25556 ( 12%)], Train Loss: 0.85256\n",
      "Epoch: 00 [ 3044/25556 ( 12%)], Train Loss: 0.84632\n",
      "Epoch: 00 [ 3084/25556 ( 12%)], Train Loss: 0.84081\n",
      "Epoch: 00 [ 3124/25556 ( 12%)], Train Loss: 0.83434\n",
      "Epoch: 00 [ 3164/25556 ( 12%)], Train Loss: 0.83371\n",
      "Epoch: 00 [ 3204/25556 ( 13%)], Train Loss: 0.83157\n",
      "Epoch: 00 [ 3244/25556 ( 13%)], Train Loss: 0.82964\n",
      "Epoch: 00 [ 3284/25556 ( 13%)], Train Loss: 0.82310\n",
      "Epoch: 00 [ 3324/25556 ( 13%)], Train Loss: 0.82187\n",
      "Epoch: 00 [ 3364/25556 ( 13%)], Train Loss: 0.81751\n",
      "Epoch: 00 [ 3404/25556 ( 13%)], Train Loss: 0.81333\n",
      "Epoch: 00 [ 3444/25556 ( 13%)], Train Loss: 0.81128\n",
      "Epoch: 00 [ 3484/25556 ( 14%)], Train Loss: 0.80644\n",
      "Epoch: 00 [ 3524/25556 ( 14%)], Train Loss: 0.80136\n",
      "Epoch: 00 [ 3564/25556 ( 14%)], Train Loss: 0.79816\n",
      "Epoch: 00 [ 3604/25556 ( 14%)], Train Loss: 0.79299\n",
      "Epoch: 00 [ 3644/25556 ( 14%)], Train Loss: 0.79137\n",
      "Epoch: 00 [ 3684/25556 ( 14%)], Train Loss: 0.78724\n",
      "Epoch: 00 [ 3724/25556 ( 15%)], Train Loss: 0.78447\n",
      "Epoch: 00 [ 3764/25556 ( 15%)], Train Loss: 0.78381\n",
      "Epoch: 00 [ 3804/25556 ( 15%)], Train Loss: 0.77975\n",
      "Epoch: 00 [ 3844/25556 ( 15%)], Train Loss: 0.77627\n",
      "Epoch: 00 [ 3884/25556 ( 15%)], Train Loss: 0.77512\n",
      "Epoch: 00 [ 3924/25556 ( 15%)], Train Loss: 0.77309\n",
      "Epoch: 00 [ 3964/25556 ( 16%)], Train Loss: 0.77161\n",
      "Epoch: 00 [ 4004/25556 ( 16%)], Train Loss: 0.76818\n",
      "Epoch: 00 [ 4044/25556 ( 16%)], Train Loss: 0.76679\n",
      "Epoch: 00 [ 4084/25556 ( 16%)], Train Loss: 0.76654\n",
      "Epoch: 00 [ 4124/25556 ( 16%)], Train Loss: 0.76476\n",
      "Epoch: 00 [ 4164/25556 ( 16%)], Train Loss: 0.76175\n",
      "Epoch: 00 [ 4204/25556 ( 16%)], Train Loss: 0.76008\n",
      "Epoch: 00 [ 4244/25556 ( 17%)], Train Loss: 0.75890\n",
      "Epoch: 00 [ 4284/25556 ( 17%)], Train Loss: 0.75750\n",
      "Epoch: 00 [ 4324/25556 ( 17%)], Train Loss: 0.75424\n",
      "Epoch: 00 [ 4364/25556 ( 17%)], Train Loss: 0.75159\n",
      "Epoch: 00 [ 4404/25556 ( 17%)], Train Loss: 0.74918\n",
      "Epoch: 00 [ 4444/25556 ( 17%)], Train Loss: 0.74679\n",
      "Epoch: 00 [ 4484/25556 ( 18%)], Train Loss: 0.74628\n",
      "Epoch: 00 [ 4524/25556 ( 18%)], Train Loss: 0.74739\n",
      "Epoch: 00 [ 4564/25556 ( 18%)], Train Loss: 0.74615\n",
      "Epoch: 00 [ 4604/25556 ( 18%)], Train Loss: 0.74506\n",
      "Epoch: 00 [ 4644/25556 ( 18%)], Train Loss: 0.74246\n",
      "Epoch: 00 [ 4684/25556 ( 18%)], Train Loss: 0.73959\n",
      "Epoch: 00 [ 4724/25556 ( 18%)], Train Loss: 0.73769\n",
      "Epoch: 00 [ 4764/25556 ( 19%)], Train Loss: 0.73614\n",
      "Epoch: 00 [ 4804/25556 ( 19%)], Train Loss: 0.73349\n",
      "Epoch: 00 [ 4844/25556 ( 19%)], Train Loss: 0.73062\n",
      "Epoch: 00 [ 4884/25556 ( 19%)], Train Loss: 0.73008\n",
      "Epoch: 00 [ 4924/25556 ( 19%)], Train Loss: 0.72864\n",
      "Epoch: 00 [ 4964/25556 ( 19%)], Train Loss: 0.72617\n",
      "Epoch: 00 [ 5004/25556 ( 20%)], Train Loss: 0.72357\n",
      "Epoch: 00 [ 5044/25556 ( 20%)], Train Loss: 0.72012\n",
      "Epoch: 00 [ 5084/25556 ( 20%)], Train Loss: 0.71752\n",
      "Epoch: 00 [ 5124/25556 ( 20%)], Train Loss: 0.71619\n",
      "Epoch: 00 [ 5164/25556 ( 20%)], Train Loss: 0.71315\n",
      "Epoch: 00 [ 5204/25556 ( 20%)], Train Loss: 0.71212\n",
      "Epoch: 00 [ 5244/25556 ( 21%)], Train Loss: 0.71038\n",
      "Epoch: 00 [ 5284/25556 ( 21%)], Train Loss: 0.70872\n",
      "Epoch: 00 [ 5324/25556 ( 21%)], Train Loss: 0.70780\n",
      "Epoch: 00 [ 5364/25556 ( 21%)], Train Loss: 0.70524\n",
      "Epoch: 00 [ 5404/25556 ( 21%)], Train Loss: 0.70311\n",
      "Epoch: 00 [ 5444/25556 ( 21%)], Train Loss: 0.70142\n",
      "Epoch: 00 [ 5484/25556 ( 21%)], Train Loss: 0.69907\n",
      "Epoch: 00 [ 5524/25556 ( 22%)], Train Loss: 0.69714\n",
      "Epoch: 00 [ 5564/25556 ( 22%)], Train Loss: 0.69545\n",
      "Epoch: 00 [ 5604/25556 ( 22%)], Train Loss: 0.69238\n",
      "Epoch: 00 [ 5644/25556 ( 22%)], Train Loss: 0.69051\n",
      "Epoch: 00 [ 5684/25556 ( 22%)], Train Loss: 0.69004\n",
      "Epoch: 00 [ 5724/25556 ( 22%)], Train Loss: 0.68856\n",
      "Epoch: 00 [ 5764/25556 ( 23%)], Train Loss: 0.68580\n",
      "Epoch: 00 [ 5804/25556 ( 23%)], Train Loss: 0.68546\n",
      "Epoch: 00 [ 5844/25556 ( 23%)], Train Loss: 0.68556\n",
      "Epoch: 00 [ 5884/25556 ( 23%)], Train Loss: 0.68384\n",
      "Epoch: 00 [ 5924/25556 ( 23%)], Train Loss: 0.68336\n",
      "Epoch: 00 [ 5964/25556 ( 23%)], Train Loss: 0.68258\n",
      "Epoch: 00 [ 6004/25556 ( 23%)], Train Loss: 0.68058\n",
      "Epoch: 00 [ 6044/25556 ( 24%)], Train Loss: 0.68125\n",
      "Epoch: 00 [ 6084/25556 ( 24%)], Train Loss: 0.67998\n",
      "Epoch: 00 [ 6124/25556 ( 24%)], Train Loss: 0.67959\n",
      "Epoch: 00 [ 6164/25556 ( 24%)], Train Loss: 0.67942\n",
      "Epoch: 00 [ 6204/25556 ( 24%)], Train Loss: 0.67804\n",
      "Epoch: 00 [ 6244/25556 ( 24%)], Train Loss: 0.67871\n",
      "Epoch: 00 [ 6284/25556 ( 25%)], Train Loss: 0.67826\n",
      "Epoch: 00 [ 6324/25556 ( 25%)], Train Loss: 0.67998\n",
      "Epoch: 00 [ 6364/25556 ( 25%)], Train Loss: 0.67869\n",
      "Epoch: 00 [ 6404/25556 ( 25%)], Train Loss: 0.67963\n",
      "Epoch: 00 [ 6444/25556 ( 25%)], Train Loss: 0.67953\n",
      "Epoch: 00 [ 6484/25556 ( 25%)], Train Loss: 0.67783\n",
      "Epoch: 00 [ 6524/25556 ( 26%)], Train Loss: 0.67884\n",
      "Epoch: 00 [ 6564/25556 ( 26%)], Train Loss: 0.67910\n",
      "Epoch: 00 [ 6604/25556 ( 26%)], Train Loss: 0.67702\n",
      "Epoch: 00 [ 6644/25556 ( 26%)], Train Loss: 0.67723\n",
      "Epoch: 00 [ 6684/25556 ( 26%)], Train Loss: 0.67597\n",
      "Epoch: 00 [ 6724/25556 ( 26%)], Train Loss: 0.67525\n",
      "Epoch: 00 [ 6764/25556 ( 26%)], Train Loss: 0.67365\n",
      "Epoch: 00 [ 6804/25556 ( 27%)], Train Loss: 0.67161\n",
      "Epoch: 00 [ 6844/25556 ( 27%)], Train Loss: 0.67135\n",
      "Epoch: 00 [ 6884/25556 ( 27%)], Train Loss: 0.67013\n",
      "Epoch: 00 [ 6924/25556 ( 27%)], Train Loss: 0.66891\n",
      "Epoch: 00 [ 6964/25556 ( 27%)], Train Loss: 0.66797\n",
      "Epoch: 00 [ 7004/25556 ( 27%)], Train Loss: 0.66821\n",
      "Epoch: 00 [ 7044/25556 ( 28%)], Train Loss: 0.66680\n",
      "Epoch: 00 [ 7084/25556 ( 28%)], Train Loss: 0.66482\n",
      "Epoch: 00 [ 7124/25556 ( 28%)], Train Loss: 0.66331\n",
      "Epoch: 00 [ 7164/25556 ( 28%)], Train Loss: 0.66418\n",
      "Epoch: 00 [ 7204/25556 ( 28%)], Train Loss: 0.66297\n",
      "Epoch: 00 [ 7244/25556 ( 28%)], Train Loss: 0.66225\n",
      "Epoch: 00 [ 7284/25556 ( 29%)], Train Loss: 0.66137\n",
      "Epoch: 00 [ 7324/25556 ( 29%)], Train Loss: 0.66039\n",
      "Epoch: 00 [ 7364/25556 ( 29%)], Train Loss: 0.65979\n",
      "Epoch: 00 [ 7404/25556 ( 29%)], Train Loss: 0.65848\n",
      "Epoch: 00 [ 7444/25556 ( 29%)], Train Loss: 0.65760\n",
      "Epoch: 00 [ 7484/25556 ( 29%)], Train Loss: 0.65828\n",
      "Epoch: 00 [ 7524/25556 ( 29%)], Train Loss: 0.65825\n",
      "Epoch: 00 [ 7564/25556 ( 30%)], Train Loss: 0.65685\n",
      "Epoch: 00 [ 7604/25556 ( 30%)], Train Loss: 0.65582\n",
      "Epoch: 00 [ 7644/25556 ( 30%)], Train Loss: 0.65367\n",
      "Epoch: 00 [ 7684/25556 ( 30%)], Train Loss: 0.65213\n",
      "Epoch: 00 [ 7724/25556 ( 30%)], Train Loss: 0.65220\n",
      "Epoch: 00 [ 7764/25556 ( 30%)], Train Loss: 0.65113\n",
      "Epoch: 00 [ 7804/25556 ( 31%)], Train Loss: 0.64992\n",
      "Epoch: 00 [ 7844/25556 ( 31%)], Train Loss: 0.64953\n",
      "Epoch: 00 [ 7884/25556 ( 31%)], Train Loss: 0.64744\n",
      "Epoch: 00 [ 7924/25556 ( 31%)], Train Loss: 0.64543\n",
      "Epoch: 00 [ 7964/25556 ( 31%)], Train Loss: 0.64449\n",
      "Epoch: 00 [ 8004/25556 ( 31%)], Train Loss: 0.64333\n",
      "Epoch: 00 [ 8044/25556 ( 31%)], Train Loss: 0.64215\n",
      "Epoch: 00 [ 8084/25556 ( 32%)], Train Loss: 0.64196\n",
      "Epoch: 00 [ 8124/25556 ( 32%)], Train Loss: 0.64164\n",
      "Epoch: 00 [ 8164/25556 ( 32%)], Train Loss: 0.64158\n",
      "Epoch: 00 [ 8204/25556 ( 32%)], Train Loss: 0.64174\n",
      "Epoch: 00 [ 8244/25556 ( 32%)], Train Loss: 0.64060\n",
      "Epoch: 00 [ 8284/25556 ( 32%)], Train Loss: 0.63950\n",
      "Epoch: 00 [ 8324/25556 ( 33%)], Train Loss: 0.63894\n",
      "Epoch: 00 [ 8364/25556 ( 33%)], Train Loss: 0.63867\n",
      "Epoch: 00 [ 8404/25556 ( 33%)], Train Loss: 0.63719\n",
      "Epoch: 00 [ 8444/25556 ( 33%)], Train Loss: 0.63622\n",
      "Epoch: 00 [ 8484/25556 ( 33%)], Train Loss: 0.63547\n",
      "Epoch: 00 [ 8524/25556 ( 33%)], Train Loss: 0.63458\n",
      "Epoch: 00 [ 8564/25556 ( 34%)], Train Loss: 0.63351\n",
      "Epoch: 00 [ 8604/25556 ( 34%)], Train Loss: 0.63341\n",
      "Epoch: 00 [ 8644/25556 ( 34%)], Train Loss: 0.63271\n",
      "Epoch: 00 [ 8684/25556 ( 34%)], Train Loss: 0.63193\n",
      "Epoch: 00 [ 8724/25556 ( 34%)], Train Loss: 0.63116\n",
      "Epoch: 00 [ 8764/25556 ( 34%)], Train Loss: 0.63079\n",
      "Epoch: 00 [ 8804/25556 ( 34%)], Train Loss: 0.63054\n",
      "Epoch: 00 [ 8844/25556 ( 35%)], Train Loss: 0.63001\n",
      "Epoch: 00 [ 8884/25556 ( 35%)], Train Loss: 0.62889\n",
      "Epoch: 00 [ 8924/25556 ( 35%)], Train Loss: 0.62875\n",
      "Epoch: 00 [ 8964/25556 ( 35%)], Train Loss: 0.62956\n",
      "Epoch: 00 [ 9004/25556 ( 35%)], Train Loss: 0.62827\n",
      "Epoch: 00 [ 9044/25556 ( 35%)], Train Loss: 0.62831\n",
      "Epoch: 00 [ 9084/25556 ( 36%)], Train Loss: 0.62855\n",
      "Epoch: 00 [ 9124/25556 ( 36%)], Train Loss: 0.62768\n",
      "Epoch: 00 [ 9164/25556 ( 36%)], Train Loss: 0.62757\n",
      "Epoch: 00 [ 9204/25556 ( 36%)], Train Loss: 0.62676\n",
      "Epoch: 00 [ 9244/25556 ( 36%)], Train Loss: 0.62571\n",
      "Epoch: 00 [ 9284/25556 ( 36%)], Train Loss: 0.62508\n",
      "Epoch: 00 [ 9324/25556 ( 36%)], Train Loss: 0.62483\n",
      "Epoch: 00 [ 9364/25556 ( 37%)], Train Loss: 0.62523\n",
      "Epoch: 00 [ 9404/25556 ( 37%)], Train Loss: 0.62366\n",
      "Epoch: 00 [ 9444/25556 ( 37%)], Train Loss: 0.62420\n",
      "Epoch: 00 [ 9484/25556 ( 37%)], Train Loss: 0.62383\n",
      "Epoch: 00 [ 9524/25556 ( 37%)], Train Loss: 0.62369\n",
      "Epoch: 00 [ 9564/25556 ( 37%)], Train Loss: 0.62325\n",
      "Epoch: 00 [ 9604/25556 ( 38%)], Train Loss: 0.62238\n",
      "Epoch: 00 [ 9644/25556 ( 38%)], Train Loss: 0.62125\n",
      "Epoch: 00 [ 9684/25556 ( 38%)], Train Loss: 0.62104\n",
      "Epoch: 00 [ 9724/25556 ( 38%)], Train Loss: 0.61995\n",
      "Epoch: 00 [ 9764/25556 ( 38%)], Train Loss: 0.61949\n",
      "Epoch: 00 [ 9804/25556 ( 38%)], Train Loss: 0.61840\n",
      "Epoch: 00 [ 9844/25556 ( 39%)], Train Loss: 0.61746\n",
      "Epoch: 00 [ 9884/25556 ( 39%)], Train Loss: 0.61675\n",
      "Epoch: 00 [ 9924/25556 ( 39%)], Train Loss: 0.61616\n",
      "Epoch: 00 [ 9964/25556 ( 39%)], Train Loss: 0.61584\n",
      "Epoch: 00 [10004/25556 ( 39%)], Train Loss: 0.61654\n",
      "Epoch: 00 [10044/25556 ( 39%)], Train Loss: 0.61566\n",
      "Epoch: 00 [10084/25556 ( 39%)], Train Loss: 0.61460\n",
      "Epoch: 00 [10124/25556 ( 40%)], Train Loss: 0.61374\n",
      "Epoch: 00 [10164/25556 ( 40%)], Train Loss: 0.61292\n",
      "Epoch: 00 [10204/25556 ( 40%)], Train Loss: 0.61203\n",
      "Epoch: 00 [10244/25556 ( 40%)], Train Loss: 0.61116\n",
      "Epoch: 00 [10284/25556 ( 40%)], Train Loss: 0.61140\n",
      "Epoch: 00 [10324/25556 ( 40%)], Train Loss: 0.61056\n",
      "Epoch: 00 [10364/25556 ( 41%)], Train Loss: 0.61006\n",
      "Epoch: 00 [10404/25556 ( 41%)], Train Loss: 0.60944\n",
      "Epoch: 00 [10444/25556 ( 41%)], Train Loss: 0.60913\n",
      "Epoch: 00 [10484/25556 ( 41%)], Train Loss: 0.60824\n",
      "Epoch: 00 [10524/25556 ( 41%)], Train Loss: 0.60709\n",
      "Epoch: 00 [10564/25556 ( 41%)], Train Loss: 0.60628\n",
      "Epoch: 00 [10604/25556 ( 41%)], Train Loss: 0.60649\n",
      "Epoch: 00 [10644/25556 ( 42%)], Train Loss: 0.60650\n",
      "Epoch: 00 [10684/25556 ( 42%)], Train Loss: 0.60542\n",
      "Epoch: 00 [10724/25556 ( 42%)], Train Loss: 0.60592\n",
      "Epoch: 00 [10764/25556 ( 42%)], Train Loss: 0.60500\n",
      "Epoch: 00 [10804/25556 ( 42%)], Train Loss: 0.60440\n",
      "Epoch: 00 [10844/25556 ( 42%)], Train Loss: 0.60383\n",
      "Epoch: 00 [10884/25556 ( 43%)], Train Loss: 0.60319\n",
      "Epoch: 00 [10924/25556 ( 43%)], Train Loss: 0.60244\n",
      "Epoch: 00 [10964/25556 ( 43%)], Train Loss: 0.60196\n",
      "Epoch: 00 [11004/25556 ( 43%)], Train Loss: 0.60187\n",
      "Epoch: 00 [11044/25556 ( 43%)], Train Loss: 0.60111\n",
      "Epoch: 00 [11084/25556 ( 43%)], Train Loss: 0.60004\n",
      "Epoch: 00 [11124/25556 ( 44%)], Train Loss: 0.59960\n",
      "Epoch: 00 [11164/25556 ( 44%)], Train Loss: 0.59941\n",
      "Epoch: 00 [11204/25556 ( 44%)], Train Loss: 0.59862\n",
      "Epoch: 00 [11244/25556 ( 44%)], Train Loss: 0.59783\n",
      "Epoch: 00 [11284/25556 ( 44%)], Train Loss: 0.59664\n",
      "Epoch: 00 [11324/25556 ( 44%)], Train Loss: 0.59564\n",
      "Epoch: 00 [11364/25556 ( 44%)], Train Loss: 0.59469\n",
      "Epoch: 00 [11404/25556 ( 45%)], Train Loss: 0.59482\n",
      "Epoch: 00 [11444/25556 ( 45%)], Train Loss: 0.59455\n",
      "Epoch: 00 [11484/25556 ( 45%)], Train Loss: 0.59403\n",
      "Epoch: 00 [11524/25556 ( 45%)], Train Loss: 0.59345\n",
      "Epoch: 00 [11564/25556 ( 45%)], Train Loss: 0.59264\n",
      "Epoch: 00 [11604/25556 ( 45%)], Train Loss: 0.59309\n",
      "Epoch: 00 [11644/25556 ( 46%)], Train Loss: 0.59276\n",
      "Epoch: 00 [11684/25556 ( 46%)], Train Loss: 0.59218\n",
      "Epoch: 00 [11724/25556 ( 46%)], Train Loss: 0.59178\n",
      "Epoch: 00 [11764/25556 ( 46%)], Train Loss: 0.59099\n",
      "Epoch: 00 [11804/25556 ( 46%)], Train Loss: 0.59037\n",
      "Epoch: 00 [11844/25556 ( 46%)], Train Loss: 0.58987\n",
      "Epoch: 00 [11884/25556 ( 47%)], Train Loss: 0.58941\n",
      "Epoch: 00 [11924/25556 ( 47%)], Train Loss: 0.58854\n",
      "Epoch: 00 [11964/25556 ( 47%)], Train Loss: 0.58811\n",
      "Epoch: 00 [12004/25556 ( 47%)], Train Loss: 0.58747\n",
      "Epoch: 00 [12044/25556 ( 47%)], Train Loss: 0.58661\n",
      "Epoch: 00 [12084/25556 ( 47%)], Train Loss: 0.58659\n",
      "Epoch: 00 [12124/25556 ( 47%)], Train Loss: 0.58620\n",
      "Epoch: 00 [12164/25556 ( 48%)], Train Loss: 0.58554\n",
      "Epoch: 00 [12204/25556 ( 48%)], Train Loss: 0.58532\n",
      "Epoch: 00 [12244/25556 ( 48%)], Train Loss: 0.58515\n",
      "Epoch: 00 [12284/25556 ( 48%)], Train Loss: 0.58495\n",
      "Epoch: 00 [12324/25556 ( 48%)], Train Loss: 0.58464\n",
      "Epoch: 00 [12364/25556 ( 48%)], Train Loss: 0.58467\n",
      "Epoch: 00 [12404/25556 ( 49%)], Train Loss: 0.58424\n",
      "Epoch: 00 [12444/25556 ( 49%)], Train Loss: 0.58380\n",
      "Epoch: 00 [12484/25556 ( 49%)], Train Loss: 0.58288\n",
      "Epoch: 00 [12524/25556 ( 49%)], Train Loss: 0.58246\n",
      "Epoch: 00 [12564/25556 ( 49%)], Train Loss: 0.58184\n",
      "Epoch: 00 [12604/25556 ( 49%)], Train Loss: 0.58063\n",
      "Epoch: 00 [12644/25556 ( 49%)], Train Loss: 0.58036\n",
      "Epoch: 00 [12684/25556 ( 50%)], Train Loss: 0.58036\n",
      "Epoch: 00 [12724/25556 ( 50%)], Train Loss: 0.58033\n",
      "Epoch: 00 [12764/25556 ( 50%)], Train Loss: 0.58028\n",
      "Epoch: 00 [12804/25556 ( 50%)], Train Loss: 0.57985\n",
      "Epoch: 00 [12844/25556 ( 50%)], Train Loss: 0.57979\n",
      "Epoch: 00 [12884/25556 ( 50%)], Train Loss: 0.58006\n",
      "Epoch: 00 [12924/25556 ( 51%)], Train Loss: 0.57957\n",
      "Epoch: 00 [12964/25556 ( 51%)], Train Loss: 0.57924\n",
      "Epoch: 00 [13004/25556 ( 51%)], Train Loss: 0.57860\n",
      "Epoch: 00 [13044/25556 ( 51%)], Train Loss: 0.57814\n",
      "Epoch: 00 [13084/25556 ( 51%)], Train Loss: 0.57814\n",
      "Epoch: 00 [13124/25556 ( 51%)], Train Loss: 0.57749\n",
      "Epoch: 00 [13164/25556 ( 52%)], Train Loss: 0.57737\n",
      "Epoch: 00 [13204/25556 ( 52%)], Train Loss: 0.57698\n",
      "Epoch: 00 [13244/25556 ( 52%)], Train Loss: 0.57590\n",
      "Epoch: 00 [13284/25556 ( 52%)], Train Loss: 0.57477\n",
      "Epoch: 00 [13324/25556 ( 52%)], Train Loss: 0.57451\n",
      "Epoch: 00 [13364/25556 ( 52%)], Train Loss: 0.57380\n",
      "Epoch: 00 [13404/25556 ( 52%)], Train Loss: 0.57313\n",
      "Epoch: 00 [13444/25556 ( 53%)], Train Loss: 0.57259\n",
      "Epoch: 00 [13484/25556 ( 53%)], Train Loss: 0.57188\n",
      "Epoch: 00 [13524/25556 ( 53%)], Train Loss: 0.57153\n",
      "Epoch: 00 [13564/25556 ( 53%)], Train Loss: 0.57112\n",
      "Epoch: 00 [13604/25556 ( 53%)], Train Loss: 0.57066\n",
      "Epoch: 00 [13644/25556 ( 53%)], Train Loss: 0.57002\n",
      "Epoch: 00 [13684/25556 ( 54%)], Train Loss: 0.56931\n",
      "Epoch: 00 [13724/25556 ( 54%)], Train Loss: 0.56890\n",
      "Epoch: 00 [13764/25556 ( 54%)], Train Loss: 0.56805\n",
      "Epoch: 00 [13804/25556 ( 54%)], Train Loss: 0.56729\n",
      "Epoch: 00 [13844/25556 ( 54%)], Train Loss: 0.56670\n",
      "Epoch: 00 [13884/25556 ( 54%)], Train Loss: 0.56631\n",
      "Epoch: 00 [13924/25556 ( 54%)], Train Loss: 0.56591\n",
      "Epoch: 00 [13964/25556 ( 55%)], Train Loss: 0.56526\n",
      "Epoch: 00 [14004/25556 ( 55%)], Train Loss: 0.56540\n",
      "Epoch: 00 [14044/25556 ( 55%)], Train Loss: 0.56513\n",
      "Epoch: 00 [14084/25556 ( 55%)], Train Loss: 0.56448\n",
      "Epoch: 00 [14124/25556 ( 55%)], Train Loss: 0.56378\n",
      "Epoch: 00 [14164/25556 ( 55%)], Train Loss: 0.56353\n",
      "Epoch: 00 [14204/25556 ( 56%)], Train Loss: 0.56344\n",
      "Epoch: 00 [14244/25556 ( 56%)], Train Loss: 0.56288\n",
      "Epoch: 00 [14284/25556 ( 56%)], Train Loss: 0.56260\n",
      "Epoch: 00 [14324/25556 ( 56%)], Train Loss: 0.56203\n",
      "Epoch: 00 [14364/25556 ( 56%)], Train Loss: 0.56114\n",
      "Epoch: 00 [14404/25556 ( 56%)], Train Loss: 0.56138\n",
      "Epoch: 00 [14444/25556 ( 57%)], Train Loss: 0.56125\n",
      "Epoch: 00 [14484/25556 ( 57%)], Train Loss: 0.56125\n",
      "Epoch: 00 [14524/25556 ( 57%)], Train Loss: 0.56085\n",
      "Epoch: 00 [14564/25556 ( 57%)], Train Loss: 0.56042\n",
      "Epoch: 00 [14604/25556 ( 57%)], Train Loss: 0.55991\n",
      "Epoch: 00 [14644/25556 ( 57%)], Train Loss: 0.55977\n",
      "Epoch: 00 [14684/25556 ( 57%)], Train Loss: 0.55903\n",
      "Epoch: 00 [14724/25556 ( 58%)], Train Loss: 0.55884\n",
      "Epoch: 00 [14764/25556 ( 58%)], Train Loss: 0.55886\n",
      "Epoch: 00 [14804/25556 ( 58%)], Train Loss: 0.55848\n",
      "Epoch: 00 [14844/25556 ( 58%)], Train Loss: 0.55816\n",
      "Epoch: 00 [14884/25556 ( 58%)], Train Loss: 0.55761\n",
      "Epoch: 00 [14924/25556 ( 58%)], Train Loss: 0.55721\n",
      "Epoch: 00 [14964/25556 ( 59%)], Train Loss: 0.55743\n",
      "Epoch: 00 [15004/25556 ( 59%)], Train Loss: 0.55727\n",
      "Epoch: 00 [15044/25556 ( 59%)], Train Loss: 0.55664\n",
      "Epoch: 00 [15084/25556 ( 59%)], Train Loss: 0.55625\n",
      "Epoch: 00 [15124/25556 ( 59%)], Train Loss: 0.55559\n",
      "Epoch: 00 [15164/25556 ( 59%)], Train Loss: 0.55477\n",
      "Epoch: 00 [15204/25556 ( 59%)], Train Loss: 0.55417\n",
      "Epoch: 00 [15244/25556 ( 60%)], Train Loss: 0.55402\n",
      "Epoch: 00 [15284/25556 ( 60%)], Train Loss: 0.55348\n",
      "Epoch: 00 [15324/25556 ( 60%)], Train Loss: 0.55322\n",
      "Epoch: 00 [15364/25556 ( 60%)], Train Loss: 0.55250\n",
      "Epoch: 00 [15404/25556 ( 60%)], Train Loss: 0.55247\n",
      "Epoch: 00 [15444/25556 ( 60%)], Train Loss: 0.55196\n",
      "Epoch: 00 [15484/25556 ( 61%)], Train Loss: 0.55154\n",
      "Epoch: 00 [15524/25556 ( 61%)], Train Loss: 0.55077\n",
      "Epoch: 00 [15564/25556 ( 61%)], Train Loss: 0.55057\n",
      "Epoch: 00 [15604/25556 ( 61%)], Train Loss: 0.55065\n",
      "Epoch: 00 [15644/25556 ( 61%)], Train Loss: 0.55065\n",
      "Epoch: 00 [15684/25556 ( 61%)], Train Loss: 0.55050\n",
      "Epoch: 00 [15724/25556 ( 62%)], Train Loss: 0.54970\n",
      "Epoch: 00 [15764/25556 ( 62%)], Train Loss: 0.54887\n",
      "Epoch: 00 [15804/25556 ( 62%)], Train Loss: 0.54813\n",
      "Epoch: 00 [15844/25556 ( 62%)], Train Loss: 0.54796\n",
      "Epoch: 00 [15884/25556 ( 62%)], Train Loss: 0.54743\n",
      "Epoch: 00 [15924/25556 ( 62%)], Train Loss: 0.54744\n",
      "Epoch: 00 [15964/25556 ( 62%)], Train Loss: 0.54732\n",
      "Epoch: 00 [16004/25556 ( 63%)], Train Loss: 0.54691\n",
      "Epoch: 00 [16044/25556 ( 63%)], Train Loss: 0.54650\n",
      "Epoch: 00 [16084/25556 ( 63%)], Train Loss: 0.54626\n",
      "Epoch: 00 [16124/25556 ( 63%)], Train Loss: 0.54629\n",
      "Epoch: 00 [16164/25556 ( 63%)], Train Loss: 0.54600\n",
      "Epoch: 00 [16204/25556 ( 63%)], Train Loss: 0.54547\n",
      "Epoch: 00 [16244/25556 ( 64%)], Train Loss: 0.54540\n",
      "Epoch: 00 [16284/25556 ( 64%)], Train Loss: 0.54510\n",
      "Epoch: 00 [16324/25556 ( 64%)], Train Loss: 0.54476\n",
      "Epoch: 00 [16364/25556 ( 64%)], Train Loss: 0.54457\n",
      "Epoch: 00 [16404/25556 ( 64%)], Train Loss: 0.54397\n",
      "Epoch: 00 [16444/25556 ( 64%)], Train Loss: 0.54383\n",
      "Epoch: 00 [16484/25556 ( 65%)], Train Loss: 0.54383\n",
      "Epoch: 00 [16524/25556 ( 65%)], Train Loss: 0.54334\n",
      "Epoch: 00 [16564/25556 ( 65%)], Train Loss: 0.54338\n",
      "Epoch: 00 [16604/25556 ( 65%)], Train Loss: 0.54311\n",
      "Epoch: 00 [16644/25556 ( 65%)], Train Loss: 0.54259\n",
      "Epoch: 00 [16684/25556 ( 65%)], Train Loss: 0.54276\n",
      "Epoch: 00 [16724/25556 ( 65%)], Train Loss: 0.54283\n",
      "Epoch: 00 [16764/25556 ( 66%)], Train Loss: 0.54239\n",
      "Epoch: 00 [16804/25556 ( 66%)], Train Loss: 0.54181\n",
      "Epoch: 00 [16844/25556 ( 66%)], Train Loss: 0.54135\n",
      "Epoch: 00 [16884/25556 ( 66%)], Train Loss: 0.54057\n",
      "Epoch: 00 [16924/25556 ( 66%)], Train Loss: 0.54008\n",
      "Epoch: 00 [16964/25556 ( 66%)], Train Loss: 0.54002\n",
      "Epoch: 00 [17004/25556 ( 67%)], Train Loss: 0.53966\n",
      "Epoch: 00 [17044/25556 ( 67%)], Train Loss: 0.53908\n",
      "Epoch: 00 [17084/25556 ( 67%)], Train Loss: 0.53894\n",
      "Epoch: 00 [17124/25556 ( 67%)], Train Loss: 0.53827\n",
      "Epoch: 00 [17164/25556 ( 67%)], Train Loss: 0.53770\n",
      "Epoch: 00 [17204/25556 ( 67%)], Train Loss: 0.53778\n",
      "Epoch: 00 [17244/25556 ( 67%)], Train Loss: 0.53766\n",
      "Epoch: 00 [17284/25556 ( 68%)], Train Loss: 0.53733\n",
      "Epoch: 00 [17324/25556 ( 68%)], Train Loss: 0.53687\n",
      "Epoch: 00 [17364/25556 ( 68%)], Train Loss: 0.53641\n",
      "Epoch: 00 [17404/25556 ( 68%)], Train Loss: 0.53582\n",
      "Epoch: 00 [17444/25556 ( 68%)], Train Loss: 0.53632\n",
      "Epoch: 00 [17484/25556 ( 68%)], Train Loss: 0.53584\n",
      "Epoch: 00 [17524/25556 ( 69%)], Train Loss: 0.53545\n",
      "Epoch: 00 [17564/25556 ( 69%)], Train Loss: 0.53576\n",
      "Epoch: 00 [17604/25556 ( 69%)], Train Loss: 0.53522\n",
      "Epoch: 00 [17644/25556 ( 69%)], Train Loss: 0.53495\n",
      "Epoch: 00 [17684/25556 ( 69%)], Train Loss: 0.53454\n",
      "Epoch: 00 [17724/25556 ( 69%)], Train Loss: 0.53395\n",
      "Epoch: 00 [17764/25556 ( 70%)], Train Loss: 0.53340\n",
      "Epoch: 00 [17804/25556 ( 70%)], Train Loss: 0.53318\n",
      "Epoch: 00 [17844/25556 ( 70%)], Train Loss: 0.53306\n",
      "Epoch: 00 [17884/25556 ( 70%)], Train Loss: 0.53304\n",
      "Epoch: 00 [17924/25556 ( 70%)], Train Loss: 0.53284\n",
      "Epoch: 00 [17964/25556 ( 70%)], Train Loss: 0.53222\n",
      "Epoch: 00 [18004/25556 ( 70%)], Train Loss: 0.53173\n",
      "Epoch: 00 [18044/25556 ( 71%)], Train Loss: 0.53149\n",
      "Epoch: 00 [18084/25556 ( 71%)], Train Loss: 0.53100\n",
      "Epoch: 00 [18124/25556 ( 71%)], Train Loss: 0.53089\n",
      "Epoch: 00 [18164/25556 ( 71%)], Train Loss: 0.53115\n",
      "Epoch: 00 [18204/25556 ( 71%)], Train Loss: 0.53105\n",
      "Epoch: 00 [18244/25556 ( 71%)], Train Loss: 0.53079\n",
      "Epoch: 00 [18284/25556 ( 72%)], Train Loss: 0.53063\n",
      "Epoch: 00 [18324/25556 ( 72%)], Train Loss: 0.53065\n",
      "Epoch: 00 [18364/25556 ( 72%)], Train Loss: 0.53041\n",
      "Epoch: 00 [18404/25556 ( 72%)], Train Loss: 0.53015\n",
      "Epoch: 00 [18444/25556 ( 72%)], Train Loss: 0.52991\n",
      "Epoch: 00 [18484/25556 ( 72%)], Train Loss: 0.52958\n",
      "Epoch: 00 [18524/25556 ( 72%)], Train Loss: 0.52918\n",
      "Epoch: 00 [18564/25556 ( 73%)], Train Loss: 0.52871\n",
      "Epoch: 00 [18604/25556 ( 73%)], Train Loss: 0.52838\n",
      "Epoch: 00 [18644/25556 ( 73%)], Train Loss: 0.52823\n",
      "Epoch: 00 [18684/25556 ( 73%)], Train Loss: 0.52777\n",
      "Epoch: 00 [18724/25556 ( 73%)], Train Loss: 0.52707\n",
      "Epoch: 00 [18764/25556 ( 73%)], Train Loss: 0.52685\n",
      "Epoch: 00 [18804/25556 ( 74%)], Train Loss: 0.52644\n",
      "Epoch: 00 [18844/25556 ( 74%)], Train Loss: 0.52592\n",
      "Epoch: 00 [18884/25556 ( 74%)], Train Loss: 0.52592\n",
      "Epoch: 00 [18924/25556 ( 74%)], Train Loss: 0.52572\n",
      "Epoch: 00 [18964/25556 ( 74%)], Train Loss: 0.52535\n",
      "Epoch: 00 [19004/25556 ( 74%)], Train Loss: 0.52473\n",
      "Epoch: 00 [19044/25556 ( 75%)], Train Loss: 0.52409\n",
      "Epoch: 00 [19084/25556 ( 75%)], Train Loss: 0.52392\n",
      "Epoch: 00 [19124/25556 ( 75%)], Train Loss: 0.52362\n",
      "Epoch: 00 [19164/25556 ( 75%)], Train Loss: 0.52336\n",
      "Epoch: 00 [19204/25556 ( 75%)], Train Loss: 0.52312\n",
      "Epoch: 00 [19244/25556 ( 75%)], Train Loss: 0.52263\n",
      "Epoch: 00 [19284/25556 ( 75%)], Train Loss: 0.52229\n",
      "Epoch: 00 [19324/25556 ( 76%)], Train Loss: 0.52195\n",
      "Epoch: 00 [19364/25556 ( 76%)], Train Loss: 0.52192\n",
      "Epoch: 00 [19404/25556 ( 76%)], Train Loss: 0.52197\n",
      "Epoch: 00 [19444/25556 ( 76%)], Train Loss: 0.52183\n",
      "Epoch: 00 [19484/25556 ( 76%)], Train Loss: 0.52163\n",
      "Epoch: 00 [19524/25556 ( 76%)], Train Loss: 0.52131\n",
      "Epoch: 00 [19564/25556 ( 77%)], Train Loss: 0.52116\n",
      "Epoch: 00 [19604/25556 ( 77%)], Train Loss: 0.52092\n",
      "Epoch: 00 [19644/25556 ( 77%)], Train Loss: 0.52045\n",
      "Epoch: 00 [19684/25556 ( 77%)], Train Loss: 0.52035\n",
      "Epoch: 00 [19724/25556 ( 77%)], Train Loss: 0.51986\n",
      "Epoch: 00 [19764/25556 ( 77%)], Train Loss: 0.51937\n",
      "Epoch: 00 [19804/25556 ( 77%)], Train Loss: 0.51923\n",
      "Epoch: 00 [19844/25556 ( 78%)], Train Loss: 0.51890\n",
      "Epoch: 00 [19884/25556 ( 78%)], Train Loss: 0.51863\n",
      "Epoch: 00 [19924/25556 ( 78%)], Train Loss: 0.51823\n",
      "Epoch: 00 [19964/25556 ( 78%)], Train Loss: 0.51815\n",
      "Epoch: 00 [20004/25556 ( 78%)], Train Loss: 0.51769\n",
      "Epoch: 00 [20044/25556 ( 78%)], Train Loss: 0.51759\n",
      "Epoch: 00 [20084/25556 ( 79%)], Train Loss: 0.51709\n",
      "Epoch: 00 [20124/25556 ( 79%)], Train Loss: 0.51656\n",
      "Epoch: 00 [20164/25556 ( 79%)], Train Loss: 0.51647\n",
      "Epoch: 00 [20204/25556 ( 79%)], Train Loss: 0.51608\n",
      "Epoch: 00 [20244/25556 ( 79%)], Train Loss: 0.51580\n",
      "Epoch: 00 [20284/25556 ( 79%)], Train Loss: 0.51551\n",
      "Epoch: 00 [20324/25556 ( 80%)], Train Loss: 0.51496\n",
      "Epoch: 00 [20364/25556 ( 80%)], Train Loss: 0.51458\n",
      "Epoch: 00 [20404/25556 ( 80%)], Train Loss: 0.51450\n",
      "Epoch: 00 [20444/25556 ( 80%)], Train Loss: 0.51432\n",
      "Epoch: 00 [20484/25556 ( 80%)], Train Loss: 0.51428\n",
      "Epoch: 00 [20524/25556 ( 80%)], Train Loss: 0.51390\n",
      "Epoch: 00 [20564/25556 ( 80%)], Train Loss: 0.51370\n",
      "Epoch: 00 [20604/25556 ( 81%)], Train Loss: 0.51362\n",
      "Epoch: 00 [20644/25556 ( 81%)], Train Loss: 0.51348\n",
      "Epoch: 00 [20684/25556 ( 81%)], Train Loss: 0.51331\n",
      "Epoch: 00 [20724/25556 ( 81%)], Train Loss: 0.51309\n",
      "Epoch: 00 [20764/25556 ( 81%)], Train Loss: 0.51306\n",
      "Epoch: 00 [20804/25556 ( 81%)], Train Loss: 0.51263\n",
      "Epoch: 00 [20844/25556 ( 82%)], Train Loss: 0.51230\n",
      "Epoch: 00 [20884/25556 ( 82%)], Train Loss: 0.51180\n",
      "Epoch: 00 [20924/25556 ( 82%)], Train Loss: 0.51157\n",
      "Epoch: 00 [20964/25556 ( 82%)], Train Loss: 0.51161\n",
      "Epoch: 00 [21004/25556 ( 82%)], Train Loss: 0.51139\n",
      "Epoch: 00 [21044/25556 ( 82%)], Train Loss: 0.51082\n",
      "Epoch: 00 [21084/25556 ( 83%)], Train Loss: 0.51053\n",
      "Epoch: 00 [21124/25556 ( 83%)], Train Loss: 0.51025\n",
      "Epoch: 00 [21164/25556 ( 83%)], Train Loss: 0.50986\n",
      "Epoch: 00 [21204/25556 ( 83%)], Train Loss: 0.51035\n",
      "Epoch: 00 [21244/25556 ( 83%)], Train Loss: 0.51010\n",
      "Epoch: 00 [21284/25556 ( 83%)], Train Loss: 0.51007\n",
      "Epoch: 00 [21324/25556 ( 83%)], Train Loss: 0.50983\n",
      "Epoch: 00 [21364/25556 ( 84%)], Train Loss: 0.50960\n",
      "Epoch: 00 [21404/25556 ( 84%)], Train Loss: 0.50904\n",
      "Epoch: 00 [21444/25556 ( 84%)], Train Loss: 0.50879\n",
      "Epoch: 00 [21484/25556 ( 84%)], Train Loss: 0.50833\n",
      "Epoch: 00 [21524/25556 ( 84%)], Train Loss: 0.50810\n",
      "Epoch: 00 [21564/25556 ( 84%)], Train Loss: 0.50781\n",
      "Epoch: 00 [21604/25556 ( 85%)], Train Loss: 0.50801\n",
      "Epoch: 00 [21644/25556 ( 85%)], Train Loss: 0.50782\n",
      "Epoch: 00 [21684/25556 ( 85%)], Train Loss: 0.50780\n",
      "Epoch: 00 [21724/25556 ( 85%)], Train Loss: 0.50724\n",
      "Epoch: 00 [21764/25556 ( 85%)], Train Loss: 0.50678\n",
      "Epoch: 00 [21804/25556 ( 85%)], Train Loss: 0.50676\n",
      "Epoch: 00 [21844/25556 ( 85%)], Train Loss: 0.50662\n",
      "Epoch: 00 [21884/25556 ( 86%)], Train Loss: 0.50599\n",
      "Epoch: 00 [21924/25556 ( 86%)], Train Loss: 0.50569\n",
      "Epoch: 00 [21964/25556 ( 86%)], Train Loss: 0.50529\n",
      "Epoch: 00 [22004/25556 ( 86%)], Train Loss: 0.50517\n",
      "Epoch: 00 [22044/25556 ( 86%)], Train Loss: 0.50512\n",
      "Epoch: 00 [22084/25556 ( 86%)], Train Loss: 0.50458\n",
      "Epoch: 00 [22124/25556 ( 87%)], Train Loss: 0.50457\n",
      "Epoch: 00 [22164/25556 ( 87%)], Train Loss: 0.50437\n",
      "Epoch: 00 [22204/25556 ( 87%)], Train Loss: 0.50407\n",
      "Epoch: 00 [22244/25556 ( 87%)], Train Loss: 0.50401\n",
      "Epoch: 00 [22284/25556 ( 87%)], Train Loss: 0.50425\n",
      "Epoch: 00 [22324/25556 ( 87%)], Train Loss: 0.50404\n",
      "Epoch: 00 [22364/25556 ( 88%)], Train Loss: 0.50366\n",
      "Epoch: 00 [22404/25556 ( 88%)], Train Loss: 0.50325\n",
      "Epoch: 00 [22444/25556 ( 88%)], Train Loss: 0.50299\n",
      "Epoch: 00 [22484/25556 ( 88%)], Train Loss: 0.50285\n",
      "Epoch: 00 [22524/25556 ( 88%)], Train Loss: 0.50234\n",
      "Epoch: 00 [22564/25556 ( 88%)], Train Loss: 0.50192\n",
      "Epoch: 00 [22604/25556 ( 88%)], Train Loss: 0.50133\n",
      "Epoch: 00 [22644/25556 ( 89%)], Train Loss: 0.50138\n",
      "Epoch: 00 [22684/25556 ( 89%)], Train Loss: 0.50106\n",
      "Epoch: 00 [22724/25556 ( 89%)], Train Loss: 0.50109\n",
      "Epoch: 00 [22764/25556 ( 89%)], Train Loss: 0.50091\n",
      "Epoch: 00 [22804/25556 ( 89%)], Train Loss: 0.50075\n",
      "Epoch: 00 [22844/25556 ( 89%)], Train Loss: 0.50052\n",
      "Epoch: 00 [22884/25556 ( 90%)], Train Loss: 0.50031\n",
      "Epoch: 00 [22924/25556 ( 90%)], Train Loss: 0.50016\n",
      "Epoch: 00 [22964/25556 ( 90%)], Train Loss: 0.49985\n",
      "Epoch: 00 [23004/25556 ( 90%)], Train Loss: 0.49995\n",
      "Epoch: 00 [23044/25556 ( 90%)], Train Loss: 0.49968\n",
      "Epoch: 00 [23084/25556 ( 90%)], Train Loss: 0.49928\n",
      "Epoch: 00 [23124/25556 ( 90%)], Train Loss: 0.49890\n",
      "Epoch: 00 [23164/25556 ( 91%)], Train Loss: 0.49882\n",
      "Epoch: 00 [23204/25556 ( 91%)], Train Loss: 0.49866\n",
      "Epoch: 00 [23244/25556 ( 91%)], Train Loss: 0.49883\n",
      "Epoch: 00 [23284/25556 ( 91%)], Train Loss: 0.49834\n",
      "Epoch: 00 [23324/25556 ( 91%)], Train Loss: 0.49820\n",
      "Epoch: 00 [23364/25556 ( 91%)], Train Loss: 0.49780\n",
      "Epoch: 00 [23404/25556 ( 92%)], Train Loss: 0.49753\n",
      "Epoch: 00 [23444/25556 ( 92%)], Train Loss: 0.49738\n",
      "Epoch: 00 [23484/25556 ( 92%)], Train Loss: 0.49686\n",
      "Epoch: 00 [23524/25556 ( 92%)], Train Loss: 0.49653\n",
      "Epoch: 00 [23564/25556 ( 92%)], Train Loss: 0.49631\n",
      "Epoch: 00 [23604/25556 ( 92%)], Train Loss: 0.49627\n",
      "Epoch: 00 [23644/25556 ( 93%)], Train Loss: 0.49608\n",
      "Epoch: 00 [23684/25556 ( 93%)], Train Loss: 0.49564\n",
      "Epoch: 00 [23724/25556 ( 93%)], Train Loss: 0.49533\n",
      "Epoch: 00 [23764/25556 ( 93%)], Train Loss: 0.49526\n",
      "Epoch: 00 [23804/25556 ( 93%)], Train Loss: 0.49548\n",
      "Epoch: 00 [23844/25556 ( 93%)], Train Loss: 0.49543\n",
      "Epoch: 00 [23884/25556 ( 93%)], Train Loss: 0.49545\n",
      "Epoch: 00 [23924/25556 ( 94%)], Train Loss: 0.49534\n",
      "Epoch: 00 [23964/25556 ( 94%)], Train Loss: 0.49530\n",
      "Epoch: 00 [24004/25556 ( 94%)], Train Loss: 0.49500\n",
      "Epoch: 00 [24044/25556 ( 94%)], Train Loss: 0.49502\n",
      "Epoch: 00 [24084/25556 ( 94%)], Train Loss: 0.49479\n",
      "Epoch: 00 [24124/25556 ( 94%)], Train Loss: 0.49465\n",
      "Epoch: 00 [24164/25556 ( 95%)], Train Loss: 0.49437\n",
      "Epoch: 00 [24204/25556 ( 95%)], Train Loss: 0.49430\n",
      "Epoch: 00 [24244/25556 ( 95%)], Train Loss: 0.49389\n",
      "Epoch: 00 [24284/25556 ( 95%)], Train Loss: 0.49358\n",
      "Epoch: 00 [24324/25556 ( 95%)], Train Loss: 0.49321\n",
      "Epoch: 00 [24364/25556 ( 95%)], Train Loss: 0.49299\n",
      "Epoch: 00 [24404/25556 ( 95%)], Train Loss: 0.49275\n",
      "Epoch: 00 [24444/25556 ( 96%)], Train Loss: 0.49251\n",
      "Epoch: 00 [24484/25556 ( 96%)], Train Loss: 0.49254\n",
      "Epoch: 00 [24524/25556 ( 96%)], Train Loss: 0.49248\n",
      "Epoch: 00 [24564/25556 ( 96%)], Train Loss: 0.49221\n",
      "Epoch: 00 [24604/25556 ( 96%)], Train Loss: 0.49206\n",
      "Epoch: 00 [24644/25556 ( 96%)], Train Loss: 0.49173\n",
      "Epoch: 00 [24684/25556 ( 97%)], Train Loss: 0.49162\n",
      "Epoch: 00 [24724/25556 ( 97%)], Train Loss: 0.49127\n",
      "Epoch: 00 [24764/25556 ( 97%)], Train Loss: 0.49101\n",
      "Epoch: 00 [24804/25556 ( 97%)], Train Loss: 0.49099\n",
      "Epoch: 00 [24844/25556 ( 97%)], Train Loss: 0.49095\n",
      "Epoch: 00 [24884/25556 ( 97%)], Train Loss: 0.49049\n",
      "Epoch: 00 [24924/25556 ( 98%)], Train Loss: 0.49010\n",
      "Epoch: 00 [24964/25556 ( 98%)], Train Loss: 0.48980\n",
      "Epoch: 00 [25004/25556 ( 98%)], Train Loss: 0.48935\n",
      "Epoch: 00 [25044/25556 ( 98%)], Train Loss: 0.48908\n",
      "Epoch: 00 [25084/25556 ( 98%)], Train Loss: 0.48903\n",
      "Epoch: 00 [25124/25556 ( 98%)], Train Loss: 0.48884\n",
      "Epoch: 00 [25164/25556 ( 98%)], Train Loss: 0.48871\n",
      "Epoch: 00 [25204/25556 ( 99%)], Train Loss: 0.48866\n",
      "Epoch: 00 [25244/25556 ( 99%)], Train Loss: 0.48821\n",
      "Epoch: 00 [25284/25556 ( 99%)], Train Loss: 0.48809\n",
      "Epoch: 00 [25324/25556 ( 99%)], Train Loss: 0.48785\n",
      "Epoch: 00 [25364/25556 ( 99%)], Train Loss: 0.48764\n",
      "Epoch: 00 [25404/25556 ( 99%)], Train Loss: 0.48759\n",
      "Epoch: 00 [25444/25556 (100%)], Train Loss: 0.48759\n",
      "Epoch: 00 [25484/25556 (100%)], Train Loss: 0.48733\n",
      "Epoch: 00 [25524/25556 (100%)], Train Loss: 0.48709\n",
      "Epoch: 00 [25556/25556 (100%)], Train Loss: 0.48720\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] Valid Loss: 0.21004\n",
      "Post-processing 223 example predictions split into 2944 features.\n",
      "valid jaccard:  0.6792707666026053\n",
      "0 Epoch, Best epoch was updated! Valid Loss: 0.21004\n",
      "Saving model checkpoint to output/checkpoint-fold-3-epoch-0.\n",
      "\n",
      "Total Training Time: 3759.957547903061secs, Average Training Time per Epoch: 3759.957547903061secs.\n",
      "Total Validation Time: 141.084636926651secs, Average Validation Time per Epoch: 141.084636926651secs.\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "FOLD: 4\n",
      "--------------------------------------------------\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Num examples Train= 25568, Num examples Valid=2932\n",
      "Total Training Steps: 3196, Total Warmup Steps: 319\n",
      "Epoch: 00 [    4/25568 (  0%)], Train Loss: 3.40332\n",
      "Epoch: 00 [   44/25568 (  0%)], Train Loss: 3.26017\n",
      "Epoch: 00 [   84/25568 (  0%)], Train Loss: 3.26337\n",
      "Epoch: 00 [  124/25568 (  0%)], Train Loss: 3.23496\n",
      "Epoch: 00 [  164/25568 (  1%)], Train Loss: 3.19414\n",
      "Epoch: 00 [  204/25568 (  1%)], Train Loss: 3.14809\n",
      "Epoch: 00 [  244/25568 (  1%)], Train Loss: 3.08096\n",
      "Epoch: 00 [  284/25568 (  1%)], Train Loss: 2.99897\n",
      "Epoch: 00 [  324/25568 (  1%)], Train Loss: 2.92017\n",
      "Epoch: 00 [  364/25568 (  1%)], Train Loss: 2.81433\n",
      "Epoch: 00 [  404/25568 (  2%)], Train Loss: 2.68277\n",
      "Epoch: 00 [  444/25568 (  2%)], Train Loss: 2.56041\n",
      "Epoch: 00 [  484/25568 (  2%)], Train Loss: 2.41492\n",
      "Epoch: 00 [  524/25568 (  2%)], Train Loss: 2.28793\n",
      "Epoch: 00 [  564/25568 (  2%)], Train Loss: 2.18793\n",
      "Epoch: 00 [  604/25568 (  2%)], Train Loss: 2.09279\n",
      "Epoch: 00 [  644/25568 (  3%)], Train Loss: 1.99546\n",
      "Epoch: 00 [  684/25568 (  3%)], Train Loss: 1.90980\n",
      "Epoch: 00 [  724/25568 (  3%)], Train Loss: 1.82842\n",
      "Epoch: 00 [  764/25568 (  3%)], Train Loss: 1.77230\n",
      "Epoch: 00 [  804/25568 (  3%)], Train Loss: 1.71666\n",
      "Epoch: 00 [  844/25568 (  3%)], Train Loss: 1.65501\n",
      "Epoch: 00 [  884/25568 (  3%)], Train Loss: 1.60776\n",
      "Epoch: 00 [  924/25568 (  4%)], Train Loss: 1.56171\n",
      "Epoch: 00 [  964/25568 (  4%)], Train Loss: 1.52044\n",
      "Epoch: 00 [ 1004/25568 (  4%)], Train Loss: 1.47808\n",
      "Epoch: 00 [ 1044/25568 (  4%)], Train Loss: 1.45143\n",
      "Epoch: 00 [ 1084/25568 (  4%)], Train Loss: 1.42627\n",
      "Epoch: 00 [ 1124/25568 (  4%)], Train Loss: 1.39472\n",
      "Epoch: 00 [ 1164/25568 (  5%)], Train Loss: 1.37144\n",
      "Epoch: 00 [ 1204/25568 (  5%)], Train Loss: 1.35142\n",
      "Epoch: 00 [ 1244/25568 (  5%)], Train Loss: 1.32860\n",
      "Epoch: 00 [ 1284/25568 (  5%)], Train Loss: 1.30219\n",
      "Epoch: 00 [ 1324/25568 (  5%)], Train Loss: 1.27996\n",
      "Epoch: 00 [ 1364/25568 (  5%)], Train Loss: 1.26045\n",
      "Epoch: 00 [ 1404/25568 (  5%)], Train Loss: 1.23748\n",
      "Epoch: 00 [ 1444/25568 (  6%)], Train Loss: 1.21720\n",
      "Epoch: 00 [ 1484/25568 (  6%)], Train Loss: 1.19902\n",
      "Epoch: 00 [ 1524/25568 (  6%)], Train Loss: 1.18442\n",
      "Epoch: 00 [ 1564/25568 (  6%)], Train Loss: 1.16761\n",
      "Epoch: 00 [ 1604/25568 (  6%)], Train Loss: 1.15270\n",
      "Epoch: 00 [ 1644/25568 (  6%)], Train Loss: 1.13600\n",
      "Epoch: 00 [ 1684/25568 (  7%)], Train Loss: 1.11886\n",
      "Epoch: 00 [ 1724/25568 (  7%)], Train Loss: 1.10006\n",
      "Epoch: 00 [ 1764/25568 (  7%)], Train Loss: 1.08491\n",
      "Epoch: 00 [ 1804/25568 (  7%)], Train Loss: 1.07232\n",
      "Epoch: 00 [ 1844/25568 (  7%)], Train Loss: 1.05973\n",
      "Epoch: 00 [ 1884/25568 (  7%)], Train Loss: 1.05255\n",
      "Epoch: 00 [ 1924/25568 (  8%)], Train Loss: 1.04338\n",
      "Epoch: 00 [ 1964/25568 (  8%)], Train Loss: 1.03472\n",
      "Epoch: 00 [ 2004/25568 (  8%)], Train Loss: 1.02527\n",
      "Epoch: 00 [ 2044/25568 (  8%)], Train Loss: 1.01675\n",
      "Epoch: 00 [ 2084/25568 (  8%)], Train Loss: 1.00708\n",
      "Epoch: 00 [ 2124/25568 (  8%)], Train Loss: 0.99984\n",
      "Epoch: 00 [ 2164/25568 (  8%)], Train Loss: 0.99444\n",
      "Epoch: 00 [ 2204/25568 (  9%)], Train Loss: 0.98531\n",
      "Epoch: 00 [ 2244/25568 (  9%)], Train Loss: 0.97716\n",
      "Epoch: 00 [ 2284/25568 (  9%)], Train Loss: 0.97270\n",
      "Epoch: 00 [ 2324/25568 (  9%)], Train Loss: 0.96411\n",
      "Epoch: 00 [ 2364/25568 (  9%)], Train Loss: 0.95841\n",
      "Epoch: 00 [ 2404/25568 (  9%)], Train Loss: 0.95431\n",
      "Epoch: 00 [ 2444/25568 ( 10%)], Train Loss: 0.94440\n",
      "Epoch: 00 [ 2484/25568 ( 10%)], Train Loss: 0.93735\n",
      "Epoch: 00 [ 2524/25568 ( 10%)], Train Loss: 0.93092\n",
      "Epoch: 00 [ 2564/25568 ( 10%)], Train Loss: 0.92445\n",
      "Epoch: 00 [ 2604/25568 ( 10%)], Train Loss: 0.91931\n",
      "Epoch: 00 [ 2644/25568 ( 10%)], Train Loss: 0.91483\n",
      "Epoch: 00 [ 2684/25568 ( 10%)], Train Loss: 0.91267\n",
      "Epoch: 00 [ 2724/25568 ( 11%)], Train Loss: 0.90739\n",
      "Epoch: 00 [ 2764/25568 ( 11%)], Train Loss: 0.90134\n",
      "Epoch: 00 [ 2804/25568 ( 11%)], Train Loss: 0.89625\n",
      "Epoch: 00 [ 2844/25568 ( 11%)], Train Loss: 0.89524\n",
      "Epoch: 00 [ 2884/25568 ( 11%)], Train Loss: 0.88949\n",
      "Epoch: 00 [ 2924/25568 ( 11%)], Train Loss: 0.88180\n",
      "Epoch: 00 [ 2964/25568 ( 12%)], Train Loss: 0.87647\n",
      "Epoch: 00 [ 3004/25568 ( 12%)], Train Loss: 0.87558\n",
      "Epoch: 00 [ 3044/25568 ( 12%)], Train Loss: 0.87193\n",
      "Epoch: 00 [ 3084/25568 ( 12%)], Train Loss: 0.86783\n",
      "Epoch: 00 [ 3124/25568 ( 12%)], Train Loss: 0.86296\n",
      "Epoch: 00 [ 3164/25568 ( 12%)], Train Loss: 0.85955\n",
      "Epoch: 00 [ 3204/25568 ( 13%)], Train Loss: 0.85394\n",
      "Epoch: 00 [ 3244/25568 ( 13%)], Train Loss: 0.84770\n",
      "Epoch: 00 [ 3284/25568 ( 13%)], Train Loss: 0.84407\n",
      "Epoch: 00 [ 3324/25568 ( 13%)], Train Loss: 0.84045\n",
      "Epoch: 00 [ 3364/25568 ( 13%)], Train Loss: 0.83717\n",
      "Epoch: 00 [ 3404/25568 ( 13%)], Train Loss: 0.83200\n",
      "Epoch: 00 [ 3444/25568 ( 13%)], Train Loss: 0.83076\n",
      "Epoch: 00 [ 3484/25568 ( 14%)], Train Loss: 0.82636\n",
      "Epoch: 00 [ 3524/25568 ( 14%)], Train Loss: 0.82366\n",
      "Epoch: 00 [ 3564/25568 ( 14%)], Train Loss: 0.82135\n",
      "Epoch: 00 [ 3604/25568 ( 14%)], Train Loss: 0.82003\n",
      "Epoch: 00 [ 3644/25568 ( 14%)], Train Loss: 0.81762\n",
      "Epoch: 00 [ 3684/25568 ( 14%)], Train Loss: 0.81605\n",
      "Epoch: 00 [ 3724/25568 ( 15%)], Train Loss: 0.81490\n",
      "Epoch: 00 [ 3764/25568 ( 15%)], Train Loss: 0.81407\n",
      "Epoch: 00 [ 3804/25568 ( 15%)], Train Loss: 0.81020\n",
      "Epoch: 00 [ 3844/25568 ( 15%)], Train Loss: 0.81078\n",
      "Epoch: 00 [ 3884/25568 ( 15%)], Train Loss: 0.80736\n",
      "Epoch: 00 [ 3924/25568 ( 15%)], Train Loss: 0.80741\n",
      "Epoch: 00 [ 3964/25568 ( 16%)], Train Loss: 0.80443\n",
      "Epoch: 00 [ 4004/25568 ( 16%)], Train Loss: 0.80186\n",
      "Epoch: 00 [ 4044/25568 ( 16%)], Train Loss: 0.79800\n",
      "Epoch: 00 [ 4084/25568 ( 16%)], Train Loss: 0.79349\n",
      "Epoch: 00 [ 4124/25568 ( 16%)], Train Loss: 0.78988\n",
      "Epoch: 00 [ 4164/25568 ( 16%)], Train Loss: 0.78798\n",
      "Epoch: 00 [ 4204/25568 ( 16%)], Train Loss: 0.78559\n",
      "Epoch: 00 [ 4244/25568 ( 17%)], Train Loss: 0.78311\n",
      "Epoch: 00 [ 4284/25568 ( 17%)], Train Loss: 0.78125\n",
      "Epoch: 00 [ 4324/25568 ( 17%)], Train Loss: 0.78073\n",
      "Epoch: 00 [ 4364/25568 ( 17%)], Train Loss: 0.77701\n",
      "Epoch: 00 [ 4404/25568 ( 17%)], Train Loss: 0.77623\n",
      "Epoch: 00 [ 4444/25568 ( 17%)], Train Loss: 0.77371\n",
      "Epoch: 00 [ 4484/25568 ( 18%)], Train Loss: 0.77299\n",
      "Epoch: 00 [ 4524/25568 ( 18%)], Train Loss: 0.77206\n",
      "Epoch: 00 [ 4564/25568 ( 18%)], Train Loss: 0.76947\n",
      "Epoch: 00 [ 4604/25568 ( 18%)], Train Loss: 0.76629\n",
      "Epoch: 00 [ 4644/25568 ( 18%)], Train Loss: 0.76343\n",
      "Epoch: 00 [ 4684/25568 ( 18%)], Train Loss: 0.76054\n",
      "Epoch: 00 [ 4724/25568 ( 18%)], Train Loss: 0.75988\n",
      "Epoch: 00 [ 4764/25568 ( 19%)], Train Loss: 0.75783\n",
      "Epoch: 00 [ 4804/25568 ( 19%)], Train Loss: 0.75628\n",
      "Epoch: 00 [ 4844/25568 ( 19%)], Train Loss: 0.75574\n",
      "Epoch: 00 [ 4884/25568 ( 19%)], Train Loss: 0.75278\n",
      "Epoch: 00 [ 4924/25568 ( 19%)], Train Loss: 0.74945\n",
      "Epoch: 00 [ 4964/25568 ( 19%)], Train Loss: 0.74692\n",
      "Epoch: 00 [ 5004/25568 ( 20%)], Train Loss: 0.74886\n",
      "Epoch: 00 [ 5044/25568 ( 20%)], Train Loss: 0.74882\n",
      "Epoch: 00 [ 5084/25568 ( 20%)], Train Loss: 0.74885\n",
      "Epoch: 00 [ 5124/25568 ( 20%)], Train Loss: 0.74648\n",
      "Epoch: 00 [ 5164/25568 ( 20%)], Train Loss: 0.74348\n",
      "Epoch: 00 [ 5204/25568 ( 20%)], Train Loss: 0.74134\n",
      "Epoch: 00 [ 5244/25568 ( 21%)], Train Loss: 0.74058\n",
      "Epoch: 00 [ 5284/25568 ( 21%)], Train Loss: 0.73877\n",
      "Epoch: 00 [ 5324/25568 ( 21%)], Train Loss: 0.73802\n",
      "Epoch: 00 [ 5364/25568 ( 21%)], Train Loss: 0.73497\n",
      "Epoch: 00 [ 5404/25568 ( 21%)], Train Loss: 0.73311\n",
      "Epoch: 00 [ 5444/25568 ( 21%)], Train Loss: 0.73055\n",
      "Epoch: 00 [ 5484/25568 ( 21%)], Train Loss: 0.72993\n",
      "Epoch: 00 [ 5524/25568 ( 22%)], Train Loss: 0.72891\n",
      "Epoch: 00 [ 5564/25568 ( 22%)], Train Loss: 0.72775\n",
      "Epoch: 00 [ 5604/25568 ( 22%)], Train Loss: 0.72658\n",
      "Epoch: 00 [ 5644/25568 ( 22%)], Train Loss: 0.72577\n",
      "Epoch: 00 [ 5684/25568 ( 22%)], Train Loss: 0.72370\n",
      "Epoch: 00 [ 5724/25568 ( 22%)], Train Loss: 0.72190\n",
      "Epoch: 00 [ 5764/25568 ( 23%)], Train Loss: 0.72071\n",
      "Epoch: 00 [ 5804/25568 ( 23%)], Train Loss: 0.71783\n",
      "Epoch: 00 [ 5844/25568 ( 23%)], Train Loss: 0.71595\n",
      "Epoch: 00 [ 5884/25568 ( 23%)], Train Loss: 0.71431\n",
      "Epoch: 00 [ 5924/25568 ( 23%)], Train Loss: 0.71298\n",
      "Epoch: 00 [ 5964/25568 ( 23%)], Train Loss: 0.71108\n",
      "Epoch: 00 [ 6004/25568 ( 23%)], Train Loss: 0.70891\n",
      "Epoch: 00 [ 6044/25568 ( 24%)], Train Loss: 0.70770\n",
      "Epoch: 00 [ 6084/25568 ( 24%)], Train Loss: 0.70568\n",
      "Epoch: 00 [ 6124/25568 ( 24%)], Train Loss: 0.70563\n",
      "Epoch: 00 [ 6164/25568 ( 24%)], Train Loss: 0.70462\n",
      "Epoch: 00 [ 6204/25568 ( 24%)], Train Loss: 0.70453\n",
      "Epoch: 00 [ 6244/25568 ( 24%)], Train Loss: 0.70283\n",
      "Epoch: 00 [ 6284/25568 ( 25%)], Train Loss: 0.70195\n",
      "Epoch: 00 [ 6324/25568 ( 25%)], Train Loss: 0.70153\n",
      "Epoch: 00 [ 6364/25568 ( 25%)], Train Loss: 0.69983\n",
      "Epoch: 00 [ 6404/25568 ( 25%)], Train Loss: 0.69830\n",
      "Epoch: 00 [ 6444/25568 ( 25%)], Train Loss: 0.69846\n",
      "Epoch: 00 [ 6484/25568 ( 25%)], Train Loss: 0.69735\n",
      "Epoch: 00 [ 6524/25568 ( 26%)], Train Loss: 0.69571\n",
      "Epoch: 00 [ 6564/25568 ( 26%)], Train Loss: 0.69397\n",
      "Epoch: 00 [ 6604/25568 ( 26%)], Train Loss: 0.69198\n",
      "Epoch: 00 [ 6644/25568 ( 26%)], Train Loss: 0.68959\n",
      "Epoch: 00 [ 6684/25568 ( 26%)], Train Loss: 0.69146\n",
      "Epoch: 00 [ 6724/25568 ( 26%)], Train Loss: 0.69148\n",
      "Epoch: 00 [ 6764/25568 ( 26%)], Train Loss: 0.68926\n",
      "Epoch: 00 [ 6804/25568 ( 27%)], Train Loss: 0.68813\n",
      "Epoch: 00 [ 6844/25568 ( 27%)], Train Loss: 0.68724\n",
      "Epoch: 00 [ 6884/25568 ( 27%)], Train Loss: 0.68577\n",
      "Epoch: 00 [ 6924/25568 ( 27%)], Train Loss: 0.68385\n",
      "Epoch: 00 [ 6964/25568 ( 27%)], Train Loss: 0.68369\n",
      "Epoch: 00 [ 7004/25568 ( 27%)], Train Loss: 0.68285\n",
      "Epoch: 00 [ 7044/25568 ( 28%)], Train Loss: 0.68281\n",
      "Epoch: 00 [ 7084/25568 ( 28%)], Train Loss: 0.68185\n",
      "Epoch: 00 [ 7124/25568 ( 28%)], Train Loss: 0.68002\n",
      "Epoch: 00 [ 7164/25568 ( 28%)], Train Loss: 0.67911\n",
      "Epoch: 00 [ 7204/25568 ( 28%)], Train Loss: 0.67714\n",
      "Epoch: 00 [ 7244/25568 ( 28%)], Train Loss: 0.67578\n",
      "Epoch: 00 [ 7284/25568 ( 28%)], Train Loss: 0.67569\n",
      "Epoch: 00 [ 7324/25568 ( 29%)], Train Loss: 0.67622\n",
      "Epoch: 00 [ 7364/25568 ( 29%)], Train Loss: 0.67501\n",
      "Epoch: 00 [ 7404/25568 ( 29%)], Train Loss: 0.67329\n",
      "Epoch: 00 [ 7444/25568 ( 29%)], Train Loss: 0.67215\n",
      "Epoch: 00 [ 7484/25568 ( 29%)], Train Loss: 0.67207\n",
      "Epoch: 00 [ 7524/25568 ( 29%)], Train Loss: 0.67077\n",
      "Epoch: 00 [ 7564/25568 ( 30%)], Train Loss: 0.66925\n",
      "Epoch: 00 [ 7604/25568 ( 30%)], Train Loss: 0.66797\n",
      "Epoch: 00 [ 7644/25568 ( 30%)], Train Loss: 0.66776\n",
      "Epoch: 00 [ 7684/25568 ( 30%)], Train Loss: 0.66703\n",
      "Epoch: 00 [ 7724/25568 ( 30%)], Train Loss: 0.66520\n",
      "Epoch: 00 [ 7764/25568 ( 30%)], Train Loss: 0.66497\n",
      "Epoch: 00 [ 7804/25568 ( 31%)], Train Loss: 0.66416\n",
      "Epoch: 00 [ 7844/25568 ( 31%)], Train Loss: 0.66349\n",
      "Epoch: 00 [ 7884/25568 ( 31%)], Train Loss: 0.66242\n",
      "Epoch: 00 [ 7924/25568 ( 31%)], Train Loss: 0.66221\n",
      "Epoch: 00 [ 7964/25568 ( 31%)], Train Loss: 0.66219\n",
      "Epoch: 00 [ 8004/25568 ( 31%)], Train Loss: 0.66204\n",
      "Epoch: 00 [ 8044/25568 ( 31%)], Train Loss: 0.66060\n",
      "Epoch: 00 [ 8084/25568 ( 32%)], Train Loss: 0.65950\n",
      "Epoch: 00 [ 8124/25568 ( 32%)], Train Loss: 0.65808\n",
      "Epoch: 00 [ 8164/25568 ( 32%)], Train Loss: 0.65742\n",
      "Epoch: 00 [ 8204/25568 ( 32%)], Train Loss: 0.65653\n",
      "Epoch: 00 [ 8244/25568 ( 32%)], Train Loss: 0.65689\n",
      "Epoch: 00 [ 8284/25568 ( 32%)], Train Loss: 0.65602\n",
      "Epoch: 00 [ 8324/25568 ( 33%)], Train Loss: 0.65404\n",
      "Epoch: 00 [ 8364/25568 ( 33%)], Train Loss: 0.65313\n",
      "Epoch: 00 [ 8404/25568 ( 33%)], Train Loss: 0.65182\n",
      "Epoch: 00 [ 8444/25568 ( 33%)], Train Loss: 0.65115\n",
      "Epoch: 00 [ 8484/25568 ( 33%)], Train Loss: 0.64967\n",
      "Epoch: 00 [ 8524/25568 ( 33%)], Train Loss: 0.64938\n",
      "Epoch: 00 [ 8564/25568 ( 33%)], Train Loss: 0.64796\n",
      "Epoch: 00 [ 8604/25568 ( 34%)], Train Loss: 0.64861\n",
      "Epoch: 00 [ 8644/25568 ( 34%)], Train Loss: 0.64731\n",
      "Epoch: 00 [ 8684/25568 ( 34%)], Train Loss: 0.64614\n",
      "Epoch: 00 [ 8724/25568 ( 34%)], Train Loss: 0.64535\n",
      "Epoch: 00 [ 8764/25568 ( 34%)], Train Loss: 0.64610\n",
      "Epoch: 00 [ 8804/25568 ( 34%)], Train Loss: 0.64674\n",
      "Epoch: 00 [ 8844/25568 ( 35%)], Train Loss: 0.64606\n",
      "Epoch: 00 [ 8884/25568 ( 35%)], Train Loss: 0.64607\n",
      "Epoch: 00 [ 8924/25568 ( 35%)], Train Loss: 0.64610\n",
      "Epoch: 00 [ 8964/25568 ( 35%)], Train Loss: 0.64536\n",
      "Epoch: 00 [ 9004/25568 ( 35%)], Train Loss: 0.64509\n",
      "Epoch: 00 [ 9044/25568 ( 35%)], Train Loss: 0.64462\n",
      "Epoch: 00 [ 9084/25568 ( 36%)], Train Loss: 0.64347\n",
      "Epoch: 00 [ 9124/25568 ( 36%)], Train Loss: 0.64245\n",
      "Epoch: 00 [ 9164/25568 ( 36%)], Train Loss: 0.64092\n",
      "Epoch: 00 [ 9204/25568 ( 36%)], Train Loss: 0.64049\n",
      "Epoch: 00 [ 9244/25568 ( 36%)], Train Loss: 0.63946\n",
      "Epoch: 00 [ 9284/25568 ( 36%)], Train Loss: 0.63847\n",
      "Epoch: 00 [ 9324/25568 ( 36%)], Train Loss: 0.63688\n",
      "Epoch: 00 [ 9364/25568 ( 37%)], Train Loss: 0.63539\n",
      "Epoch: 00 [ 9404/25568 ( 37%)], Train Loss: 0.63416\n",
      "Epoch: 00 [ 9444/25568 ( 37%)], Train Loss: 0.63402\n",
      "Epoch: 00 [ 9484/25568 ( 37%)], Train Loss: 0.63334\n",
      "Epoch: 00 [ 9524/25568 ( 37%)], Train Loss: 0.63208\n",
      "Epoch: 00 [ 9564/25568 ( 37%)], Train Loss: 0.63156\n",
      "Epoch: 00 [ 9604/25568 ( 38%)], Train Loss: 0.63018\n",
      "Epoch: 00 [ 9644/25568 ( 38%)], Train Loss: 0.63001\n",
      "Epoch: 00 [ 9684/25568 ( 38%)], Train Loss: 0.62977\n",
      "Epoch: 00 [ 9724/25568 ( 38%)], Train Loss: 0.62867\n",
      "Epoch: 00 [ 9764/25568 ( 38%)], Train Loss: 0.62822\n",
      "Epoch: 00 [ 9804/25568 ( 38%)], Train Loss: 0.62823\n",
      "Epoch: 00 [ 9844/25568 ( 39%)], Train Loss: 0.62758\n",
      "Epoch: 00 [ 9884/25568 ( 39%)], Train Loss: 0.62732\n",
      "Epoch: 00 [ 9924/25568 ( 39%)], Train Loss: 0.62659\n",
      "Epoch: 00 [ 9964/25568 ( 39%)], Train Loss: 0.62599\n",
      "Epoch: 00 [10004/25568 ( 39%)], Train Loss: 0.62534\n",
      "Epoch: 00 [10044/25568 ( 39%)], Train Loss: 0.62516\n",
      "Epoch: 00 [10084/25568 ( 39%)], Train Loss: 0.62481\n",
      "Epoch: 00 [10124/25568 ( 40%)], Train Loss: 0.62447\n",
      "Epoch: 00 [10164/25568 ( 40%)], Train Loss: 0.62327\n",
      "Epoch: 00 [10204/25568 ( 40%)], Train Loss: 0.62272\n",
      "Epoch: 00 [10244/25568 ( 40%)], Train Loss: 0.62217\n",
      "Epoch: 00 [10284/25568 ( 40%)], Train Loss: 0.62128\n",
      "Epoch: 00 [10324/25568 ( 40%)], Train Loss: 0.62131\n",
      "Epoch: 00 [10364/25568 ( 41%)], Train Loss: 0.62075\n",
      "Epoch: 00 [10404/25568 ( 41%)], Train Loss: 0.62066\n",
      "Epoch: 00 [10444/25568 ( 41%)], Train Loss: 0.62015\n",
      "Epoch: 00 [10484/25568 ( 41%)], Train Loss: 0.61975\n",
      "Epoch: 00 [10524/25568 ( 41%)], Train Loss: 0.61875\n",
      "Epoch: 00 [10564/25568 ( 41%)], Train Loss: 0.61751\n",
      "Epoch: 00 [10604/25568 ( 41%)], Train Loss: 0.61724\n",
      "Epoch: 00 [10644/25568 ( 42%)], Train Loss: 0.61662\n",
      "Epoch: 00 [10684/25568 ( 42%)], Train Loss: 0.61555\n",
      "Epoch: 00 [10724/25568 ( 42%)], Train Loss: 0.61488\n",
      "Epoch: 00 [10764/25568 ( 42%)], Train Loss: 0.61433\n",
      "Epoch: 00 [10804/25568 ( 42%)], Train Loss: 0.61396\n",
      "Epoch: 00 [10844/25568 ( 42%)], Train Loss: 0.61245\n",
      "Epoch: 00 [10884/25568 ( 43%)], Train Loss: 0.61125\n",
      "Epoch: 00 [10924/25568 ( 43%)], Train Loss: 0.61085\n",
      "Epoch: 00 [10964/25568 ( 43%)], Train Loss: 0.61013\n",
      "Epoch: 00 [11004/25568 ( 43%)], Train Loss: 0.61005\n",
      "Epoch: 00 [11044/25568 ( 43%)], Train Loss: 0.60936\n",
      "Epoch: 00 [11084/25568 ( 43%)], Train Loss: 0.60868\n",
      "Epoch: 00 [11124/25568 ( 44%)], Train Loss: 0.60866\n",
      "Epoch: 00 [11164/25568 ( 44%)], Train Loss: 0.60795\n",
      "Epoch: 00 [11204/25568 ( 44%)], Train Loss: 0.60701\n",
      "Epoch: 00 [11244/25568 ( 44%)], Train Loss: 0.60684\n",
      "Epoch: 00 [11284/25568 ( 44%)], Train Loss: 0.60585\n",
      "Epoch: 00 [11324/25568 ( 44%)], Train Loss: 0.60467\n",
      "Epoch: 00 [11364/25568 ( 44%)], Train Loss: 0.60359\n",
      "Epoch: 00 [11404/25568 ( 45%)], Train Loss: 0.60380\n",
      "Epoch: 00 [11444/25568 ( 45%)], Train Loss: 0.60294\n",
      "Epoch: 00 [11484/25568 ( 45%)], Train Loss: 0.60299\n",
      "Epoch: 00 [11524/25568 ( 45%)], Train Loss: 0.60208\n",
      "Epoch: 00 [11564/25568 ( 45%)], Train Loss: 0.60107\n",
      "Epoch: 00 [11604/25568 ( 45%)], Train Loss: 0.59998\n",
      "Epoch: 00 [11644/25568 ( 46%)], Train Loss: 0.59938\n",
      "Epoch: 00 [11684/25568 ( 46%)], Train Loss: 0.59848\n",
      "Epoch: 00 [11724/25568 ( 46%)], Train Loss: 0.59777\n",
      "Epoch: 00 [11764/25568 ( 46%)], Train Loss: 0.59767\n",
      "Epoch: 00 [11804/25568 ( 46%)], Train Loss: 0.59687\n",
      "Epoch: 00 [11844/25568 ( 46%)], Train Loss: 0.59605\n",
      "Epoch: 00 [11884/25568 ( 46%)], Train Loss: 0.59519\n",
      "Epoch: 00 [11924/25568 ( 47%)], Train Loss: 0.59514\n",
      "Epoch: 00 [11964/25568 ( 47%)], Train Loss: 0.59446\n",
      "Epoch: 00 [12004/25568 ( 47%)], Train Loss: 0.59451\n",
      "Epoch: 00 [12044/25568 ( 47%)], Train Loss: 0.59397\n",
      "Epoch: 00 [12084/25568 ( 47%)], Train Loss: 0.59350\n",
      "Epoch: 00 [12124/25568 ( 47%)], Train Loss: 0.59330\n",
      "Epoch: 00 [12164/25568 ( 48%)], Train Loss: 0.59229\n",
      "Epoch: 00 [12204/25568 ( 48%)], Train Loss: 0.59220\n",
      "Epoch: 00 [12244/25568 ( 48%)], Train Loss: 0.59192\n",
      "Epoch: 00 [12284/25568 ( 48%)], Train Loss: 0.59138\n",
      "Epoch: 00 [12324/25568 ( 48%)], Train Loss: 0.59070\n",
      "Epoch: 00 [12364/25568 ( 48%)], Train Loss: 0.59000\n",
      "Epoch: 00 [12404/25568 ( 49%)], Train Loss: 0.58974\n",
      "Epoch: 00 [12444/25568 ( 49%)], Train Loss: 0.58953\n",
      "Epoch: 00 [12484/25568 ( 49%)], Train Loss: 0.58881\n",
      "Epoch: 00 [12524/25568 ( 49%)], Train Loss: 0.58814\n",
      "Epoch: 00 [12564/25568 ( 49%)], Train Loss: 0.58743\n",
      "Epoch: 00 [12604/25568 ( 49%)], Train Loss: 0.58631\n",
      "Epoch: 00 [12644/25568 ( 49%)], Train Loss: 0.58582\n",
      "Epoch: 00 [12684/25568 ( 50%)], Train Loss: 0.58512\n",
      "Epoch: 00 [12724/25568 ( 50%)], Train Loss: 0.58469\n",
      "Epoch: 00 [12764/25568 ( 50%)], Train Loss: 0.58461\n",
      "Epoch: 00 [12804/25568 ( 50%)], Train Loss: 0.58401\n",
      "Epoch: 00 [12844/25568 ( 50%)], Train Loss: 0.58366\n",
      "Epoch: 00 [12884/25568 ( 50%)], Train Loss: 0.58337\n",
      "Epoch: 00 [12924/25568 ( 51%)], Train Loss: 0.58276\n",
      "Epoch: 00 [12964/25568 ( 51%)], Train Loss: 0.58227\n",
      "Epoch: 00 [13004/25568 ( 51%)], Train Loss: 0.58194\n",
      "Epoch: 00 [13044/25568 ( 51%)], Train Loss: 0.58151\n",
      "Epoch: 00 [13084/25568 ( 51%)], Train Loss: 0.58082\n",
      "Epoch: 00 [13124/25568 ( 51%)], Train Loss: 0.58035\n",
      "Epoch: 00 [13164/25568 ( 51%)], Train Loss: 0.58047\n",
      "Epoch: 00 [13204/25568 ( 52%)], Train Loss: 0.57977\n",
      "Epoch: 00 [13244/25568 ( 52%)], Train Loss: 0.57888\n",
      "Epoch: 00 [13284/25568 ( 52%)], Train Loss: 0.57791\n",
      "Epoch: 00 [13324/25568 ( 52%)], Train Loss: 0.57715\n",
      "Epoch: 00 [13364/25568 ( 52%)], Train Loss: 0.57649\n",
      "Epoch: 00 [13404/25568 ( 52%)], Train Loss: 0.57561\n",
      "Epoch: 00 [13444/25568 ( 53%)], Train Loss: 0.57475\n",
      "Epoch: 00 [13484/25568 ( 53%)], Train Loss: 0.57497\n",
      "Epoch: 00 [13524/25568 ( 53%)], Train Loss: 0.57453\n",
      "Epoch: 00 [13564/25568 ( 53%)], Train Loss: 0.57401\n",
      "Epoch: 00 [13604/25568 ( 53%)], Train Loss: 0.57384\n",
      "Epoch: 00 [13644/25568 ( 53%)], Train Loss: 0.57295\n",
      "Epoch: 00 [13684/25568 ( 54%)], Train Loss: 0.57234\n",
      "Epoch: 00 [13724/25568 ( 54%)], Train Loss: 0.57235\n",
      "Epoch: 00 [13764/25568 ( 54%)], Train Loss: 0.57120\n",
      "Epoch: 00 [13804/25568 ( 54%)], Train Loss: 0.57136\n",
      "Epoch: 00 [13844/25568 ( 54%)], Train Loss: 0.57103\n",
      "Epoch: 00 [13884/25568 ( 54%)], Train Loss: 0.57032\n",
      "Epoch: 00 [13924/25568 ( 54%)], Train Loss: 0.56945\n",
      "Epoch: 00 [13964/25568 ( 55%)], Train Loss: 0.56892\n",
      "Epoch: 00 [14004/25568 ( 55%)], Train Loss: 0.56846\n",
      "Epoch: 00 [14044/25568 ( 55%)], Train Loss: 0.56786\n",
      "Epoch: 00 [14084/25568 ( 55%)], Train Loss: 0.56676\n",
      "Epoch: 00 [14124/25568 ( 55%)], Train Loss: 0.56709\n",
      "Epoch: 00 [14164/25568 ( 55%)], Train Loss: 0.56695\n",
      "Epoch: 00 [14204/25568 ( 56%)], Train Loss: 0.56658\n",
      "Epoch: 00 [14244/25568 ( 56%)], Train Loss: 0.56683\n",
      "Epoch: 00 [14284/25568 ( 56%)], Train Loss: 0.56671\n",
      "Epoch: 00 [14324/25568 ( 56%)], Train Loss: 0.56585\n",
      "Epoch: 00 [14364/25568 ( 56%)], Train Loss: 0.56507\n",
      "Epoch: 00 [14404/25568 ( 56%)], Train Loss: 0.56429\n",
      "Epoch: 00 [14444/25568 ( 56%)], Train Loss: 0.56429\n",
      "Epoch: 00 [14484/25568 ( 57%)], Train Loss: 0.56389\n",
      "Epoch: 00 [14524/25568 ( 57%)], Train Loss: 0.56336\n",
      "Epoch: 00 [14564/25568 ( 57%)], Train Loss: 0.56346\n",
      "Epoch: 00 [14604/25568 ( 57%)], Train Loss: 0.56343\n",
      "Epoch: 00 [14644/25568 ( 57%)], Train Loss: 0.56304\n",
      "Epoch: 00 [14684/25568 ( 57%)], Train Loss: 0.56311\n",
      "Epoch: 00 [14724/25568 ( 58%)], Train Loss: 0.56331\n",
      "Epoch: 00 [14764/25568 ( 58%)], Train Loss: 0.56224\n",
      "Epoch: 00 [14804/25568 ( 58%)], Train Loss: 0.56205\n",
      "Epoch: 00 [14844/25568 ( 58%)], Train Loss: 0.56189\n",
      "Epoch: 00 [14884/25568 ( 58%)], Train Loss: 0.56152\n",
      "Epoch: 00 [14924/25568 ( 58%)], Train Loss: 0.56092\n",
      "Epoch: 00 [14964/25568 ( 59%)], Train Loss: 0.56065\n",
      "Epoch: 00 [15004/25568 ( 59%)], Train Loss: 0.55986\n",
      "Epoch: 00 [15044/25568 ( 59%)], Train Loss: 0.55930\n",
      "Epoch: 00 [15084/25568 ( 59%)], Train Loss: 0.55855\n",
      "Epoch: 00 [15124/25568 ( 59%)], Train Loss: 0.55757\n",
      "Epoch: 00 [15164/25568 ( 59%)], Train Loss: 0.55762\n",
      "Epoch: 00 [15204/25568 ( 59%)], Train Loss: 0.55748\n",
      "Epoch: 00 [15244/25568 ( 60%)], Train Loss: 0.55666\n",
      "Epoch: 00 [15284/25568 ( 60%)], Train Loss: 0.55650\n",
      "Epoch: 00 [15324/25568 ( 60%)], Train Loss: 0.55662\n",
      "Epoch: 00 [15364/25568 ( 60%)], Train Loss: 0.55584\n",
      "Epoch: 00 [15404/25568 ( 60%)], Train Loss: 0.55600\n",
      "Epoch: 00 [15444/25568 ( 60%)], Train Loss: 0.55537\n",
      "Epoch: 00 [15484/25568 ( 61%)], Train Loss: 0.55505\n",
      "Epoch: 00 [15524/25568 ( 61%)], Train Loss: 0.55465\n",
      "Epoch: 00 [15564/25568 ( 61%)], Train Loss: 0.55397\n",
      "Epoch: 00 [15604/25568 ( 61%)], Train Loss: 0.55364\n",
      "Epoch: 00 [15644/25568 ( 61%)], Train Loss: 0.55322\n",
      "Epoch: 00 [15684/25568 ( 61%)], Train Loss: 0.55291\n",
      "Epoch: 00 [15724/25568 ( 61%)], Train Loss: 0.55191\n",
      "Epoch: 00 [15764/25568 ( 62%)], Train Loss: 0.55141\n",
      "Epoch: 00 [15804/25568 ( 62%)], Train Loss: 0.55082\n",
      "Epoch: 00 [15844/25568 ( 62%)], Train Loss: 0.55053\n",
      "Epoch: 00 [15884/25568 ( 62%)], Train Loss: 0.55025\n",
      "Epoch: 00 [15924/25568 ( 62%)], Train Loss: 0.54940\n",
      "Epoch: 00 [15964/25568 ( 62%)], Train Loss: 0.54880\n",
      "Epoch: 00 [16004/25568 ( 63%)], Train Loss: 0.54805\n",
      "Epoch: 00 [16044/25568 ( 63%)], Train Loss: 0.54805\n",
      "Epoch: 00 [16084/25568 ( 63%)], Train Loss: 0.54787\n",
      "Epoch: 00 [16124/25568 ( 63%)], Train Loss: 0.54754\n",
      "Epoch: 00 [16164/25568 ( 63%)], Train Loss: 0.54692\n",
      "Epoch: 00 [16204/25568 ( 63%)], Train Loss: 0.54662\n",
      "Epoch: 00 [16244/25568 ( 64%)], Train Loss: 0.54605\n",
      "Epoch: 00 [16284/25568 ( 64%)], Train Loss: 0.54629\n",
      "Epoch: 00 [16324/25568 ( 64%)], Train Loss: 0.54600\n",
      "Epoch: 00 [16364/25568 ( 64%)], Train Loss: 0.54568\n",
      "Epoch: 00 [16404/25568 ( 64%)], Train Loss: 0.54516\n",
      "Epoch: 00 [16444/25568 ( 64%)], Train Loss: 0.54450\n",
      "Epoch: 00 [16484/25568 ( 64%)], Train Loss: 0.54434\n",
      "Epoch: 00 [16524/25568 ( 65%)], Train Loss: 0.54351\n",
      "Epoch: 00 [16564/25568 ( 65%)], Train Loss: 0.54281\n",
      "Epoch: 00 [16604/25568 ( 65%)], Train Loss: 0.54299\n",
      "Epoch: 00 [16644/25568 ( 65%)], Train Loss: 0.54281\n",
      "Epoch: 00 [16684/25568 ( 65%)], Train Loss: 0.54255\n",
      "Epoch: 00 [16724/25568 ( 65%)], Train Loss: 0.54240\n",
      "Epoch: 00 [16764/25568 ( 66%)], Train Loss: 0.54165\n",
      "Epoch: 00 [16804/25568 ( 66%)], Train Loss: 0.54122\n",
      "Epoch: 00 [16844/25568 ( 66%)], Train Loss: 0.54077\n",
      "Epoch: 00 [16884/25568 ( 66%)], Train Loss: 0.54066\n",
      "Epoch: 00 [16924/25568 ( 66%)], Train Loss: 0.54049\n",
      "Epoch: 00 [16964/25568 ( 66%)], Train Loss: 0.53980\n",
      "Epoch: 00 [17004/25568 ( 67%)], Train Loss: 0.54038\n",
      "Epoch: 00 [17044/25568 ( 67%)], Train Loss: 0.54020\n",
      "Epoch: 00 [17084/25568 ( 67%)], Train Loss: 0.53999\n",
      "Epoch: 00 [17124/25568 ( 67%)], Train Loss: 0.53957\n",
      "Epoch: 00 [17164/25568 ( 67%)], Train Loss: 0.53925\n",
      "Epoch: 00 [17204/25568 ( 67%)], Train Loss: 0.53873\n",
      "Epoch: 00 [17244/25568 ( 67%)], Train Loss: 0.53858\n",
      "Epoch: 00 [17284/25568 ( 68%)], Train Loss: 0.53840\n",
      "Epoch: 00 [17324/25568 ( 68%)], Train Loss: 0.53808\n",
      "Epoch: 00 [17364/25568 ( 68%)], Train Loss: 0.53784\n",
      "Epoch: 00 [17404/25568 ( 68%)], Train Loss: 0.53752\n",
      "Epoch: 00 [17444/25568 ( 68%)], Train Loss: 0.53688\n",
      "Epoch: 00 [17484/25568 ( 68%)], Train Loss: 0.53604\n",
      "Epoch: 00 [17524/25568 ( 69%)], Train Loss: 0.53604\n",
      "Epoch: 00 [17564/25568 ( 69%)], Train Loss: 0.53584\n",
      "Epoch: 00 [17604/25568 ( 69%)], Train Loss: 0.53608\n",
      "Epoch: 00 [17644/25568 ( 69%)], Train Loss: 0.53607\n",
      "Epoch: 00 [17684/25568 ( 69%)], Train Loss: 0.53601\n",
      "Epoch: 00 [17724/25568 ( 69%)], Train Loss: 0.53579\n",
      "Epoch: 00 [17764/25568 ( 69%)], Train Loss: 0.53511\n",
      "Epoch: 00 [17804/25568 ( 70%)], Train Loss: 0.53499\n",
      "Epoch: 00 [17844/25568 ( 70%)], Train Loss: 0.53475\n",
      "Epoch: 00 [17884/25568 ( 70%)], Train Loss: 0.53427\n",
      "Epoch: 00 [17924/25568 ( 70%)], Train Loss: 0.53402\n",
      "Epoch: 00 [17964/25568 ( 70%)], Train Loss: 0.53405\n",
      "Epoch: 00 [18004/25568 ( 70%)], Train Loss: 0.53355\n",
      "Epoch: 00 [18044/25568 ( 71%)], Train Loss: 0.53344\n",
      "Epoch: 00 [18084/25568 ( 71%)], Train Loss: 0.53311\n",
      "Epoch: 00 [18124/25568 ( 71%)], Train Loss: 0.53251\n",
      "Epoch: 00 [18164/25568 ( 71%)], Train Loss: 0.53276\n",
      "Epoch: 00 [18204/25568 ( 71%)], Train Loss: 0.53219\n",
      "Epoch: 00 [18244/25568 ( 71%)], Train Loss: 0.53186\n",
      "Epoch: 00 [18284/25568 ( 72%)], Train Loss: 0.53137\n",
      "Epoch: 00 [18324/25568 ( 72%)], Train Loss: 0.53130\n",
      "Epoch: 00 [18364/25568 ( 72%)], Train Loss: 0.53141\n",
      "Epoch: 00 [18404/25568 ( 72%)], Train Loss: 0.53126\n",
      "Epoch: 00 [18444/25568 ( 72%)], Train Loss: 0.53080\n",
      "Epoch: 00 [18484/25568 ( 72%)], Train Loss: 0.53078\n",
      "Epoch: 00 [18524/25568 ( 72%)], Train Loss: 0.53048\n",
      "Epoch: 00 [18564/25568 ( 73%)], Train Loss: 0.52993\n",
      "Epoch: 00 [18604/25568 ( 73%)], Train Loss: 0.52948\n",
      "Epoch: 00 [18644/25568 ( 73%)], Train Loss: 0.52910\n",
      "Epoch: 00 [18684/25568 ( 73%)], Train Loss: 0.52891\n",
      "Epoch: 00 [18724/25568 ( 73%)], Train Loss: 0.52868\n",
      "Epoch: 00 [18764/25568 ( 73%)], Train Loss: 0.52836\n",
      "Epoch: 00 [18804/25568 ( 74%)], Train Loss: 0.52816\n",
      "Epoch: 00 [18844/25568 ( 74%)], Train Loss: 0.52798\n",
      "Epoch: 00 [18884/25568 ( 74%)], Train Loss: 0.52757\n",
      "Epoch: 00 [18924/25568 ( 74%)], Train Loss: 0.52757\n",
      "Epoch: 00 [18964/25568 ( 74%)], Train Loss: 0.52705\n",
      "Epoch: 00 [19004/25568 ( 74%)], Train Loss: 0.52685\n",
      "Epoch: 00 [19044/25568 ( 74%)], Train Loss: 0.52677\n",
      "Epoch: 00 [19084/25568 ( 75%)], Train Loss: 0.52660\n",
      "Epoch: 00 [19124/25568 ( 75%)], Train Loss: 0.52626\n",
      "Epoch: 00 [19164/25568 ( 75%)], Train Loss: 0.52606\n",
      "Epoch: 00 [19204/25568 ( 75%)], Train Loss: 0.52595\n",
      "Epoch: 00 [19244/25568 ( 75%)], Train Loss: 0.52551\n",
      "Epoch: 00 [19284/25568 ( 75%)], Train Loss: 0.52540\n",
      "Epoch: 00 [19324/25568 ( 76%)], Train Loss: 0.52520\n",
      "Epoch: 00 [19364/25568 ( 76%)], Train Loss: 0.52445\n",
      "Epoch: 00 [19404/25568 ( 76%)], Train Loss: 0.52435\n",
      "Epoch: 00 [19444/25568 ( 76%)], Train Loss: 0.52398\n",
      "Epoch: 00 [19484/25568 ( 76%)], Train Loss: 0.52382\n",
      "Epoch: 00 [19524/25568 ( 76%)], Train Loss: 0.52357\n",
      "Epoch: 00 [19564/25568 ( 77%)], Train Loss: 0.52309\n",
      "Epoch: 00 [19604/25568 ( 77%)], Train Loss: 0.52264\n",
      "Epoch: 00 [19644/25568 ( 77%)], Train Loss: 0.52241\n",
      "Epoch: 00 [19684/25568 ( 77%)], Train Loss: 0.52195\n",
      "Epoch: 00 [19724/25568 ( 77%)], Train Loss: 0.52187\n",
      "Epoch: 00 [19764/25568 ( 77%)], Train Loss: 0.52162\n",
      "Epoch: 00 [19804/25568 ( 77%)], Train Loss: 0.52116\n",
      "Epoch: 00 [19844/25568 ( 78%)], Train Loss: 0.52117\n",
      "Epoch: 00 [19884/25568 ( 78%)], Train Loss: 0.52128\n",
      "Epoch: 00 [19924/25568 ( 78%)], Train Loss: 0.52148\n",
      "Epoch: 00 [19964/25568 ( 78%)], Train Loss: 0.52133\n",
      "Epoch: 00 [20004/25568 ( 78%)], Train Loss: 0.52088\n",
      "Epoch: 00 [20044/25568 ( 78%)], Train Loss: 0.52058\n",
      "Epoch: 00 [20084/25568 ( 79%)], Train Loss: 0.52058\n",
      "Epoch: 00 [20124/25568 ( 79%)], Train Loss: 0.52030\n",
      "Epoch: 00 [20164/25568 ( 79%)], Train Loss: 0.52006\n",
      "Epoch: 00 [20204/25568 ( 79%)], Train Loss: 0.51975\n",
      "Epoch: 00 [20244/25568 ( 79%)], Train Loss: 0.51944\n",
      "Epoch: 00 [20284/25568 ( 79%)], Train Loss: 0.51932\n",
      "Epoch: 00 [20324/25568 ( 79%)], Train Loss: 0.51882\n",
      "Epoch: 00 [20364/25568 ( 80%)], Train Loss: 0.51858\n",
      "Epoch: 00 [20404/25568 ( 80%)], Train Loss: 0.51838\n",
      "Epoch: 00 [20444/25568 ( 80%)], Train Loss: 0.51798\n",
      "Epoch: 00 [20484/25568 ( 80%)], Train Loss: 0.51760\n",
      "Epoch: 00 [20524/25568 ( 80%)], Train Loss: 0.51708\n",
      "Epoch: 00 [20564/25568 ( 80%)], Train Loss: 0.51667\n",
      "Epoch: 00 [20604/25568 ( 81%)], Train Loss: 0.51646\n",
      "Epoch: 00 [20644/25568 ( 81%)], Train Loss: 0.51632\n",
      "Epoch: 00 [20684/25568 ( 81%)], Train Loss: 0.51631\n",
      "Epoch: 00 [20724/25568 ( 81%)], Train Loss: 0.51577\n",
      "Epoch: 00 [20764/25568 ( 81%)], Train Loss: 0.51560\n",
      "Epoch: 00 [20804/25568 ( 81%)], Train Loss: 0.51517\n",
      "Epoch: 00 [20844/25568 ( 82%)], Train Loss: 0.51527\n",
      "Epoch: 00 [20884/25568 ( 82%)], Train Loss: 0.51495\n",
      "Epoch: 00 [20924/25568 ( 82%)], Train Loss: 0.51435\n",
      "Epoch: 00 [20964/25568 ( 82%)], Train Loss: 0.51413\n",
      "Epoch: 00 [21004/25568 ( 82%)], Train Loss: 0.51356\n",
      "Epoch: 00 [21044/25568 ( 82%)], Train Loss: 0.51317\n",
      "Epoch: 00 [21084/25568 ( 82%)], Train Loss: 0.51268\n",
      "Epoch: 00 [21124/25568 ( 83%)], Train Loss: 0.51232\n",
      "Epoch: 00 [21164/25568 ( 83%)], Train Loss: 0.51177\n",
      "Epoch: 00 [21204/25568 ( 83%)], Train Loss: 0.51147\n",
      "Epoch: 00 [21244/25568 ( 83%)], Train Loss: 0.51144\n",
      "Epoch: 00 [21284/25568 ( 83%)], Train Loss: 0.51122\n",
      "Epoch: 00 [21324/25568 ( 83%)], Train Loss: 0.51095\n",
      "Epoch: 00 [21364/25568 ( 84%)], Train Loss: 0.51058\n",
      "Epoch: 00 [21404/25568 ( 84%)], Train Loss: 0.50987\n",
      "Epoch: 00 [21444/25568 ( 84%)], Train Loss: 0.50945\n",
      "Epoch: 00 [21484/25568 ( 84%)], Train Loss: 0.50925\n",
      "Epoch: 00 [21524/25568 ( 84%)], Train Loss: 0.50895\n",
      "Epoch: 00 [21564/25568 ( 84%)], Train Loss: 0.50878\n",
      "Epoch: 00 [21604/25568 ( 84%)], Train Loss: 0.50841\n",
      "Epoch: 00 [21644/25568 ( 85%)], Train Loss: 0.50788\n",
      "Epoch: 00 [21684/25568 ( 85%)], Train Loss: 0.50737\n",
      "Epoch: 00 [21724/25568 ( 85%)], Train Loss: 0.50694\n",
      "Epoch: 00 [21764/25568 ( 85%)], Train Loss: 0.50668\n",
      "Epoch: 00 [21804/25568 ( 85%)], Train Loss: 0.50618\n",
      "Epoch: 00 [21844/25568 ( 85%)], Train Loss: 0.50583\n",
      "Epoch: 00 [21884/25568 ( 86%)], Train Loss: 0.50599\n",
      "Epoch: 00 [21924/25568 ( 86%)], Train Loss: 0.50579\n",
      "Epoch: 00 [21964/25568 ( 86%)], Train Loss: 0.50558\n",
      "Epoch: 00 [22004/25568 ( 86%)], Train Loss: 0.50568\n",
      "Epoch: 00 [22044/25568 ( 86%)], Train Loss: 0.50534\n",
      "Epoch: 00 [22084/25568 ( 86%)], Train Loss: 0.50519\n",
      "Epoch: 00 [22124/25568 ( 87%)], Train Loss: 0.50482\n",
      "Epoch: 00 [22164/25568 ( 87%)], Train Loss: 0.50460\n",
      "Epoch: 00 [22204/25568 ( 87%)], Train Loss: 0.50431\n",
      "Epoch: 00 [22244/25568 ( 87%)], Train Loss: 0.50419\n",
      "Epoch: 00 [22284/25568 ( 87%)], Train Loss: 0.50399\n",
      "Epoch: 00 [22324/25568 ( 87%)], Train Loss: 0.50415\n",
      "Epoch: 00 [22364/25568 ( 87%)], Train Loss: 0.50359\n",
      "Epoch: 00 [22404/25568 ( 88%)], Train Loss: 0.50327\n",
      "Epoch: 00 [22444/25568 ( 88%)], Train Loss: 0.50296\n",
      "Epoch: 00 [22484/25568 ( 88%)], Train Loss: 0.50265\n",
      "Epoch: 00 [22524/25568 ( 88%)], Train Loss: 0.50262\n",
      "Epoch: 00 [22564/25568 ( 88%)], Train Loss: 0.50238\n",
      "Epoch: 00 [22604/25568 ( 88%)], Train Loss: 0.50258\n",
      "Epoch: 00 [22644/25568 ( 89%)], Train Loss: 0.50226\n",
      "Epoch: 00 [22684/25568 ( 89%)], Train Loss: 0.50180\n",
      "Epoch: 00 [22724/25568 ( 89%)], Train Loss: 0.50147\n",
      "Epoch: 00 [22764/25568 ( 89%)], Train Loss: 0.50162\n",
      "Epoch: 00 [22804/25568 ( 89%)], Train Loss: 0.50125\n",
      "Epoch: 00 [22844/25568 ( 89%)], Train Loss: 0.50093\n",
      "Epoch: 00 [22884/25568 ( 90%)], Train Loss: 0.50053\n",
      "Epoch: 00 [22924/25568 ( 90%)], Train Loss: 0.50037\n",
      "Epoch: 00 [22964/25568 ( 90%)], Train Loss: 0.50062\n",
      "Epoch: 00 [23004/25568 ( 90%)], Train Loss: 0.50029\n",
      "Epoch: 00 [23044/25568 ( 90%)], Train Loss: 0.49987\n",
      "Epoch: 00 [23084/25568 ( 90%)], Train Loss: 0.49953\n",
      "Epoch: 00 [23124/25568 ( 90%)], Train Loss: 0.49909\n",
      "Epoch: 00 [23164/25568 ( 91%)], Train Loss: 0.49897\n",
      "Epoch: 00 [23204/25568 ( 91%)], Train Loss: 0.49864\n",
      "Epoch: 00 [23244/25568 ( 91%)], Train Loss: 0.49841\n",
      "Epoch: 00 [23284/25568 ( 91%)], Train Loss: 0.49814\n",
      "Epoch: 00 [23324/25568 ( 91%)], Train Loss: 0.49784\n",
      "Epoch: 00 [23364/25568 ( 91%)], Train Loss: 0.49771\n",
      "Epoch: 00 [23404/25568 ( 92%)], Train Loss: 0.49742\n",
      "Epoch: 00 [23444/25568 ( 92%)], Train Loss: 0.49720\n",
      "Epoch: 00 [23484/25568 ( 92%)], Train Loss: 0.49695\n",
      "Epoch: 00 [23524/25568 ( 92%)], Train Loss: 0.49650\n",
      "Epoch: 00 [23564/25568 ( 92%)], Train Loss: 0.49624\n",
      "Epoch: 00 [23604/25568 ( 92%)], Train Loss: 0.49568\n",
      "Epoch: 00 [23644/25568 ( 92%)], Train Loss: 0.49522\n",
      "Epoch: 00 [23684/25568 ( 93%)], Train Loss: 0.49536\n",
      "Epoch: 00 [23724/25568 ( 93%)], Train Loss: 0.49491\n",
      "Epoch: 00 [23764/25568 ( 93%)], Train Loss: 0.49464\n",
      "Epoch: 00 [23804/25568 ( 93%)], Train Loss: 0.49455\n",
      "Epoch: 00 [23844/25568 ( 93%)], Train Loss: 0.49439\n",
      "Epoch: 00 [23884/25568 ( 93%)], Train Loss: 0.49386\n",
      "Epoch: 00 [23924/25568 ( 94%)], Train Loss: 0.49347\n",
      "Epoch: 00 [23964/25568 ( 94%)], Train Loss: 0.49317\n",
      "Epoch: 00 [24004/25568 ( 94%)], Train Loss: 0.49276\n",
      "Epoch: 00 [24044/25568 ( 94%)], Train Loss: 0.49256\n",
      "Epoch: 00 [24084/25568 ( 94%)], Train Loss: 0.49223\n",
      "Epoch: 00 [24124/25568 ( 94%)], Train Loss: 0.49190\n",
      "Epoch: 00 [24164/25568 ( 95%)], Train Loss: 0.49161\n",
      "Epoch: 00 [24204/25568 ( 95%)], Train Loss: 0.49120\n",
      "Epoch: 00 [24244/25568 ( 95%)], Train Loss: 0.49086\n",
      "Epoch: 00 [24284/25568 ( 95%)], Train Loss: 0.49075\n",
      "Epoch: 00 [24324/25568 ( 95%)], Train Loss: 0.49046\n",
      "Epoch: 00 [24364/25568 ( 95%)], Train Loss: 0.49024\n",
      "Epoch: 00 [24404/25568 ( 95%)], Train Loss: 0.49006\n",
      "Epoch: 00 [24444/25568 ( 96%)], Train Loss: 0.48999\n",
      "Epoch: 00 [24484/25568 ( 96%)], Train Loss: 0.48998\n",
      "Epoch: 00 [24524/25568 ( 96%)], Train Loss: 0.48964\n",
      "Epoch: 00 [24564/25568 ( 96%)], Train Loss: 0.48929\n",
      "Epoch: 00 [24604/25568 ( 96%)], Train Loss: 0.48914\n",
      "Epoch: 00 [24644/25568 ( 96%)], Train Loss: 0.48877\n",
      "Epoch: 00 [24684/25568 ( 97%)], Train Loss: 0.48858\n",
      "Epoch: 00 [24724/25568 ( 97%)], Train Loss: 0.48847\n",
      "Epoch: 00 [24764/25568 ( 97%)], Train Loss: 0.48827\n",
      "Epoch: 00 [24804/25568 ( 97%)], Train Loss: 0.48835\n",
      "Epoch: 00 [24844/25568 ( 97%)], Train Loss: 0.48812\n",
      "Epoch: 00 [24884/25568 ( 97%)], Train Loss: 0.48782\n",
      "Epoch: 00 [24924/25568 ( 97%)], Train Loss: 0.48742\n",
      "Epoch: 00 [24964/25568 ( 98%)], Train Loss: 0.48749\n",
      "Epoch: 00 [25004/25568 ( 98%)], Train Loss: 0.48763\n",
      "Epoch: 00 [25044/25568 ( 98%)], Train Loss: 0.48741\n",
      "Epoch: 00 [25084/25568 ( 98%)], Train Loss: 0.48744\n",
      "Epoch: 00 [25124/25568 ( 98%)], Train Loss: 0.48732\n",
      "Epoch: 00 [25164/25568 ( 98%)], Train Loss: 0.48700\n",
      "Epoch: 00 [25204/25568 ( 99%)], Train Loss: 0.48666\n",
      "Epoch: 00 [25244/25568 ( 99%)], Train Loss: 0.48639\n",
      "Epoch: 00 [25284/25568 ( 99%)], Train Loss: 0.48594\n",
      "Epoch: 00 [25324/25568 ( 99%)], Train Loss: 0.48551\n",
      "Epoch: 00 [25364/25568 ( 99%)], Train Loss: 0.48536\n",
      "Epoch: 00 [25404/25568 ( 99%)], Train Loss: 0.48531\n",
      "Epoch: 00 [25444/25568 (100%)], Train Loss: 0.48514\n",
      "Epoch: 00 [25484/25568 (100%)], Train Loss: 0.48518\n",
      "Epoch: 00 [25524/25568 (100%)], Train Loss: 0.48487\n",
      "Epoch: 00 [25564/25568 (100%)], Train Loss: 0.48476\n",
      "Epoch: 00 [25568/25568 (100%)], Train Loss: 0.48478\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] Valid Loss: 0.22463\n",
      "Post-processing 222 example predictions split into 2932 features.\n",
      "valid jaccard:  0.6816324441324442\n",
      "0 Epoch, Best epoch was updated! Valid Loss: 0.22463\n",
      "Saving model checkpoint to output/checkpoint-fold-4-epoch-0.\n",
      "\n",
      "Total Training Time: 3764.234701395035secs, Average Training Time per Epoch: 3764.234701395035secs.\n",
      "Total Validation Time: 140.0582571029663secs, Average Validation Time per Epoch: 140.0582571029663secs.\n",
      "**************************************************\n",
      "Final jacard scores, 5-fold:  [0.67525 0.70943 0.68576 0.67927 0.68163]\n",
      "Average jacard: 0.6862684912684913\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "for fold in range(5):\n",
    "  print();print()\n",
    "  print('-'*50)\n",
    "  print(f'FOLD: {fold}')\n",
    "  print('-'*50)\n",
    "  run(train, fold)\n",
    "\n",
    "print(\"*\"*50)\n",
    "print(\"Final jacard scores, 5-fold: \", np.round(all_jacard_scores,5))\n",
    "print(\"Average jacard:\",np.mean(all_jacard_scores))\n",
    "print(\"*\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 20086.486376,
   "end_time": "2021-11-15T00:02:11.869264",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-11-14T18:27:25.382888",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
